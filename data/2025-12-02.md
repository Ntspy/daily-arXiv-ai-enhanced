<div id=toc></div>

# Table of Contents

- [astro-ph.HE](#astro-ph.HE) [Total: 14]
- [hep-ph](#hep-ph) [Total: 45]
- [gr-qc](#gr-qc) [Total: 29]
- [astro-ph.IM](#astro-ph.IM) [Total: 11]


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [1] [The impact of magnetic fields during tidal disruption events](https://arxiv.org/abs/2511.21818)
*Simona Pacuraru,Clément Bonnerot,Martin E. Pessah*

Main category: astro-ph.HE

TL;DR: Magnetic fields stronger than 10^4 G in TDE events significantly influence stream debris dynamics, causing rapid expansion that enhances radio emission and alters gas evolution post-pericentre return. This work provides foundational MHD data for future studies on accretion disk dynamics and observable signatures like X-ray emissions.


<details>
  <summary>Details</summary>
Motivation: Understand how stellar magnetic fields influence the evolution of debris streams in TDEs, particularly their impact on stream thickness, emission properties, and subsequent accretion processes.

Method: Combination of magnetohydrodynamic (MHD) simulations and semi-analytic models to simulate the disruption of a main-sequence star by a supermassive black hole, focusing on magnetic field evolution and dynamical effects on debris streams.

Result: For magnetic fields above ~10^4 G, magnetic pressure causes stream expansion (H ∝ R^(5/4)), increasing thickness in unbound tails and affecting post-pericentre shocks and radio emission. This establishes MHD-dependent initial conditions for later TDE phases.

Conclusion: Magnetic fields play a critical role in TDE stream dynamics from disruption to pericentre return, impacting observable phenomena and necessitating their inclusion in models to accurately predict accretion disk behavior and resulting emissions.

Abstract: During a tidal disruption event (TDE) the stream debris inherits the magnetic field of the star. As the stream stretches, the magnetic field evolves and can eventually become dynamically important. We study this effect by means of magnetohydrodynamic simulations and a semi-analytic model of the disruption of a main-sequence star by a supermassive black hole. For stellar magnetic fields stronger than $\sim 10^4\,\rm{G}$, magnetic pressure becomes important in a significant fraction of the mass of the stream, leading to a fast increase in its thickness, an effect that may impact its subsequent evolution. We find that this dynamical effect is associated with a phase of transverse equilibrium between magnetic and tidal forces, which causes the stream width to increase with distance to the black hole as $H \propto R^{5/4}$. In the unbound tail, this fast expansion could enhance the radio emission produced by the interaction with the ambient medium, while in the returning stream, it may qualitatively affect the subsequent gas evolution, particularly the gas dynamics and radiative properties of shocks occurring after the stream's return to pericentre. By characterizing the magnetohydrodynamic properties of the stream from disruption to the first return to pericentre, this work provides physically motivated initial conditions for future studies of the later phases of TDEs, accounting for magnetic fields. This will ultimately shed light on the role of magnetic fields in enabling angular momentum transport in the ensuing accretion disk, thereby affecting observable signatures such as X-ray radiation and relativistic outflows.

</details>


### [2] [Gravitational Waves as a Probe of Core Collapse Supernova Progenitor Structure](https://arxiv.org/abs/2511.21895)
*R. Daniel Murphy,Elle Brinkman,Colter J. Richardson,Evan Semenak,Anthony Mezzacappa,Pedro Marronetti,Eric J. Lentz,Stephen W. Bruenn*

Main category: astro-ph.HE

TL;DR: The paper demonstrates that gravitational wave signals from core-collapse supernovae can reveal differences in progenitor stellar structures through variations in signal features like strain amplitude and mode frequency evolution, highlighting potential for stellar interior insights.


<details>
  <summary>Details</summary>
Motivation: To investigate how differences in progenitor internal structures affect gravitational wave signals and assess the feasibility of using these signals to infer stellar interiors.

Method: Simulations of two nearly identical CCSN progenitors with different compactness parameters (0.136 vs 0.206). Analyzed gravitational wave features (strain amplitude, gf-mode frequency slopes) linked to explosion dynamics.

Result: The more compact progenitor showed higher gravitational wave strain amplitudes, greater energy release, and a 26% steeper gfF slope due to enhanced accretion and faster contraction, establishing detectable structural correlations.

Conclusion: Gravitational wave observations can potentially provide information about progenitor stellar structures like compactness, offering a new probe of stellar interiors inaccessible through electromagnetic observations.

Abstract: We present the gravitational wave predictions from two-dimensional core collapse supernova (CCSN) simulations initiated from two nearly identical progenitors that have significantly different internal structures due to their late-stage stellar evolution. At the time of collapse, the 15.78 $M_{\odot}$ and 15.79 $M_{\odot}$ progenitors have compactness parameters $ξ_{2.5}$ of 0.136 and 0.206, respectively. We connect several features of the gravitational wave signal from each model to its previously explored explosion dynamics. In particular, the greater accretion onto the PNS of the more compact model is evident in broad-band frequency features with larger amplitude gravitational wave strains and greater gravitational wave energy release when compared to the less compact model. Additionally, the faster contraction rate of the more compact model is reflected in the $\sim$26% greater slope of the $g$-/$f$-mode feature (gfF) evolution of the gravitational wave signal. This work shows that in principle gravitational wave detection may provide information about interior stellar structure.

</details>


### [3] [Data span and frequency coverage requirements for robust detection and inference in PTAs: A case study with EPTA DR2](https://arxiv.org/abs/2511.21933)
*Irene Ferranti,Mikel Falxa,Federico Fantoccoli,Alberto Sesana,Golam Mohiuddin Shaifullah*

Main category: astro-ph.HE

TL;DR: The study investigates the counterintuitive increase in GWB significance when using a shorter dataset (DR2new) compared to the full dataset (DR2full) in the EPTA analysis, concluding that it can arise from noise fluctuations (∼2σ) and highlights the importance of proper modeling of spectral leakage and combining multi-array data to enhance sensitivity.


<details>
  <summary>Details</summary>
Motivation: To understand why restricting the EPTA dataset to a shorter timeframe increases GWB significance and ensure the robustness of analysis pipelines as PTAs near detection thresholds.

Method: The authors perform realistic simulations of EPTA DR2-like data with varying timespans, analyze the overlap of HD S/N distributions, and incorporate spectral leakage effects in parameter estimation. They also combine EPTA with other arrays' data to assess enhanced constraints.

Result: The first 10 years of DR2full contribute minimally to GWB evidence due to limited frequency coverage, leading to a 15% chance of higher significance in DR2new purely from noise. Simulations with spectral leakage corrections resolve biases, and multi-array combinations improve GWB parameter accuracy. Short-baseline datasets introduce amplitude biases.

Conclusion: The observed DR2new behavior is statistically consistent with noise fluctuations (∼2σ) and does not indicate an anomaly. Proper modeling of spectral leakage is essential for unbiased parameter estimation. Multi-array data integration significantly boosts GWB detection sensitivity and accuracy.

Abstract: Pulsar Timing Arrays (PTAs) are approaching the sensitivity required for a $5σ$ detection of the nanohertz stochastic gravitational-wave background (GWB). This makes it crucial to deeply understand the behaviour of our analysis pipelines. A counterintuitive feature of the European Pulsar Timing Array (EPTA) second data release is that restricting the dataset to the last 10.3 years (DR2new) increases the inferred GWB significance from $\leq2σ$ for the full 25-year dataset (DR2full) to $\geq3.5σ$. We investigate whether this behaviour indicates an anomaly or is a possible outcome of the pipeline. Using realistic, DR2-like simulations with varying timespans, we find that the first 10 years contribute little to the GWB evidence due to their limited frequency coverage. This produces substantial overlap between the HD S/N distributions of DR2full and DR2new. Random noise fluctuations therefore yield a higher GWB evidence in DR2new than in DR2full in $15\%$ of cases. Furthermore, $5\%$ of simulations match the HD S/N of the real data, indicating that the observed behaviour is consistent with being a $\sim2σ$ outcome due to noise fluctuations. Regardless of significance, DR2new simulations introduce biases in the GWB parameter estimation due to spectral leakage effects that are ignored in standard analyses and which flatten the inferred spectrum. Including leakage removes these biases, demonstrating the reliability of DR2new when the signal is properly modelled. Furthermore, we demonstrate that combining EPTA DR2full with long-baseline data from NANOGrav and PPTA, as well as low-frequency data from LOFAR and NenuFAR, significantly enhances GWB evidence and parameter accuracy. Finally, we examine the impact of the observation timespan and find that short-baseline datasets introduce strong amplitude biases and are ineffective at constraining the GWB.

</details>


### [4] [Characterizing Binary Black Hole Subpopulations in GWTC-4 with Binned Gaussian Processes: On the Origins of the $35M_{\odot}$ Peak](https://arxiv.org/abs/2511.22093)
*Omkar Sridhar,Anarya Ray,Vicky Kalogera*

Main category: astro-ph.HE

TL;DR: The paper identifies three subpopulations of binary black holes (BBHs) in the GWTC-4 sample, with one subpopulation showing a 35M☉ peak linked to dynamical formation in globular clusters. This subpopulation favors equal masses and isotropic spins, requiring black hole birth spins of 0.1-0.2. The dynamical origin is consistent with theoretical models, and globular clusters contribute at least 0.69 Gpc⁻³yr⁻¹ to BBH merger rates.


<details>
  <summary>Details</summary>
Motivation: The need to understand the astrophysical origins of BBHs by modeling their population properties, particularly the 35M☉ mass peak observed in gravitational wave data. Existing models are insufficiently flexible or accurate for characterizing multi-dimensional distributions of mass ratios, masses, and spins.

Method: Used a binned Gaussian processes framework to model the joint distribution of BBH primary masses, mass ratios, and effective inspiral spins. Analyzed GWTC-4 data to identify three subpopulations. Compared results with theoretical simulations of globular cluster dynamics and other formation channels.

Result: Found one subpopulation with a 35M☉ mass peak preferring equal masses and isotropic spins, best explained by dynamical assembly in globular clusters with specific birth spin constraints. The other two subpopulations require contributions from other formation channels. Constrained the minimal globular cluster BBH merger rate to 0.69⁺⁰˙²³₋₀˙³³ Gpc⁻³yr⁻¹.

Conclusion: Dynamical formation in globular clusters is a strong candidate for the 30-40M☉ BBH excess, but further data and parametric models are needed to confirm this. The results highlight the importance of multi-dimensional population studies for distinguishing formation channels.

Abstract: Understanding the astrophysical origins of binary black holes requires accurate and flexible modeling of multi-dimensional population properties. In this paper, using a data-driven framework based on binned Gaussian processes, we characterize the joint distribution of BBH primary masses, mass ratios, and effective inspiral spins. We identify three distinct subpopulations in the GWTC-4 sample of observations and investigate their astrophysical origins. We find that only one of the three subpopulations exhibits the $35M_{\odot}$ peak, which is characterized by a strong preference for equal mass systems and isotropic spin orientations. Our inferred distributions are consistent with a predominantly dynamical origin of this feature. By comparing with theoretical simulations, we further show that the subpopulation that exhibits the $35M_{\sun}$ peak can exclusively comprise dynamically assembled systems in globular clusters, specifically if black hole birth spins are in the range~$(0.1-0.2)$, whereas the other two subpopulations require substantial contributions from alternative formation channels. We constrain the \textit{lower bound} on the merger rate of BBHs in globular clusters to be $0.69^{+0.23}_{-0.33} \rm{Gpc}^{-3}\rm{yr}^{-1}$, which is consistent with theoretical predictions. We conclude that dynamical formation in globular clusters remains a strong candidate for the origin of this excess near $30-40M_{\odot}$ and that more data and targeted parametric models are necessary to rigorously establish this interpretation.

</details>


### [5] [Constraining the Properties of GRB Accreting Magnetar with $R/I$ Evolutionary Effects Using \emph{Swift}/XRT Data](https://arxiv.org/abs/2511.22149)
*Lin Lan,He Gao,Litao Zhao,Shunke Ai,Jie Lin,Long Li,Lang Xie,Li-Ping Xin,Jian-Yan Wei*

Main category: astro-ph.HE

TL;DR: This study investigates the propeller properties of accreting millisecond magnetars powering long gamma-ray bursts (LGRBs) with X-ray plateaus. The authors used an observed correlation between the magnetar's initial spin period and surface magnetic field to explore how accretion affects their evolution. They found that the derived magnetic field versus spin period relation aligns with theoretical propeller model predictions, suggesting the observed spin periods are equilibrium values set by accretion rather than the true natal spin. Considering neutron star structure evolution (R/I effects), the inferred accretion rates are lower than previous estimates, indicating that constant structure assumptions overestimate accretion rates. This has implications for understanding magnetar environments and suggests low-metallicity supernova progenitors may provide sufficient material for such accretion.


<details>
  <summary>Details</summary>
Motivation: To understand why previous studies reported higher accretion rates and what effects neutron star structure evolution has on magnetar-powered LGRBs. The authors aimed to reconcile observed LGRB properties with magnetar propeller models, addressing potential biases from assuming constant neutron star compactness (R/I) and gravitational mass in earlier works.

Method: Analzyed an existing LGRB sample using a new Bp-P0 correlation, incorporated evolving neutron star structure (R/I and M_g/M_b relationships), calculated accretion rates using propeller model equations, compared results with earlier constant-parameter studies, and linked accretion requirements to progenitor metallicities.

Result: Discovered that Bp-P0 follows Bp ∝ P_eq^(7/6), showing spin periods reflect equilibrium states from accretion. Derived accretion rates (1e-5-1e-2 Msun/s) are an order of magnitude lower than previous estimates. The lower rates mean magnetars can reach equilibrium spin without being disrupted. Found low-metallicity environments likely supply enough fallback material to sustain these accretion rates.

Conclusion: Proper accounting of neutron star structural evolution is critical for accurate propeller modeling. Earlier constant-parameter approaches overestimated accretion rates, misrepresenting magnetar environments. The results support magnetar central engines for LGRBs and highlight the importance of progenitor metallicity in providing sufficient fallback material for prolonged accretion phases.

Abstract: A newly born millisecond magnetar has been proposed as one possible central engine of some long gamma-ray bursts (LGRBs) with X-ray plateau. In this work, we used a universal correlation between initial spin period ($P_0$) and surface magnetic field ($B_p$) of newborn magnetar based on an LGRB sample in \cite{Lan2025} to explore the propeller properties of accreting magnetar with $R/I$ evolutionary effects. We found that $B_p-P_0$ relation is approximately consistent with $B_p\propto P_{\rm eq}^{7/6}$. Here, $P_{\rm eq}$ is equilibrium spin period in magnetic propeller model. The $B_p-P_0$ relation indicates that $P_0$ may not be true initial spin period of newborn magnetar, but had reached an equilibrium spin period via fallback accretion in propeller model. The magnetar accretion rate in our LGRBs is in range of $\dot{M}\sim10^{-5}-10^{-2} M_{\odot} \rm s^{-1}$ by incorporating $R/I$ evolutionary effects, and using the transition relation between gravitational mass $M_g$ and baryonic mass $M_b$ in different equation of states. Such accretion rates ensure that the accreting magnetars in our sample survive until reaching the equilibrium spin period, and the accretion rate is one order of magnitude lower compared to the statistical results in \cite{Stratta2018} and \cite{Linweili2020}, which used constant $R/I/M_g$ scenario. We suggested that adopting a constant $R/I/M_g$ scenario for modeling propeller regime in accreting magnetar results in a higher mass accretion rate, which may impair our understanding of the physical nature and its surroundings of accreting magnetar, and low-metallicity progenitors can provide enough material to satisfy the accretion requirements of newborn accreting magnetar in LGRBs.

</details>


### [6] [Needle in a Poissonian haystack -- An X-ray astronomer's guide to QPE fishing](https://arxiv.org/abs/2511.22520)
*E. Quintin,N. Khan,N. A. Webb,R. Webbe,R. D. Saxton,G. Miniutti,M. Giustini*

Main category: astro-ph.HE

TL;DR: The paper addresses the need to explore alternative methods for detecting Quasi-Periodic Eruptions (QPEs) beyond current TDE-focused strategies, proposing two observational techniques for discovering QPEs in XMM-Newton archives and real-time follow-ups to avoid confirmation bias.


<details>
  <summary>Details</summary>
Motivation: Current observing strategies focusing on late-time X-ray follow-ups of TDEs risk confirmation bias, potentially leading to premature conclusions about QPEs. Diversifying methods is essential to uncover previously missed transient events.

Method: 1. Long-term analysis of XMM-Newton archives to detect decade-scale QPEs. 2. Short-term monitoring to capture minute-scale transients, both using non-standard detection pipelines.

Result: Proposed methods offer pathways to identify QPE sources beyond existing biased samples, facilitating a more complete understanding of QPE-TDE relationships.

Conclusion: Agnostic observational approaches are critical to avoid skewed conclusions about QPEs. Combining archival mining with real-time triggers provides a balanced strategy for future research.

Abstract: After six years of studies following the discovery of GSN069, a link is starting to appear between the elusive Quasi-Periodic Eruptions (QPEs) and other types of nuclear transients, among which are Tidal Disruption Events (TDEs). As such, observing strategies are adapting, with a current trend focusing on late-time X-ray follow-ups of (optical) TDEs. While these campaigns are so far proving quite successful, the inherent confirmation bias they introduce in our sample could lead the community to hasty, and perhaps erroneous, conclusions. It is thus important to still pursue the search for nuclear transients in other, more agnostic directions. In this work, we focus on the observational aspects of our field, and lay out two different methods that can be deployed in order to reveal new QPE sources. These complementary methods enable the detection of long-term ($\sim$years) and short term ($\sim$minutes) transient events, that would have otherwise been missed by the standard detection pipelines. Both of these methods can be used either for data mining in the 25 years worth of XMM-Newton archive, or to trigger real-time follow-ups upon a more recent discovery.

</details>


### [7] [Towards Understanding the Origin of Swift Gamma-Ray Bursts Driven by Magnetars](https://arxiv.org/abs/2511.22534)
*C. T. Hao,J. H. Jing,X. L. Han,H. R. Lan,W. C. Du,X. N. Liu,Z. B. Zhang,H. C. Liu,J. F. Wu,X. L. Xia*

Main category: astro-ph.HE

TL;DR: The study provides evidence that magnetars can power both short and long GRBs with different X-ray plateau characteristics, suggesting multiple formation channels.


<details>
  <summary>Details</summary>
Motivation: To determine if magnetars are the central engines for GRBs with extended emissions and X-ray plateaus, and to understand their role in both short and long GRBs.

Method: Analyzing a Swift GRB sample using multi-wavelength data and multi-standards. Tested relationships between luminosity, time, and energy; classified GRBs by magnetic field and period; standardized X-ray afterglows; and performed K-S tests on redshift and energy distributions.

Result: Confirmed a three-parameter relation consistent with normal GRBs; most GRBs except GRB 211024B align with magnetar-powered models; standardized X-ray plateaus resemble magnetar profiles; isotropic energy distributions link internal/external plateaus to short/long GRBs.

Conclusion: Magnetars likely have multiple formation channels explaining their association with various GRB types, supporting their role in powering both short and long GRBs through different mechanisms.

Abstract: We analyze a sample of\textit{ Swift} gamma-ray bursts (GRBs) with extended emissions in $γ$-rays and/or X-ray plateaus that may be driven by magnetars. Multi-wavelength data and multi-standards have been adopted to investigate the issue jointly. First, we find that GRBs with both extended emission and X-ray plateau satisfy a three-parameter relation between the luminosity and the end time of X-ray plateaus and the $γ$-ray isotropic energy as $L_X\varpropto T_a^{-1.13}E_{γ,iso}^{0.74}$, which is consistent with that of normal GRBs. Second, we distinguish these GRBs in the plane of magnetic field versus period of neutron star and find that almost all GRBs but GRB 211024B have reasonable periods and majority of them could be powered by magnetars. Third, we standardize the X-ray afterglows with distinct characteristics and find that the standard X-ray light curves with/without plateaus are significantly different. The standardized X-ray plateaus are similar to the mean temporal profile of magnetars. Fourth, it is verified with a K-S test that all types of GRBs except short ones have the similar distributions of redshift and isotropic energy in the observer/rest frame. GRBs with internal plateaus are significantly different from those of normal long GRBs and GRBs with external plateaus and/or extended emissions. Interestingly, the isotropic energy distributions of GRBs with internal and external plateaus are identical with those of short and long GRBs, respectively. Overall, our study can bring solid evidence that the fascinating magnetars could have multi-formation channels to account for not only short but also long GRBs with either internal or external X-ray plateaus as well.

</details>


### [8] [Vortex Dynamics in the Neutron Star Inner Crust](https://arxiv.org/abs/2511.22670)
*Xin Sheng,Bennett Link,Matthew E. Caplan,Yuri Levin*

Main category: astro-ph.HE

TL;DR: The paper investigates superfluid vortex motion in neutron star inner crusts using 3D simulations, focusing on vortex-lattice interactions. It finds that attractive interactions enhance vortex pinning, while unpinning is influenced by lattice parameters and can propagate across grains, potentially reducing critical unpinning force and causing avalanches.


<details>
  <summary>Details</summary>
Motivation: To understand how superfluid vortices interact with the nuclear lattice in neutron star crusts, which is crucial for modeling neutron star dynamics, viscosity, and potential astrophysical phenomena like starquakes.

Method: Direct three-dimensional numerical simulations of coupled vortex and nuclear lattice dynamics. Parameters explored include pinning force sign, lattice orientation, composition, temperature, and pinning energy. Scenarios include shearing from starquakes and vortex interactions.

Result: Vortex pinning efficiency is higher for attractive nucleus-vortex interactions. Unpinning propagates through Kelvin waves, starting in weaker-pinned grains and crossing into stronger ones, lowering critical force. Shearing triggers unpinning across slip planes, and unpinned vortices can cause avalanches by triggering neighboring vortex unpinning.

Conclusion: Vortex behavior in neutron star crusts is highly sensitive to lattice structure and interactions, with unpinning dynamics potentially influencing starquake effects and long-term neutron star evolution through altered viscosity and energy dissipation pathways.

Abstract: We study the superfluid vortex motion in the neutron star inner crust through direct three-dimensional simulations of the coupled dynamics of the vortex and the nuclear lattice. We demonstrate the pinning of an initially moving vortex to the lattice through excitation of lattice vibrations, and show that the efficiency of this process is higher for attractive than for repulsive nucleus-vortex interactions. We explore the unpinning of a vortex under the action of the applied Magnus force, and find that it is influenced by multiple parameters, including the sign of the pinning force, the lattice orientation, composition, temperature, and the energy of pinning to individual nucleus. In lattices with multiple grains, the unpinning transition is triggered inside the grains with weaker pinning, propagates along the vortex (mediated by the excited Kelvin waves) and crosses into grains with stronger pinning. This is likely to effectively decrease the critical force at which vortices unpin and to produce extended regions of unpinned vorticity. Shearing of the crust lattice (e.g., by a starquake) initiates the unpinning of the vortices that are crossing the slip plane. A close encounter of an unpinned vortex with a pinned vortex would cause the latter to unpin, perhaps initiating an unpinning avalanche of many vortices.

</details>


### [9] [Incorporating neutron star physics into gravitational wave inference with neural priors](https://arxiv.org/abs/2511.22987)
*Thibeau Wouters,Peter T. H. Pang,Tim Dietrich,Chris Van Den Broeck*

Main category: astro-ph.HE

TL;DR: The paper introduces neural priors using normalizing flows to incorporate nuclear physics and neutron star data into gravitational-wave analyses, improving parameter estimation and source classification.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of traditional agnostic priors which ignore valuable information from nuclear physics and neutron star observations, leading to less accurate parameter estimation.

Method: Develop data-driven prior distributions (neural priors) with normalizing flows, informed by nuclear equation of state constraints and neutron star population models. Applied to three gravitational-wave events (GW170817, GW190425, GW230529).

Result: Enhanced classification of merger sources, enabled equation of state model selection, and tighter constraints on source properties (e.g., higher recovered luminosity distances) compared to standard methods.

Conclusion: Neural priors improve gravitational-wave analysis by integrating physically informed data, aiding in classification of ambiguous mergers and facilitating updates from advancing neutron star research.

Abstract: Bayesian inference, widely used in gravitational-wave parameter estimation, depends on the choice of priors, i.e., on our previously existing knowledge. However, to investigate neutron star mergers, priors are often chosen in an agnostic way, leaving valuable information from nuclear physics and independent observations of neutron stars unused. In this work, we propose to encode information on neutron star physics into data-driven prior distributions constructed with normalizing flows, referred to as neural priors. These priors take input from constraints on the nuclear equation of state and neutron star population models. Applied to GW170817, GW190425, and GW230529, we highlight two contributions of the framework. First, we demonstrate its ability to provide source classification and to enable model selection of equation of state constraints for loud signals such as GW170817, directly from the gravitational-wave data. Second, we obtain narrower constraints on the source properties through these informed priors. As a result, the neural priors consistently recover higher luminosity distances compared to agnostic priors. Our method paves the way for classifying future ambiguous low-mass mergers observed through gravitational waves and for continuously incorporating advances in our understanding of neutron star properties into gravitational-wave data analysis.

</details>


### [10] [Blazar classification from multi-wavelength data using Deep Learning](https://arxiv.org/abs/2511.23134)
*Saqlain Afroz,Titir Mukherjee,Raj Prince*

Main category: astro-ph.HE

TL;DR: A deep feed-forward ANN is developed to classify blazars of uncertain types (BCUs) into FSRQs and BL Lacs using multi-wavelength data, starting with four broadband parameters and extending to additional inputs to assess their impact.


<details>
  <summary>Details</summary>
Motivation: Classify BCUs into known blazar types due to limited multi-wavelength data availability and the need for automated methods given the large number of Fermi-LAT detected objects.

Method: Development of a first-time ANN model using parameters like radio, optical, X-ray fluxes, and redshift, then expanding inputs to evaluate effects on classification accuracy.

Result: The ANN framework effectively addresses BCU classification challenges, with analysis of how added parameters influence outcomes, demonstrating potential for machine learning in astrophysical classification tasks.

Conclusion: ANN models offer a viable pathway to classify BCUs with current data limitations, highlighting the importance of multi-wavelength information and setting foundation for future studies with expanded datasets.

Abstract: The launch of the Fermi-LAT telescope has revolutionized gamma-ray astronomy by detecting over 7,000 gamma-ray emitting objects. A major fraction of the objects are blazars of known type, and a similar fraction of objects were classified as blazars of uncertain types (BCUs). Apart from that, some of the objects were found to be unassociated with any other classes, and no information is available in other wavebands. These types of objects are classified as unassociated objects in the Fermi catalog. To classify the unassociated objects into known categories and BCUs into a known type of blazar, numerous efforts have been made using the machine learning approach. The ideal way of classification would be to have multi-wavelength temporal and spectral information, which is nearly impossible to have for this number of objects in the near future. In this paper, we focus on classifying BCUs into other types of blazars, such as FSRQs and BL Lacs. For this purpose, we have developed for the first time a deep feed-forward Artificial Neural Network (ANN) to classify them, using multi-wavelength data. The complete understanding of blazars can only be known through multi-wavelength observation, and hence, we begin with four input parameters that cover broadband information (radio, optical, X-ray fluxes, and redshift) to train the neural network, and then extend the framework by including additional parameters to examine their impact on the outcome.

</details>


### [11] [Macroscopic Dark Matter under siege: from White Dwarf Data to Gravitational Wave Detection](https://arxiv.org/abs/2511.23263)
*Siyu Jiang,Aidi Yang,Fa Peng Huang*

Main category: astro-ph.HE

TL;DR: The paper presents a comprehensive approach to constrain Fermi-ball dark matter using updated astrophysical data from white dwarfs and neutron stars, alongside future gravitational wave experiments like LISA and TianQin, showcasing the effectiveness of multi-messenger methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of macroscopic dark matter detection, particularly Fermi-balls, which are elusive due to their low number density, and improve upon existing constraints through combined observational and experimental strategies.

Method: The authors revisited and updated astrophysical constraints using new white dwarf data related to ignition and supernova events, along with rigorous theoretical models. They also analyzed gravitational wave signals from future missions (LISA, TianQin) via signal-to-noise ratios and Fisher matrix techniques. Combined, these form a multi-messenger approach.

Result: The study derived the most comprehensive constraints on Fermi-ball parameter space by integrating white dwarf neutron star limits with projected gravitational wave sensitivities, highlighting the potential of multi-messenger astronomy in probing macroscopic dark matter.

Conclusion: Multi-messenger methods combining astrophysical observations and gravitational wave data are powerful tools for exploring macroscopic dark matter candidates like Fermi-balls, offering a path to future discovery or restriction of such models.

Abstract: The nature of dark matter (DM) remains a profound mystery. Macroscopic candidates, such as Fermi-balls, offer a distinct alternative to conventional particle DM, yet their low number density makes terrestrial detection challenging. In this work, we present a unified search strategy for sub-saturated Fermi-ball DM. We first revisit and significantly update astrophysical constraints from compact objects, utilizing rigorous expressions and additional white dwarf data related to ignition and subsequent supernovae. Crucially, we then explore novel signatures of Fermi-balls in future gravitational wave experiments like LISA and TianQin, performing detailed signal-to-noise ratio and Fisher matrix analyses. By combining these updated white dwarf/neutron star limits with the projected gravitational wave sensitivities, we derive the most comprehensive constraints on Fermi-ball parameter space to date, demonstrating the power of multi-messenger approaches for probing macroscopic DM.

</details>


### [12] [Simulating AGN feedback in galaxy clusters with pre-existing turbulence](https://arxiv.org/abs/2511.23267)
*Jia-Lun Li,H. -Y. Karen Yang*

Main category: astro-ph.HE

TL;DR: The study uses 3D simulations of a Perseus-like cluster to show that while pre-existing turbulence dominates the velocity field, AGN feedback via bubbles and shocks still contributes significantly. Turbulent heating is insufficient to counteract radiative cooling, particularly in the cluster core, suggesting that multiple mechanisms are needed.


<details>
  <summary>Details</summary>
Motivation: To resolve the discrepancy between earlier observational claims of turbulence as a major heating mechanism in cool-core clusters and subsequent simulations showing its subdominant role.

Method: Conducted three-dimensional hydrodynamic simulations including AGN feedback and pre-existing turbulence at levels observed in Perseus. The simulations model both AGN-driven processes (bubbles, shocks) and existing turbulence.

Result: Pre-existing turbulence dominates velocity dynamics but AGN contributions remain significant. Turbulent heating is found to be less than radiative cooling rates, especially in the core, indicating it cannot alone prevent cooling flows.

Conclusion: Turbulent heating alone isn't sufficient to offset radiative cooling in cool-core clusters. AGN feedback plays an essential role, necessitating combined mechanisms for effective heating.

Abstract: Feedback from active galactic nuclei (AGN) is believed to play a significant role in suppressing cooling flows in cool-core (CC) clusters. Turbulence in the intracluster medium (ICM), which may be induced by AGN activity or pre-existing motions, has been proposed as a potential heating mechanism based on analysis of Chandra X-ray surface brightness fluctuations. However, subsequent simulation results have found the subdominant role of turbulence in heating the ICM. To investigate this discrepancy, we perform three-dimensional hydrodynamic simulations of a Perseus-like cluster including both AGN feedback and pre-existing turbulence, which is stirred to the observationally constrained level in the Perseus cluster. Our results indicate that, although the velocity field is dominated by the pre-existing turbulence, AGN heating through bubbles and shocks remains significant. More importantly, analysis of the velocity structure function and the energy power spectrum shows that the turbulent heating rate is smaller than the radiative cooling rate, especially in the cluster core. Our results offer insights relevant for recent XRISM observations and indicate that turbulent heating alone cannot offset radiative cooling in CC clusters.

</details>


### [13] [Synchrotron Self-Compton Model of TeV Afterglows in Gamma-Ray Bursts](https://arxiv.org/abs/2511.23349)
*Edilberto Aguilar-Ruiz,Ramandeep Gill,Paz Beniamini,Jonathan Granot*

Main category: astro-ph.HE

TL;DR: The paper presents a semi-analytical model for GRB afterglows that efficiently calculates synchrotron self-Compton (SSC) emission, enabling MCMC fitting. Applied to GRB 190114C, it reveals a blast wave with E_k,iso ~1e55 erg in a sub-wind (k=1.67) circumburst medium.


<details>
  <summary>Details</summary>
Motivation: Accurate SSC modeling is computationally intensive, hindering MCMC use. Existing analytical methods are inadequate, needing improved treatments of Klein-Nishina effects and Compton-Y parameters.

Method: Developed a semi-analytic framework incorporating adiabatic cooling, photon escape, equal-arrival-time surfaces, and Klein-Nishina effects. Compared results with kinetic models to validate accuracy.

Result: Model accurately replicates SSC emission comparable to numerical methods. Applied to GRB 190114C data: found E_k,iso =9.1e54 erg and k=1.67, indicating a less dense external medium than standard wind profiles.

Conclusion: The model enables efficient SSC parameter estimation for GRB afterglows. The observed k<2 suggests non-steady progenitor winds or ISM interaction, challenging canonical wind assumptions.

Abstract: The detection of a very-high-energy TeV spectral component in the afterglow emission of gamma-ray bursts (GRBs) has opened a new probe into the energetics of ultra-relativistic blast waves and the nature of the circumburst environment in which they propagate. The afterglow emission is well understood as the synchrotron radiation from the shock-accelerated electrons in the medium swept up by the blast wave. The same distribution of electrons also inverse-Compton upscatters the softer synchrotron photons to produce the synchrotron self-Compton (SSC) TeV emission. Accurate modeling of this component generally requires a computationally expensive numerical treatment, which makes it impractical when fitting to observations using Markov Chain Monte Carlo (MCMC) methods. Simpler analytical formalisms are often limited to broken power-law solutions and some predict an artificially high Compton-Y parameter. Here we present a semi-analytic framework for a spherical blast wave that accounts for adiabatic cooling and expansion, photon escape, and equal-arrival-time-surface integration, in addition to Klein-Nishina effects. Our treatment produces the broadband afterglow spectrum and its temporal evolution at par with results obtained from more sophisticated kinetic calculations. We fit our model to the afterglow observations of the TeV bright GRB\,190114C using MCMC, and find an energetic blast wave with kinetic energy $E_{k, \rm iso} = 9.1^{+7.41}_{-3.13} \times 10^{54} \, \rm erg$ propagating inside a radially stratified external medium with number density $n(r)\propto r^{-k}$ and $k=1.67^{+0.09}_{-0.10}$. A shallower external medium density profile ($k<2$) departs from the canonical approximation of a steady wind ($k=2$) from the progenitor star and may indicate a non-steady wind or a transition to an interstellar medium.

</details>


### [14] [Variation of Microphysical Parameters in Reverse-shock Scenario](https://arxiv.org/abs/2511.23426)
*Nissim Fraija,Boris Betancourt-Kamenetskaia,Antonio Galván,Maria Dainotti*

Main category: astro-ph.HE

TL;DR: This paper analyzes how varying microphysical parameters influence light curves and closure relations from synchrotron-self Compton processes in GRB external reverse shocks, demonstrating their ability to mimic plateau phases and steeper decay indices. The model is tested against Fermi-LAT and very high-energy GRB data using MCMC simulations.


<details>
  <summary>Details</summary>
Motivation: To explore how time-varying microphysical parameters (previously assumed constant) affect energy distribution between particles and magnetic fields in GRB relativistic shocks, and their observational signatures in multiwavelength data.

Method: Derive light curves and closure relations for SSC from external reverse shocks in both homogeneous and stellar-wind media, considering thick- and thin-shell regimes. Apply MCMC simulations to compare model predictions with 2FLGC and very high-energy GRB data.

Result: Varying microphysical parameters can produce plateau phases and steeper decay indices than high-latitude emission alone. The model successfully reproduces observed spectral and temporal index evolution in Fermi-LAT data.

Conclusion: Temporal and spectral variations in GRBs may arise from microphysical parameter evolution rather than high-latitude effects alone, providing new insights into energy partition dynamics in relativistic shocks.

Abstract: Gamma-ray bursts (GRBs), among the most compelling astrophysical phenomena, are potential candidates for exploring the evolution of energy distribution among magnetic fields and particles through multiwavelength observations. The fraction of energy transferred between particles and the magnetic field is governed by microphysical parameters, typically assumed to be constant during relativistic shocks but may in fact vary with time. In this work, we derive the light curves and closure relations (CRs) of the synchrotron-self Compton (SSC) process from the external reverse shock (RS) with variations of microphysical parameters in a homogeneous and stellar-wind medium. We consider the evolution of the RS in the thick- and thin-shell regimes. We demonstrate that, depending on the microphysical parameters, this process can mimic plateau phases and produce temporal decay indices steeper than those predicted by high-latitude emission alone. The current model is employed to examine the evolution of the spectral and temporal indices of GRBs reported in the Second Fermi-LAT Gamma-ray Burst Catalog (2FLGC) and bursts detected at very high energies, using Markov Chain Monte Carlo (MCMC) simulations.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [15] [Nuclear Detonations as Probes of Hidden Superluminal Sectors](https://arxiv.org/abs/2511.21793)
*Karl Svozil*

Main category: hep-ph

TL;DR: The paper proposes a theoretical framework using nuclear explosions and high-energy collisions to explore hidden sectors with superluminal propagation, unifying exotic concepts like extra-dimensional shortcuts and trans-metric shockwaves under a causal constraint framework.


<details>
  <summary>Details</summary>
Motivation: The motivation is to exploit the extreme stress-energy gradients from nuclear explosions to excite hidden sector modes not detectable by conventional experiments, leveraging analogies between acoustic and electromagnetic phenomena.

Method: The framework stratifies physical descriptions into three layers: a substrate layer, hidden-sector fields with extended causal cones, and the Standard Model. It unifies exotic proposals into a single formalism, analyzing causality constraints and observational limits.

Result: The work provides a structured approach to explore physics beyond the emergent metric by combining high-energy regimes, suggesting nuclear explosions and collisions as new probes for hidden sectors.

Conclusion: Such a framework could complement traditional probes by offering a new avenue to search for physics beyond the Standard Model through extreme high-energy events, despite challenges in maintaining causality and observational consistency.

Abstract: We propose a highly speculative phenomenological framework in which nuclear detonations and high-energy collisions serve as probes for hidden sectors with effective superluminal propagation. Motivated by analogies between acoustic and electromagnetic phenomena, we stratify the physical description into three layers: a fundamental ``substrate'' layer, hidden-sector fields with extended causal cones, and the emergent Standard Model. We posit that the extreme, macroscopic stress-energy gradients generated by nuclear explosions might excite substrate or hidden modes that remain kinematically inaccessible to standard laboratory probes. This work unifies various exotic proposals -- including extra-dimensional shortcuts and trans-metric shockwaves -- into a single formalism, discussing the constraints imposed by causality and observation while outlining how such distinct high-energy regimes could complement one another in searching for physics beyond the emergent metric.

</details>


### [16] [A Comprehensive Study of WIMP Models Explaining the Fermi-LAT Galactic Center Excess](https://arxiv.org/abs/2511.21808)
*Chuiyang Kong,Mattia Di Mauro*

Main category: hep-ph

TL;DR: The study explores WIMP models that can explain the Galactic Center excess (GCE) of GeV γ rays while meeting cosmological and experimental constraints. It categorizes models into hadronic, leptonic, and mixed types, finding viable solutions only in narrow resonant regions with finely tuned parameters, particularly emphasizing leptophilic vectors and pseudoscalar portals as the most robust options.


<details>
  <summary>Details</summary>
Motivation: To determine if WIMP models can explain the GCE while satisfying observed relic density, direct detection (DD), and indirect detection (ID) constraints, addressing the uncertain origin of the GCE.

Method: Surveyed WIMP models across hadronic (Higgs/simplified scalar/vector), leptonic ($U(1)_{L_i-L_j}$), and mixed ($U(1)_{B-L}$, Z-portal) classes. Analyzed exclusion by DD and dwarf-spheroidal γ-ray limits, focusing on resonant funnels where DM mass matched half the mediator mass, and small portal couplings. Evaluated UV-complete and simplified models, considering mediator types (scalar, vector, axial) and particle properties (complex scalar, pseudoscalar, Dirac).

Result: Most models require fine-tuning: Only narrow resonant mass regions (e.g., m_DM ≈ m-med/2) with very small couplings (~10^-4) survive. Hadronic Higgs portals viable near 62.5 GeV; Dirac DM disfavored except in pseudoscalar cases. Leptonic models with L_μ−L_e and mixed B−L portals viable with thermal cross-sections. Leptophilic vectors and pseudoscalar portals emerge as most robust solutions.

Conclusion: Viable WIMP explanations for GCE are tightly constrained to resonant funnels with finely tuned parameters. Leptophilic vectors and pseudoscalar portals are most promising, suggesting future searches should prioritize these models under strict parameter ranges.

Abstract: The Galactic Center excess (GCE) of GeV $γ$ rays may hint at dark matter (DM), yet its origin remains debated. Motivated by this, we survey weakly interacting massive particle (WIMP) models that can fit the GCE while satisfying relic-density, direct-detection (DD), and indirect-detection (ID) bounds. We group candidates into hadronic (Higgs portals; simplified scalar/vector mediators), leptonic ($U(1)_{L_i-L_j}$), and mixed ($U(1)_{B-L}$, $Z$-portal) classes. Across all cases, present DD and dwarf-spheroidal $γ$-ray limits exclude wide regions, leaving mainly narrow resonant funnels with $m_{\rm DM}\!\simeq\! m_{\rm med}/2$ and portal couplings $\ll 1$. In hadronic setups, scalar and vector Higgs portals survive only in a thin strip near $m_h/2\simeq62.5$ GeV with portal couplings $\sim 10^{-4}$, while the Dirac Higgs and $Z$ portals are essentially excluded. The UV-complete vector Higgs portal retains resonant bands whose viable portal strength depends on the mixing angle. Simplified scalars allow small windows for complex-scalar or vector DM; Dirac DM is strongly disfavored, whereas a pseudoscalar with Dirac DM remains viable over a broader parameter range. For a simplified $Z'$ mediator, a pure vector coupling leaves only a marginal region, while pure axial is excluded by DD/ID bounds. In leptonic scenarios, inverse-Compton emission is essential: $L_μ-L_e$ (and, to a lesser extent, $B\!-\!L$) fits the GCE with near-thermal cross sections, while $L_μ-L_τ$ is disfavored. Overall, viable WIMP explanations are constrained to finely tuned resonant regime, with leptophilic vectors and pseudoscalar portals emerging as the most robust options.

</details>


### [17] [$B\rightarrow K + invisible$ in a model with axion-like particles](https://arxiv.org/abs/2511.21811)
*Xiyuan Gao*

Main category: hep-ph

TL;DR: The Belle-II anomaly in $Bightarrow K +$ invisible decays is analyzed through the DFSZ model, highlighting two theoretical updates: dominant two-loop contributions over one-loop and an apparent non-decoupling effect requiring beyond-renormalizable interactions for accurate low-energy descriptions.


<details>
  <summary>Details</summary>
Motivation: To address the Belle-II anomaly and understand the $B	o Ka$ ALP signal within a UV-complete framework.

Method: Analyzed the $b	o sa$ decay amplitude in the DFSZ model, evaluating two-loop diagrams and identifying an apparent non-decoupling effect absent in standard renormalizable ALP interactions.

Result: Two-loop contributions dominate one-loop effects, and an unexpected non-decoupling behavior requires non-renormalizable terms for precise low-energy theory.

Conclusion: Renormalizable ALP interactions are insufficient; beyond-standard low-energyEffective Field Theories (EFTs) must include certain non-renormalizable terms to capture observed phenomena.

Abstract: The localized excess of $B\rightarrow K + \textit{invisible}$ events reported by Belle-II is commonly interpreted as a signal for $B\to Ka$, where $a$ is an axion-like particle (ALP). In these proceedings, we summarize two theoretical updates regarding the $b\to sa$ decay amplitude within a minimal UV-complete model for invisible ALPs, namely the DFSZ model. (i) Contributions from certain two-loop diagrams can dominate the one-loop ones and are thus relevant for phenomenology. (ii) Although not unique, a rare feature -- which we refer to as apparent non-decoupling -- emerges in the light effective theory. The renormalizable ALP interactions fail to capture this behavior and therefore are incomplete as low-energy effective descriptions.

</details>


### [18] [Chasing the muon EDM to constrain the SMEFT and UV models](https://arxiv.org/abs/2511.21828)
*Kuldeep Deka,Marta Losada,Yosef Nir*

Main category: hep-ph

TL;DR: The paper discusses the discovery potential of a muon EDM experiment with sensitivity at 6×10^-23 e cm, exploring CP-violating operators in SMEFT and three UV completions. It identifies optimal parameter regions for each model and highlights the complementary role of collider measurements like Γ(h→μμ).


<details>
  <summary>Details</summary>
Motivation: To explore the discovery potential of an upcoming muon EDM experiment that is three to four orders of magnitude more sensitive than current limits, and to understand its implications for beyond-Standard Model physics.

Method: Analysis of dimension-six CP-violating operators in SMEFT, focusing on dipole and four-fermion terms. UV completions studied include vector-like leptons, heavy vector bosons with off-diagonal couplings, and a two-Higgs doublet model. Parameter space regions uniquely probed by the experiment are identified.

Result: The muon EDM experiment could probe new physics scales up to ~10 TeV. For VLLs, Γ(h→μμ) measurements offer comparable sensitivity. Four-fermion operators show best sensitivity in regions far from minimal flavor violation.

Conclusion: The proposed muon EDM experiment offers unique sensitivity to CP violation in various BSM scenarios, complementing collider searches. New physics at the 10 TeV scale is within reach, emphasizing the importance of muon EDM and related collider observables.

Abstract: Following a proposal for an experiment with sensitivity to an electric dipole moment (EDM) of the muon $d_μ$ of order $6\times10^{-23}\ e$ cm, three to four orders of magnitude below the current bound, but still seven orders of magnitude above the current bound on the EDM of the electron $d_e$, we explore the discovery potential of such an experiment. Within the dimension-six CP violating operators of the Standard Model effective field theory (SMEFT), we identify two dipole operators where $d_μ$ has the strongest sensitivity, and four classes of four-fermion operators where it has the best sensitivity for regions of parameter space that are far from minimal flavor violation. We further consider three UV completions: vector-like leptons (VLLs), heavy vector boson with off-diagonal leptonic couplings, and two Higgs doublet model. For each, we identify the region in parameter space that will be uniquely explored by the proposed $d_μ$ experiment. In case of VLLs, we also find measurements of $Γ(h \rightarrow μμ)$ offer competitive sensitivity, highlighting the complementary role of collider observables. Generically, the potential reach is to ${\cal O}(10\ {\rm TeV})$ scale of new physics.

</details>


### [19] [Axion couplings in Orbifold GUTs](https://arxiv.org/abs/2511.21830)
*Prateek Agrawal,Michael Nee,Mario Reig*

Main category: hep-ph

TL;DR: The paper analyzes axion-gauge boson couplings in higher-dimensional orbifold GUTs, showing that axion-photon coupling ratios remain below or equal to QCD axion values due to mass constraints from boundary instantons.


<details>
  <summary>Details</summary>
Motivation: To understand how axion couplings behave in higher-dimensional GUT frameworks inspired by string theory, particularly focusing on how their properties are influenced by embedding in the UV gauge group rather than geometric or symmetry reduction factors.

Method: Examines two types of axions in orbifold GUTs—the bulk ones and brane-localised ones. Compares their photon couplings and mass generation mechanisms, considering the impact of gauge coupling unification and boundary instantons.

Result: Bulk axions match 4D GUT predictions (coupling to QCD and photons), while brane-localised axions can decouple from QCD but gain large masses if gauge couplings are unified, ensuring that the ratio $g_{aγγ}/m_a$ stays below QCD axion limits.

Conclusion: Orbifold GUTs maintain the QCD axion-like coupling-mass relation for all axions due to mass suppressions from boundary instantons, aligning with 4D unified theories despite higher-dimensional constructs.

Abstract: We consider the coupling of axions to gauge bosons in higher-dimensional Grand Unified Theories (GUT) inspired by string theory constructions with D-branes on orbifold singularities, the so-called orbifold GUTs. Due to their topological properties, axion couplings to gauge bosons are independent of the gauge symmetry reduction mechanism and the background geometry -- they only depend on the embedding of the SM into the UV gauge group. There are two kinds of axions in this class of theories: axions coming from gauge fields in the bulk and axions localised on the boundaries. The axion-photon coupling for the bulk axions coincide with the results from 4-dimensional GUTs, where axions which couple to photons necessarily couple also to QCD. The brane-localised axions can couple to photons independently of the QCD coupling, but if gauge couplings are approximately unified, the axions get large masses from unsuppressed instantons on the boundaries. This means that the ratio $g_{aγγ}/m_a$ is below (or equal to) the QCD axion value for all axions in orbifold GUTs, as is the case for unified theories in 4d.

</details>


### [20] [Di-Higgs to 4b with Bayesian inference: improving simulation estimates](https://arxiv.org/abs/2511.21832)
*Ezequiel Alvarez,Leandro Da Rold,Manuel Szewc,Alejandro Szynkman,Santiago Tanco,Tatiana Tarutina*

Main category: hep-ph

TL;DR: The paper introduces a Bayesian mixture model to improve di-Higgs production measurement in the four-bottom channel by addressing QCD background challenges and simulation inaccuracies. It enhances signal detection through integrated inference of signal/background fractions and shapes, using a combined likelihood of kinematic discriminators and jet flavour scores. The method self-calibrates parameters, improving sensitivity and credibility intervals.


<details>
  <summary>Details</summary>
Motivation: To overcome overwhelming QCD backgrounds and simulation mismatches in di-Higgs detection, enabling more accurate signal extraction and robust statistical inference in high-energy physics experiments.

Method: A Bayesian mixture model uses a combined likelihood of a 1D kinematic discriminator and per-jet flavour scores, accounting for correlations via kinematic bins. Monte Carlo simulations provide weak Dirichlet priors, which are adjusted by posterior inference to correct biases and improve calibration.

Result: The method corrects biased priors, provides calibrated credible intervals for signal counts, and improves ROC/AUC compared to baseline methods. It effectively handles mismatches between simulated data and observed results.

Conclusion: Bayesian inference offers a powerful approach to extract signals from complex backgrounds, self-calibrating parameters and increasing sensitivity in di-Higgs searches despite imperfect simulations.

Abstract: Measuring di-Higgs production in the four-bottom channel is challenged by overwhelming QCD backgrounds and imperfect simulations. We develop a Bayesian mixture model that simultaneously infers signal and background fractions and their individual shapes directly in the signal region. The likelihood is a nuanced combination of a one-dimensional kinematic discriminator and per-jet flavour scores; with their correlations incorporated via kinematic bins. Monte Carlo informs weak Dirichlet priors, while the posterior adjusts to the interplay of the model, priors and observed data. Using pseudo-data simulated with standard tools and with controlled mismatches, we show that the method corrects biased priors, delivers calibrated 68-95% credible intervals for the signal count, and improves dataset-level ROC/AUC relative to simple cut-and-count baselines. This study highlights how Bayesian inference can harvest information present in the signal region and self-calibrate model parameters, providing a robust route to increased sensitivity in di-Higgs searches.

</details>


### [21] [High Mass Dark Matter Searches With the High Speed Flux From the Large Magellanic Cloud](https://arxiv.org/abs/2511.21841)
*Nassim Bozorgnia,Joseph Bramante,Andrew Buchanan*

Main category: hep-ph

TL;DR: Proper modeling of the local dark matter velocity distribution, especially considering the dynamics of the Large Magellanic Cloud (LMC) and Milky Way, is crucial for accurate heavy dark matter (mass > TeV) detection. New computational techniques improve flux calculations and detector responses, with LMC's influence analyzed using data from Ohya Mine and Skylab experiments.


<details>
  <summary>Details</summary>
Motivation: Traditional Maxwellian halo models may inadequately represent high-velocity dark matter streams from the LMC-Milky Way interaction, leading to incorrect bounds on heavy dark matter. Current cosmic ray and magnetic monopole searches need revised flux modeling.

Method: Developed advanced computational methods to model dark matter velocity distributions considering LMC dynamics. Applied these to analyze cosmic ray/magnetic monopole data from Ohya Mine (plastic etch detectors) and Skylab Space Station experiments.

Result: LMC's gravitational influence creates high-velocity dark matter streams that significantly alter detector flux predictions, tightening or shifting exclusion limits for heavy dark matter compared to standard models.

Conclusion: Future heavy dark matter searches must account for galactic substructure like the LMC to avoid biased results. Improved velocity distribution modeling is essential for interpreting experimental data accurately.

Abstract: As the hunt for dark matter progresses, recently there have been advances in the search for heavy dark matter with a mass well above a TeV. We show the importance of properly modeling the local dark matter velocity distribution, beyond the standard Maxwellian halo model, and in particular how the dynamics of the Large Magellanic Cloud and Milky Way may impact heavy dark matter searches. We introduce some new computational techniques for accurately computing the dark matter flux and the associated detector response. As a specific example, we examine the effect of the Large Magellanic Cloud on heavy dark matter bounds obtained from experiments searching for cosmic rays and magnetic monopoles using plastic etch detectors at the Ohya Mine and aboard the Skylab Space Station.

</details>


### [22] [New Physics Searches at the LHC through Event-based Anomaly Detection and Development of ADFilter Web-tool](https://arxiv.org/abs/2511.21869)
*Wasikul Islam,Sergei Chekanov,Nicholas Luongo*

Main category: hep-ph

TL;DR: The paper introduces ADFilter, a web-based tool using unsupervised machine learning (specifically autoencoders) for anomaly detection in LHC collision events. It aims to improve new physics searches by reinterpreting existing data and compares the method to supervised approaches using heavy resonance examples.


<details>
  <summary>Details</summary>
Motivation: To enhance model-agnostic searches for new physics at the LHC by leveraging event-based anomaly detection, which avoids biases from specific theoretical models and potentially improves exclusion limits.

Method: Developed ADFilter, which processes LHC events with autoencoder-based neural networks to calculate loss distributions for anomaly detection. Conducted a comparative study against supervised ML techniques using heavy resonances decaying into Higgs bosons as a test case.

Result: Demonstrated the effectiveness of ADFilter through real examples, showing potential to significantly improve exclusion limits. Comparative analysis highlights advantages and scenarios where anomaly detection outperforms supervised methods.

Conclusion: Unsupervised anomaly detection via ADFilter offers a promising model-agnostic approach for reinterpreting LHC data, with clear advantages in certain physics searches compared to traditional supervised methods.

Abstract: This work presents advancements in model-agnostic searches for new physics at the Large Hadron Collider (LHC) through the application of event-based anomaly detection techniques utilizing unsupervised machine learning. We discuss the advantages of the anomaly detection approach, as demonstrated in a recent ATLAS analysis, and introduce ADFilter, a web-based tool designed to process collision events using autoencoders based on deep unsupervised neural networks. ADFilter calculates loss distributions for input events, aiding in determining the degree to which events can be considered anomalous. Real-life examples are provided to demonstrate how the tool can be used to reinterpret existing LHC results, with the goal of significantly improving exclusion limits. Furthermore, we present a comparative study between anomaly detection and supervised machine learning techniques, using the search for heavy resonances decaying into two or more Higgs bosons as a representative case to demonstrate the application and effectiveness of these methods.

</details>


### [23] [Model-independent probes of CP violation in the heavy scalar sector at muon colliders](https://arxiv.org/abs/2511.21874)
*Qianxi Li,Ying-nan Mao,Kechen Wang*

Main category: hep-ph

TL;DR: The paper proposes a model-independent test for CP violation in the scalar sector using a heavy neutral scalar h₂ at future muon colliders. By observing the process VV→h₂→Zh₁, they aim to establish CP violation through non-zero couplings c₂ and c₁₂.


<details>
  <summary>Details</summary>
Motivation: CP violation in the scalar sector is an important topic in particle physics, and existing methods are model-dependent. This test provides a model-independent approach to detect CP violation using observable processes at muon colliders.

Method: The authors analyze the VBF production of h₂ followed by its decay into Zh₁ at muon colliders with 3 and 10 TeV energies. They simulate signal and background processes to determine discovery sensitivities across parameter spaces of coupling constants c₂ and c₁₂ for various h₂ masses.

Result: Simulations show expected discovery sensitivities for different h₂ mass hypotheses and coupling parameter values, demonstrating the feasibility of detecting CP violation in the scalar sector through this method.

Conclusion: The proposed test offers a robust, model-independent way to probe CP violation in scalar sectors using muon collider data, highlighting the potential of future colliders inBeyond Standard Model physics studies.

Abstract: We propose a model-independent test of CP violation in the scalar sector. We consider a heavy neutral scalar $h_2$ with tree-level couplings at the $h_2 V V$ and $h_2 h_1 Z$ vertices (with $V=W^{\pm},Z$), alongside the 125~GeV SM-like Higgs boson $h_1$. At future muon colliders (MuC), we exploit vector-boson-fusion (VBF) production of $h_2$ followed by the decay $h_2 \to Z h_1$. In our framework, observing the single process $V V \to h_2 \to Z h_1$ implies both relevant couplings are nonzero, which is sufficient to establish CP violation in the scalar sector. We simulate signal and backgrounds at $\sqrt{s}=3~(10)$ TeV with integrated luminosity $L=0.9~(10)~\mathrm{ab}^{-1}$. We then present the expected discovery sensitivites across the $(c_2,c_{12})$ parameter space (with the coupling parameters $c_{2}$ and $c_{12}$ defined in the text) for multiple $m_{h_2}$ hypotheses.

</details>


### [24] [Energy-momentum tensor form factor D(t) of proton and neutron](https://arxiv.org/abs/2511.21916)
*Andrea Mejia,Peter Schweitzer*

Main category: hep-ph

TL;DR: The paper examines the electromagnetic effects on the energy-momentum tensor form factor $D(t)$ of nucleons, showing that proton and neutron $D(t)$ become indistinguishable at very low $(-t)$ despite divergences and sign changes from QED effects. The model aligns with lattice data and explains mass differences.


<details>
  <summary>Details</summary>
Motivation: To clarify the role of electromagnetic interactions in modifying the $D(t)$ form factor of charged protons compared to neutrons, and to understand the convergence of their form factors at low momentum transfers.

Method: Extends Białynicki-Birula's classical proton model to neutrons using residual nuclear forces as a mean field approach. Analyzes $D(t)$ behavior under electromagnetic influences, scaling forces to match lattice QCD data for nucleons up to $(-t) \lesssim 1$ GeV².

Result: Proton-neutron mass differences are accurately explained. The model reproduces lattice data on nucleon $D(t)$ incorporating QED effects. At very low $(-t) \approx 10^{-4}$ GeV², proton and neutron $D(t)$ form factors converge and become indistinguishable.

Conclusion: Electromagnetic effects cause dramatic changes in $D(t)$ for protons but lead to proton-neutron $D(t)$ equivalence at extremely low momentum transfers. Experimentally detectable differences are unlikely in the near future due to this convergence.

Abstract: The energy-momentum tensor (EMT) form factor $D(t)$ is
  finite and negative in hadronic models and lattice QCD when only
  strong forces are included. However, when electromagnetic forces are
  considered, the $D(t)$ of charged hadrons undergoes a dramatic change:
  at small $t$, it changes sign and diverges like $1/\sqrt{-t}$ as shown
  for the proton in the classical model by Białynicki-Birula based on
  residual nuclear forces which can be understood as a mean field approach.
  We construct an analogous neutron model and show that this framework
  accurately explains the electromagnetic proton-neutron mass difference.
  We demonstrate that, after appropriately rescaling the residual nuclear
  forces, the model can reproduce lattice data on the nucleon $D(t)$ up to
  $(-t)\lesssim 1\,$GeV$^2$ as well as QED effects.
  Based on this realistic model description, we show that the proton and
  neutron $D(t)$ form factors are practically indistinguishable down to
  $(-t) \approx 10^{-4}\rm GeV^2$ far below what can currently be accessed
  experimentally. We conclude that in the foreseeable future the $D(t)$
  form factors of proton and neutron will practically
  look the same in experiments and phenomenology.

</details>


### [25] [QCD axions and domain walls in dense matter under compact stellar conditions](https://arxiv.org/abs/2511.21995)
*Zhen-Yan Lu,Shu-Peng Wang,Qi Lu,Bo-Nan Zhang,Marco Ruggieri*

Main category: hep-ph

TL;DR: The study examines how temperature and chemical potential influence QCD topology and axion properties in dense stellar environments under charge neutrality and beta equilibrium. Key findings include a suppressed axion mass near the chiral phase transition and a sevenfold enhancement of the axion self-coupling near critical conditions, which may significantly affect compact star dynamics.


<details>
  <summary>Details</summary>
Motivation: To understand how dense compact stellar environments impact QCD matter's topology and axion properties, considering both with and without charge neutrality conditions.

Method: Analyzed the effects of temperature and finite chemical potential on QCD topology and axion parameters, particularly focusing on the critical behavior of the chiral phase transition.

Result: Topological susceptibility and axion properties are sensitive to chiral phase transitions. Axion mass decreases near transition points, while self-coupling peaks (over 7x enhancement at T~70 MeV, μ~346 MeV).

Conclusion: Enhanced axion self-coupling near critical points suggests axion-mediated interactions could influence compact star structure and stability, offering new astrophysical implications.

Abstract: In compact stellar environments, the stability of dense QCD matter requires the simultaneous fulfillment of charge neutrality and beta equilibrium. In this work, we study how temperature and finite chemical potential affect QCD topology and axion properties within this medium, analyzing both cases with and without the charge neutrality condition. Our results show that the topological susceptibility and axion properties are highly sensitive to the critical behavior of the chiral phase transition in both cases. In particular, the axion mass is strongly suppressed near the transition, while the axion self-coupling constant develops a pronounced peak whose magnitude depends on the temperature and density of the medium. Remarkably, around the critical point at $T\simeq70$ MeV and $μ\simeq346$ MeV, the self-coupling constant is enhanced by more than a factor of seven compared to its vacuum value, a feature that to the best of our knowledge has not been reported in previous studies. Such a strong amplification at the phase boundary indicates that axion-mediated interactions could play an important role in shaping the structure and stability of compact stars, with potential implications for their evolution and observable astrophysical signatures.

</details>


### [26] [Systematical study of exotic $1^{-+}$ tetraquark spectroscopy](https://arxiv.org/abs/2511.22111)
*Kai Xu,Zheng Zhao,Nattapat Tagsinsit,Attaphon Kaewsnod,Ayut Limphirat,Christoph Herold,Yupeng Yan*

Main category: hep-ph

TL;DR: The study calculates masses and decay properties of $1^{-+}$ tetraquark states using a constituent quark model, predicts their ground state masses across three sectors, and discusses their compatibility with observed exotic states, excluding $η_1(1855)$ as a compact tetraquark.


<details>
  <summary>Details</summary>
Motivation: To investigate the existence and characteristics of exotic $1^{-+}$ compact tetraquark states, understand their decay mechanisms, and compare theoretical predictions with experimental observations.

Method: Uses a constituent quark model with Cornell-like potentials, Breit-Fermi-derived spin interactions, and parameters from prior studies. Computes masses via potential-based calculations and decay widths using the rearrangement mechanism for various two-body decay channels.

Result: Predicted ground state masses: 1.9 GeV (light sector), 4.2 GeV (charmonium-like), 6.6 GeV (fully-charm). Decay width ratios calculated across multiple channels. The $η_1(1855)$ is deemed incompatible with compact tetraquark models.

Conclusion: The model successfully predicts tetraquark masses in different sectors and provides insights into their decay dynamics, suggesting alternatives to $η_1(1855)$ as tetraquark candidates for future observations.

Abstract: The masses of exotic quantum-number $1^{-+}$ compact tetraquark states are calculated in a constituent quark model (CQM), where Cornell-like potentials are employed as the central potentials, spin-spin and spin-orbit coupling derived from Breit-Fermi interaction are treated as hyperfine corrections, and model parameters are taken from previous works. The ground state $1^{-+}$ P-wave tetraquarks are predicted at 1.9, 4.2, and 6.6~GeV for the light, charmonium-like, and fully-charm sectors, respectively.
  The decay width ratios of $1^{-+}$ tetraquark states are calculated for possible strong two-body decay channels within the rearrangement mechanism, including $ωh_1$ and $ηf_1$ for isospin $I=0$ light tetraquarks; $ρh_1$ and $πf_1$ for isospin $I=1$ light tetraquarks; $π/η+χ_{c1}$ and $ρ/ω+ h_c$ for charmonium-like tetraquarks; and $η_c χ_{c1}$ and $J/ψh_c$ for fully-charm tetraquarks. The theoretical results are compared with the observed exotic $1^{-+}$ states, and possible channels for observing $1^{-+}$ tetraquark states are also discussed. The work suggests that $η_1(1855)$ is unlikely to be a compact tetraquark state.

</details>


### [27] [Topological production of charmonia with event-shape engineering in $pp$ collisions at $\sqrt{s} = 13$ TeV using PYTHIA8](https://arxiv.org/abs/2511.22190)
*Aswathy Menon Kavumpadikkal Radhakrishnan,Suraj Prasad,Neelkamal Mallick,Raghunath Sahoo*

Main category: hep-ph

TL;DR: This paper studies the production of J/ψ particles in proton-proton collisions at 13 TeV, focusing on how transverse spherocity—a measure of event shape—impacts prompt and nonprompt J/ψ production mechanisms in PYTHIA8 simulations. It highlights the role of underlying event multiple parton interactions in these processes and suggests spherocity as a useful selection tool for probing QCD dynamics.


<details>
  <summary>Details</summary>
Motivation: To investigate the influence of event topology (via transverse spherocity) on J/ψ production, validate QCD models using PYTHIA8, and provide a method for distinguishing hard QCD processes from softer backgrounds in high-energy collisions.

Method: Simulate prompt and nonprompt J/ψ production in pp collisions at 13 TeV using PYTHIA8. Reconstruct J/ψ via dielectron and dimuon decays in mid-/forward-rapidity. Analyze dependence on transverse spherocity, correlating it with the average number of multiple parton interactions (⟨N_mpi⟩) from underlying events.

Result: Transverse spherocity correlates with ⟨N_mpi⟩, showing its utility for event selection. The study reveals how spherocity can isolate QCD processes affecting J/ψ production, but experimental validation is needed.

Conclusion: Transverse spherocity is proposed as an experimental tool to disentangle QCD dynamics in J/ψ production, linking event topology to underlying multiple parton interactions. Results underscore the need for experimental verification to confirm these simulation-based findings.

Abstract: The production of heavy quarks (charm and beauty) in high-energy hadronic and nuclear collisions provides an excellent testing ground for the theory of strong interaction and validates models based on quantum chromodynamics. In this work, prompt and nonprompt production of $\rm{J/}ψ$ in $pp$ collisions at $\sqrt{s}=13$ TeV are studied as a function of transverse spherocity using PYTHIA8. $\rm{J/}ψ$ is reconstructed via its electromagnetic decay to dielectrons and dimuons, in mid- and forward-rapidity, respectively. Transverse spherocity, an event shape observable, is used to distinguish hard QCD events from the softer, isotropic ones. In PYTHIA8, the production of $\rm{J/}ψ$ can be influenced by the average number of multiple parton interactions ($\langle N_{\rm mpi} \rangle$), owing to the underlying events (UE), which have a dominant contribution to particle production at lower transverse momentum. Since transverse spherocity is correlated to $\langle N_{\rm mpi} \rangle$, this can serve as an experimentally available tool for event selection to study the underlying QCD processes influencing the prompt and nonprompt $\rm{J/}ψ$ production. This study reveals the correlation between heavy-flavor production dynamics and topological event selection in $pp$ collisions using PYTHIA8, whose relevance awaits experimental validation.

</details>


### [28] [Complete one-loop QED corrections to $D_s^+$ leptonic decays and impact on the CKM unitarity test](https://arxiv.org/abs/2511.22383)
*Teppei Kitahara,Jun Miyamoto,Kota Sasaki*

Main category: hep-ph

TL;DR: The paper derives complete one-loop electroweak and QED corrections for D_s^+ → ℓ^+ ν_ℓ decays, showing that proper inclusion of these corrections is crucial for aligning CKM unitarity tests with Standard Model predictions, highlighting QED correction precision as the main limitation.


<details>
  <summary>Details</summary>
Motivation: Recent charm-meson data and lattice results indicate a potential CKM unitarity violation. The study aims to resolve discrepancies by incorporating accurate radiative corrections, which are not adequately considered in existing analyses.

Method: Analytic derivation of one-loop EW and QED corrections, including short-distance terms beyond leading-log approximation (Sirlin factor) and long-distance soft-photon corrections with resummation tailored to experimental conditions.

Result: Including these corrections reduces tension between CKM unitarity tests and Standard Model. Current limitation is the precision of QED corrections in lattice simulations.

Conclusion: Accurate QED corrections must be prioritized in lattice simulations to robustly confirm CKM unitarity. Future improvements require enhanced computational methods accounting for QED effects in meson decays.

Abstract: Recently, a violation of the CKM unitarity condition has been reported in the latest charm-meson data, and the latest lattice results once the universal electroweak correction is taken into account. In this article, we analytically derive for the first time the complete one-loop electroweak (EW) and QED corrections to the $D_{s}^+ \to \ell^+ ν_\ell$ decays for $\ell = μ, τ$. Our analysis incorporates both short-distance EW-QED corrections, which are beyond the leading-logarithmic approximation (the so-called Sirlin factor), and long-distance soft-photon corrections depending on the maximum total energy of undetected photons with their resummation. Although the inclusive photon QED corrections to the meson leptonic decays are well known, they do not match the actual measurement circumstances in $D_s^+ \to μ^+ ν_μ$. We show that properly including these radiative corrections is essential to bring the second-column CKM unitarity tests into agreement with the Standard Model expectation. The study emphasizes that the current limiting factor in confirming CKM unitarity is the precision of QED corrections, and it points out that improving lattice simulations, taking the QED corrections into account, would be desired for a more robust confirmation.

</details>


### [29] [Lesser Green's Function and Chirality Entanglement Entropy via the In-Medium NJL Model](https://arxiv.org/abs/2511.22412)
*Seung-il Nam*

Main category: hep-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study chiral symmetry restoration in hot-and-dense quark matter using the von Neumann chirality entropy within the in-medium Nambu-Jona-Lasinio (NJL) model. Starting from the lesser Green function $G^{<}(k)$, the reduced correlator $C_L=P_LG^{<}P_L$ is constructed, and the associated entropy $S_χ=-\mathrm{Tr}[C_L\ln C_L+(1-C_L)\ln(1-C_L)]$ is introduced to quantify the entanglement between left- and right-handed quark sectors. The dynamical quark mass $M_q(T,μ_q)$ obtained from the gap equation exhibits the expected QCD-like phase structure: A second-order transition in the chiral limit and a smooth crossover for finite $m_q$. The chirality entropy $S_χ$ increases monotonically with temperature and chemical potential, approaching a maximal value as $M_q\to0$. We also explore the critical exponents and scaling behavior of those quantities, yielding $β_{S_χ}\simeq1$. This demonstrates that $S_χ$ serves as an information-theoretic probe for chiral symmetry restoration, linking dynamical mass generation to quantum entanglement in strongly interacting matter.

</details>


### [30] [Inclusive quarkonium photoproduction selection and the effect of pileup at the LHC](https://arxiv.org/abs/2511.22461)
*Jean-Philippe Lansberg,Kate Lynch,Ronan McNulty,Charlotte Van Hulse*

Main category: hep-ph

TL;DR: The paper discusses strategies for identifying inclusive quarkonium photoproduction in LHC pPb collisions, addressing challenges posed by varying collision environments and pileup effects.


<details>
  <summary>Details</summary>
Motivation: To address the lack of quarkonium photoproduction measurements at the LHC and to optimize selection criteria across different collision systems considering photon flux, luminosity, and pileup.

Method: Developed a selection strategy using pPb collisions with optimized photon flux and low pileup. Analyzed applicability in PbPb and pp collisions, evaluating pileup impacts on forward detectors and rapidity-gap efficiencies.

Result: Confirmed the viability of pPb collisions for quarkonium photoproduction studies. Identified that PbPb can use similar criteria, while pp requires adjustments due to higher pileup degrading forward detector selections and rapidity-gap efficiency.

Conclusion: The methodology provides a robust framework for LHC quarkonium studies, emphasizing system-dependent selection optimization to mitigate pileup challenges, especially in pp collisions.

Abstract: Measurements of inclusive quarkonium photoproduction provide strong constraints on the quarkonium production mechanism; however, this process has not yet been measured at the LHC. We summarise our previously developed selection strategy for isolating inclusive quarkonium photoproduction in pPb collisions at the LHC, which offer an optimal balance of photon flux, luminosity, and low pileup. We further examine the applicability of our selection criteria in different collision systems. While our method can be readily extended to PbPb collisions, pp collisions require additional care due to the significantly higher pileup. We discuss how pileup affects the selection criteria, highlighting that it substantially degrades forward-detector-based selections and reduces the efficiency of rapidity-gap requirements.

</details>


### [31] [Misalignment dynamics of Scalar Condensates with Yukawa coupling: Particle and Entropy Production](https://arxiv.org/abs/2511.22465)
*Nathan Herring,Daniel Boyanovsky*

Main category: hep-ph

TL;DR: The paper explores how a scalar condensate evolves in a potential landscape, focusing on misalignment dynamics with radiative corrections. It uses a Hamiltonian framework with Yukawa coupling to fermions, considering large N_f and adiabatic expansions. It suggests new states with specific particle distributions and discusses implications for physics and cosmology, including possible symmetry breaking and entropy as entanglement entropy.


<details>
  <summary>Details</summary>
Motivation: To address shortcomings in previous methods that used static effective potentials for misalignment dynamics, which are limited to near-equilibrium. The study aims to include radiative corrections dynamically and explore energy-conserving dynamics for phenomena like the strong CP problem, dark matter, and reheating.

Method: A renormalized Hamiltonian framework is used with Yukawa coupled scalars and fermions. Large N_f approximation focuses on fermionic contributions, leading to an effective potential instability. Adiabatic basis and expansion incorporate derivative terms and field renormalization, enabling analysis of particle production and dynamics beyond static potentials.

Result: Proposed asymptotic stationary states with particle distribution n_k ~ 1/k^6 and extensive entropy linked to entanglement. Resolved renormalization issues in initial value problems. New insights into symmetry breaking from dynamics without tree-level breaking and cosmological implications for dark matter and reheating.

Conclusion: Dynamic inclusion of radiative corrections via Hamiltonian methods offers deeper insights into misalignment dynamics. The results challenge assumptions about equilibrium conditions and highlight non-trivial states from quantum fluctuations, suggesting revised models for cosmological phenomena.

Abstract: Misalignment dynamics, the non-equilibrium evolution of a scalar (or pseudoscalar) condensate in a potential landscape, broadly describes a solution to the strong CP problem, a mechanism for cold dark matter production and (pre) reheating post inflation. Often, radiative corrections are included phenomenologically by replacing the potential by the effective potential which is a \emph{static quantity}, its usefulness is restricted to (near) equilibrium situations. We study the misalignment dynamics of a scalar condensate Yukawa coupled to $N_f$ fermions in a manifestly energy conserving, fully renormalized Hamiltonian framework. A large $N_f$ limit allows us to focus on the fermion degrees of freedom, which yield a negative contribution to the effective potential, a radiatively induced instability and ultraviolet divergent field renormalization. We introduce an adiabatic basis and an adiabatic expansion that embodies the derivative expansion in the effective action, the zeroth order is identified with the effective potential, higher orders account for the derivative expansion including field renormalization and describe profuse particle production. Energy conserving dynamics leads to the conjecture of emergent asymptotic highly excited stationary states with a distribution function $n_k(\infty)\propto 1/k^6$ and an extensive entropy which is identified with an entanglement entropy. Subtle aspects of renormalization associated with the initial value problem are analyzed and resolved. Possible new manifestations of asymptotic spontaneous symmetry breaking (SSB) as a consequence of the dynamics even in absence of tree level (SSB), and cosmological inferences are discussed.

</details>


### [32] [Pion photoproduction of nucleon excited states with Hamiltonian effective field theory](https://arxiv.org/abs/2511.22479)
*Yu Zhuge,Dan Guo,Zhan-Wei Liu,Derek B. Leinweber,Anthony W. Thomas*

Main category: hep-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Over the past few years, Hamiltonian effective field theory has been successfully applied to studies of nucleon and hyperon excited states. By discretizing the Hamiltonian in a finite volume, one can obtain the energy spectrum and compare it with the results calculated from lattice QCD. Through the analysis of experimental data, Hamiltonian effective field theory provides a framework that connects the finite-volume spectra from lattice QCD to infinite-volume scattering observables. The model independence of the approach is well preserved under the combined constraints from lattice QCD and experimental data. Building on these developments, recent works have attempted to extend HEFT to electromagnetic processes. Meanwhile, lattice QCD has also gradually advanced into the study of electromagnetic interactions. The combination of these analyses will undoubtedly deepen our understanding of light resonances.

</details>


### [33] [Extractions of the strong coupling from collider data without PDF refitting are biased](https://arxiv.org/abs/2511.22561)
*Stefano Forte,Juan Rojo,Roy Stegeman*

Main category: hep-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present an explicit demonstration that a determination of the strong coupling constant $α_s(m_Z)$ from deep-inelastic scattering and hadron collider data without a simultaneous determination of the parton distribution functions (PDFs) leads to a biased result for both the central value and the uncertainty, even in the ideal scenario (closure test) where there are no internal tensions between datasets and where theoretical calculations describe perfectly the experimental measurements. Specifically, we show that a determination of $α_s(m_Z)$ from a single process leads in general to a result that differs from the global best fit more than the value of $α_s(m_Z)$ that is actually favoured by this process.

</details>


### [34] [New Physics Searches via Beam Normal Spin Asymmetry in Bhabha Scattering](https://arxiv.org/abs/2511.22568)
*Aleksandr Pustyntsev,Muthubharathi S. Ramasamy,Marc Vanderhaeghen*

Main category: hep-ph

TL;DR: This paper analyzes the sensitivity of the beam normal spin asymmetry in Bhabha scattering to BSM mediators, highlighting a zero-crossing feature in the Standard Model that offers a background-free point for searches. Projected bounds suggest scalar and vector mediators can extend search ranges beyond current limits.


<details>
  <summary>Details</summary>
Motivation: To investigate the potential of the JLab polarized positron program in detecting BSM physics through the beam normal spin asymmetry, leveraging the unique zero-crossing property of the SM contribution.

Method: Examined the sensitivity of Bhabha scattering's beam normal spin asymmetry to scalar, vector, and axial-vector BSM mediators, using the SM zero-crossing angle as a clean search point.

Result: Projected bounds indicate significant extension of search ranges for scalar and vector mediators beyond existing constraints.

Conclusion: The proposed method using the zero-crossing feature enhances sensitivity to BSM mediators, particularly for scalar and vector interactions, offering improved experimental prospects for future searches.

Abstract: We examine the sensitivity of the beam normal spin asymmetry in Bhabha scattering to beyond the Standard Model (BSM) mediators, in the context of the JLab polarized positron program. A key property of this observable is that the Standard Model contribution exhibits a zero crossing at a fixed scattering angle, providing a clean, effectively background-free point for these searches. We consider scalar, vector and axial-vector mediators and present projected bounds, finding scalar and vector scenarios allowing to extend their search ranges significantly beyond existing constraints.

</details>


### [35] [Short-range production of three bottom mesons](https://arxiv.org/abs/2511.22590)
*Yong-Hui Lin,Hans-Werner Hammer,Ulf-G. Meißner*

Main category: hep-ph

TL;DR: This paper investigates three-body systems of B and B* mesons, showing they don't exhibit the Efimov effect and can be described using nonrelativistic effective field theory (NREFT) without three-body forces. Predictions for three-body point production rates are provided to experimentally probe B*-B interactions and test low-energy conformal symmetry predictions.


<details>
  <summary>Details</summary>
Motivation: To provide a reliable theoretical framework for describing three-body B-meson systems via NREFT with two-body input alone, and to offer experimental probes for B^{(*)}-ar{B}^{(*)} interactions and conformal symmetry testing.

Method: Leading-order NREFT calculations are used to predict three-body point production rates for B and B* meson systems.

Result: Predictions for three-body production rates that enable experimental studies of B*-B interactions and verification of predicted low-energy conformal symmetry.

Conclusion: The absence of Efimof effect validates NREFT's applicability, and the derived production rates offer both experimental tests for molecule interpretations of T_{bar{b}1} states and symmetry validation.

Abstract: Previous investigations of the three-body dynamics of $B$ mesons have shown that no Efimov effect arises in systems composed of three $B$ and $B^*$ mesons. This implies that the properties of such three-body systems can be described reliably in nonrelativistic effective field theory (NREFT) with two-body input alone, as three-body forces are strongly suppressed. In this work, we present leading-order predictions for the three-body point production rates of systems consisting of three $B$ and $B^*$ mesons. These predictions provide a novel way to experimentally probe the $B^{(*)}$-$\bar{B}^{(*)}$ interactions, which play a crucial role in the hadronic-molecule interpretation of the $T_{b\bar{b}1}(10610)$ and $T_{b\bar{b}1}(10650)$ states. Moreover, they provide a way to test the approximate conformal symmetry predicted for such systems at low energies experimentally.

</details>


### [36] [Less structure on $8$ Mpc scales from decaying sterile neutrino dark matter](https://arxiv.org/abs/2511.22638)
*María Dias Astros,Lukáš Gráf,Stefan Vogl*

Main category: hep-ph

TL;DR: The paper proposes a two-sterile-neutrino plus scalar singlet model where late decays of dark matter reduce matter power spectrum on small scales. The model avoids producing radiation-like particles, and parameters are constrained by relic density and observational data.


<details>
  <summary>Details</summary>
Motivation: Reduce the matter power spectrum's small-scale amplitude using late decaying dark matter without relying on scarce particle models, and link to observable phenomena.

Method: Investigate a model with two interacting sterile neutrinos and a scalar singlet. Analyze production via oscillations and dark sector interactions, non-diagonal Yukawa matrices enabling three-body decay to light sterile neutrinos. Calculate parameter space regions matching S₈ reduction and observational constraints.

Result: Identified parameter regions where decays reduce S₈ to observational levels, while satisfying relic density, X-ray, and Lyman-α forest constraints. The model provides a viable framework linking dark matter decay properties to cosmological observations.

Conclusion: The proposed model successfully bridges particle physics and cosmological observations, offering a mechanism to reconcile small-scale structure issues with dark matter physics through late decays without radiation byproducts.

Abstract: Late decays of dark matter to a lighter, warm dark matter component are a known way to reduce the amplitude of the matter power spectrum on scales of $8$ Mpc. However, only very few particle physics models have been put forward that exhibit the required properties and allow to relate them to other observables. In this work, we investigate a model based on two interacting sterile neutrinos and a scalar singlet. The heavier of the neutrinos is produced in the early Universe by the interplay of oscillations and the new interactions in the dark sector and constitutes the dominant component of dark matter. If the Yukawa matrix that describes the interactions of the steriles with the scalar is not diagonal, the heavier state can decay to three light sterile neutrinos. In contrast to the usual scenario, this leads to an all massive final state without radiation-like particles. We identify the part of the parameter space where these decays can lead to a reduction of S$_8$ at a level that matches observations. We then confront this region with the requirements of reproducing the observed relic density, as well as existing constraints from X-ray searches and Lyman-$α$ forest data.

</details>


### [37] [Zooming in on `bi-large' neutrino mixing with the first JUNO results](https://arxiv.org/abs/2511.22689)
*Gui-Jun Ding,Ranjeet Kumar,Newton Nath,Rahul Srivastava,José W. F. Valle*

Main category: hep-ph

TL;DR: The paper examines bi-large mixing patterns of the leptonic mixing matrix using data from the JUNO experiment, evaluates the viability of these schemes, and discusses JUNO's potential to distinguish between them, highlighting implications for neutrino oscillation, octant/CP predictions, and neutrinoless double beta decay.


<details>
  <summary>Details</summary>
Motivation: To assess how well bi-large mixing schemes align with JUNO's latest data and determine JUNO's capability to differentiate among these patterns, some of which are inconsistent with current oscillation data.

Method: Analyzing bi-large mixing patterns against global-fit oscillation data, evaluating JUNO's discriminatory power for these patterns, and exploring their predictions regarding neutrino mixing parameters like octant and CP violation.

Result: Some bi-large mixing schemes are significantly disfavored by current data. JUNO's data provides specific predictions about neutrino oscillation parameters and can help identify viable patterns. The study also connects these findings to neutrinoless double beta decay implications.

Conclusion: JUNO plays a crucial role in testing and narrowing down bi-large mixing models, with implications for understanding neutrino properties and potential insights into neutrinoless double beta decay.

Abstract: The leptonic mixing matrix is examined within bi-large mixing patterns and confronted with the latest results announced by the Jiangmen Underground Neutrino Observatory (JUNO). We analyze the viability of bi large mixing schemes and assess JUNO's ability to test neutrino mixing and discriminate among different bi-large mixing patterns, some of which are strongly disfavored when compared with neutrino oscillation global-fit results. Specific octant and CP predictions emerge. Finally, we comment on the implications of JUNO's findings for neutrinoless double beta decay.

</details>


### [38] [Bubble velocities in local equilibrium from a pseudopotential](https://arxiv.org/abs/2511.22711)
*Martin Münzenberg,Carlos Tamarit*

Main category: hep-ph

TL;DR: The paper introduces a novel method to estimate terminal bubble velocities in phase transitions using a pseudopotential function, avoiding the need for complex scalar field equations or ansatzes.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide an efficient and accurate method for determining bubble velocities during phase transitions without relying on simplified models or specific assumptions about plasma equations of state.

Method: The method involves calculating the extrema of a modified pseudopotential function tied to the scalar field. The pseudopotential's dependence on temperature and scalar gradients is analyzed, with terminal velocities identified where the pseudopotential minima are degenerate.

Result: The results confirm existing literature findings, such as the pressure dip for hybrid bubbles indicating stable deflagrations and unstable detonations, and demonstrate the method's accuracy without simplified assumptions.

Conclusion: The method offers a reliable alternative to existing techniques, enabling velocity calculations without solving dynamic equations of motion, thus simplifying phase transition analysis in plasma scenarios.

Abstract: We present a new method to estimate terminal bubble velocities during first-order phase transitions in a plasma in local equilibrium. The method relies on calculating the extrema of a modified potential function for the scalar field undergoing the transition. The shape of this function, which we refer to as the ``pseudopotential'', changes with the wall velocity, and if the dependence of the fluid temperature on scalar gradients is weak -- which is confirmed to hold with high accuracy in concrete examples -- the difference in pseudopotential between two appropriate extrema gives the net outward pressure acting on the bubble wall. It then follows that the correct terminal bubble velocities are those that lead to degenerate minima in the pseudopotential. This allows to compute bubble velocities without having to solve the equation of motion of the scalar field, and in contrast to other methods this can be done without relying on simplified equations of state for the plasma or without choosing a specific ansatz for the scalar field profile. We illustrate the method in a singlet extension of the Standard Model, computing the net outward pressure as a function of the wall velocity. We confirm the dip in outward pressure found in the literature for hybrid bubbles, which implies that stationary deflagrations are stable, while their detonation counterparts are unstable.

</details>


### [39] [$\texttt{history}$ in the making: A tool for NNLO cross sections](https://arxiv.org/abs/2511.22727)
*Lukas Simon,Sven Yannick Klein*

Main category: hep-ph

TL;DR: The paper presents the development and validation of the 'history' framework, implementing the fully-local Nested Soft-Collinear subtraction scheme for NNLO calculations in hadronic collisions. It applies the tool to predict a new observable for the process pp→ZH+X, which could probe new physics.


<details>
  <summary>Details</summary>
Motivation: To enable automated phase-space integration at NNLO accuracy for color-singlet processes in hadronic collisions, improving precision in theoretical predictions and providing sensitivity to beyond-Standard-Model effects through novel observables.

Method: The authors developed the 'history' framework using the fully-local Nested Soft-Collinear subtraction scheme. They validated it for quark-antiquark-initiated processes and applied it to calculate a new observable for the pp→ZH+X process.

Result: The framework was successfully validated for quark-antiquark processes. A novel observable was predicted for the inclusive ZH production, offering potential insights into new physics effects.

Conclusion: The implementation of the subtraction scheme within the 'history' framework is robust for NNLO calculations. The proposed observable provides a promising probe for new physics in ZH production at hadron colliders.

Abstract: In these proceedings, we report on our progress in developing the $\texttt{history}$ framework, which aims to implement the fully-local Nested Soft-Collinear infrared subtraction scheme for the automated phase-space integration of color-singlet production processes in hadronic collisions at NNLO accuracy. We validate our implementation for quark-antiquark-initiated processes and demonstrate a first application of the tool by predicting a novel observable for the inclusive process $pp \to ZH+X$, which may offer sensitivity to potential effects of new physics.

</details>


### [40] [Saturation effects in exclusive vector meson production in DIS](https://arxiv.org/abs/2511.22763)
*Oscar Garcia-Montero,Yannik Hoffmann,Sören Schlichting*

Main category: hep-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We investigate saturation effects in exclusive vector meson production in deep inelastic scattering (DIS), where we model fluctuations within the target protons as localized color-charge hotspots. Based on the Color Glass Condensate (CGC) framework and the dipole picture for vector meson production, we examine the dependencies of coherent and incoherent scattering cross sections on the momentum transfer. We draw conclusions on the effectiveness of our hot spot model and the strength of the suppression of the scattering cross sections caused by saturation effects. We find that saturation has mild effects in the given energy and charge-density ranges, but can also show that suppression becomes more prominent as the color-charge density inside the proton increases.

</details>


### [41] [Complexity Growth in Flavor-Dependent Systems](https://arxiv.org/abs/2511.22799)
*Wen-Bin Chang,Xun Chen,Defu Hou*

Main category: hep-ph

TL;DR: The paper investigates holographic complexity growth in a flavor-dependent EMD model using the Complexity=Action conjecture, showing dependencies on string velocity, temperature, chemical potential, and flavor number, with multi-valued complexity growth indicating phase transitions.


<details>
  <summary>Details</summary>
Motivation: To explore how holographic complexity relates to thermodynamic parameters and phase transitions in quantum systems modeled by EMD, leveraging machine learning fitted to lattice QCD data.

Method: A probe string's Nambu-Goto action time derivative on the WDW patch was computed under EMD model parameters derived via ML fits to lattice QCD EoS and baryon susceptibility data.

Result: Max complexity growth occurs for stationary strings (smaller at higher velocities), pure gluon systems have higher growth than systems with quark flavors. Temperature and chemical potential increases enhance complexity. Multi-valuedness links to first-order transitions, single-valued to crossover phases.

Conclusion: Holographic complexity effectively probes phase transitions and thermodynamic dependencies, providing insights into quantum systems' dynamic complexity through string configurations and model parameters.

Abstract: In this work, we investigate holographic complexity growth in a flavor-dependent Einstein-Maxwell-Dilaton (EMD) model, where the parameters are determined through machine learning algorithms fitted to lattice QCD equation of state (EoS) and baryon number susceptibility data. Within the Complexity=Action (CA) conjecture, we introduce a probe string into the bulk geometry and evaluate the time derivative of its Nambu-Goto (NG) action on the Wheeler-DeWitt (WDW) patch as the holographic dual of complexity growth. Our analysis explores the dependence of complexity growth on string velocity, chemical potential, temperature, and the number of flavors. Results show maximum complexity growth for stationary strings, decreasing with string velocity. At zero chemical potential, complexity growth is largest in the pure gluon system and reduces with the addition of quark flavors. Increasing temperature and chemical potential consistently enhance complexity growth. Furthermore, complexity growth exhibits multi-valued behavior in regions corresponding to first-order transitions and single-valued behavior in crossover regimes, indicating that complexity can serve as a probe for phase transitions.

</details>


### [42] [On the double counting subtraction at NLO${}^\star$ of the high-energy factorization approach](https://arxiv.org/abs/2511.22941)
*A. Chernyshev,V. Saleev*

Main category: hep-ph

TL;DR: The paper proposes enhancements to the double counting subtraction scheme to address oversubtraction issues in high-energy single photon production calculations within the NLO${}^	extstar$ high-energy factorization framework.


<details>
  <summary>Details</summary>
Motivation: To ensure consistent treatment of real corrections in the high-energy limit and eliminate oversubtraction problems in the analysis of single photon production.

Method: The authors improve the existing double counting subtraction scheme and apply these modifications to compute single photon production at NLO${}^	extstar$ approximation under the high-energy factorization approach.

Result: The proposed method successfully avoids oversubtraction, leading to more accurate and reliable results in the calculation of single photon production cross-sections.

Conclusion: The enhanced subtraction scheme provides a robust solution for handling real corrections and reducing errors in high-energy physics calculations, particularly in the context of single photon production within the specified theoretical framework.

Abstract: We suggest improvements for double counting subtraction scheme, which is needed for the consistent treatment of real corrections in the high-energy limit, and apply it to the single photon production at the NLO${}^\star$ approximation of the high-energy factorization approach. The presented improvements allow us to avoid the oversubtraction problem.

</details>


### [43] [Pion generalized parton distributions at zero skewness](https://arxiv.org/abs/2511.22947)
*Fernando Chandra,Parada T. P. Hutauruk,Terry Mart*

Main category: hep-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this paper, we systematically study the generalized parton distributions (GPDs) of the pion Goldstone Boson at zero skewness ($ξ=0$) in the framework of the covariant Nambu--Jona-Lasinio model with the help of the proper time regularization scheme to cure the divergence simultaneously to simulate the confinement. To this end, we evaluate the generalized form factors and parton distribution functions derived, respectively, from the first Mellin moments and the forward limit pion GPDs, in comparison to existing experimental data, recent lattice QCD simulations, and JAM global QCD analyses. We find that the pion parton distribution functions derived from the pion GPDs have excellent agreement with the experimental data and JAM analysis at renormalization scale $μ^2 =$ 4 and 27 GeV$^2$. In addition, our results for the pion generalized form factors involving the scalar, vector, and tensor dressed form factors are consistent with recent lattice data and existing data. We then compute the charge radii for those dressed generalized form factors, and we obtain $r_{S}^π =$ 0.56 fm, $ r_V^π =$ 0.63 fm, and $r_{T}^π =$ 0.83 fm for the pion scalar, vector, and tensor form factors, respectively.

</details>


### [44] [Holographic description of the kaon gravitational form factor](https://arxiv.org/abs/2511.23003)
*Zhibo Liu,Akira Watanabe*

Main category: hep-ph

TL;DR: The paper calculates the kaon gravitational form factor using a holographic QCD approach with SU(3) symmetry breaking, finding similar gravitational radii to the pion and confirming a 1/Q² falloff at high energies.


<details>
  <summary>Details</summary>
Motivation: To understand the gravitational form factors and gravitational radii of mesons (specifically kaons) by incorporating flavor symmetry breaking effects in holographic models, and to verify predictions of perturbative QCD in the high-energy regime.

Method: Bottom-up holographic QCD model incorporating SU(3) flavor symmetry breaking via the strange quark mass. Computed Q² dependence of the kaon GFF and compared it with pion results.

Result: Kaon GFF shows 1/Q² falloff at high Q², aligning with pQCD. The kaon gravitational radius is nearly identical to the pion's but slightly smaller.

Conclusion: Holographic QCD with flavor symmetry breaking accurately describes kaon gravitational properties, supporting the model's validity and offering insights into meson structure.

Abstract: We compute the kaon gravitational form factor (GFF) using a bottom-up holographic QCD approach that incorporates SU(3) flavor symmetry breaking through the strange quark mass. We present the resulting Q^2 dependence of the kaon GFF and compare it with that of the pion. In the high-energy limit, the kaon GFF exhibits a 1/Q^2 falloff, in agreement with perturbative QCD. Furthermore, we extract the gravitational radius of the kaon and find it to be almost the same as that of the pion, with a slightly smaller value.

</details>


### [45] [Fragmentation Functions:Modifications and new Applications](https://arxiv.org/abs/2511.23042)
*A. K. Likhoded,V. A. Petrov,V. D. Samoylenko*

Main category: hep-ph

TL;DR: The authors propose a modified quark fragmentation model incorporating Regge trajectory considerations for heavy quarkonia, which better explains experimental data on meson and baryon production.


<details>
  <summary>Details</summary>
Motivation: To improve the description of experimental data on heavy quark production, especially for quarkonia, by addressing limitations of the standard model in handling their unique Regge trajectory characteristics.

Method: Modification of the standard quark fragmentation model to explicitly account for the peculiarities of Regge trajectories in heavy quarkonium systems, allowing for more accurate predictions of meson and baryon yields.

Result: The modified model provides a more satisfactory fit to experimental data for both meson and baryon production compared to the standard model.

Conclusion: Incorporating Regge trajectory effects into fragmentation models enhances the accuracy of describing heavy quark production processes, suggesting the necessity of such modifications for precise theoretical predictions.

Abstract: We present a heavy quark fragmentation mechanism based on a modified version of the standard model of quark fragmentation, taking into account peculiarities of the Regge trajectories of quarkonia containing heavy quarks. The modified model describes the experimental data more satisfactorily both for meson and baryon production.

</details>


### [46] [An Experimental Framework for QCD studies of Event Shapes and Inclusive Hadron Spectra at FCC-ee energies](https://arxiv.org/abs/2511.23103)
*Philip Mathew,Ritu Aggarwal,Manjit Kaur*

Main category: hep-ph

TL;DR: The study uses Monte Carlo simulations to analyze hadronic final states in e⁺e⁻ annihilations at various FCC-ee energies. It evaluates QCD effects like αₛ extraction via event shape fits and examines soft gluon dynamics through particle distributions, providing a benchmark for future high-energy collider experiments.


<details>
  <summary>Details</summary>
Motivation: Investigate QCD dynamics and precision measurements of αₛ at FCC-ee energies while accounting for backgrounds and radiation effects to guide future experimental studies.

Method: Monte Carlo simulations (PYTHIA) at 91.2/160/240/365 GeV energies; analysis of event shape variables (Thrust, C-parameter) with NNLO QCD fits; study of charged particle multiplicities/momentum distributions considering ISR and backgrounds.

Result: Successful αₛ extraction using NNLO fits; observed energy-dependent evolution in particle multiplicities/momentum spectra; quantified background contributions from Z/W/tt̄/Higgs decays at higher energies.

Conclusion: Establishes a phenomenological baseline for FCC-ee's QCD program, demonstrating feasibility of precise αₛ measurements and soft gluon studies, with identified systematics for future calibration.

Abstract: We present a comprehensive analysis of hadronic final states produced in $e^+e^-$ annihilations at the planned FCC-ee center-of-mass energies of 91.2, 160, 240, and 365 GeV using Monte Carlo simulations with PYTHIA. The distortions on event shape variables at high center-of-mass-energies is investigated considering initial state photon radiation and backgrounds from hadronic decays of Z pairs, W pairs, top-quark pairs, and Higgs. An extraction of the strong coupling constant $α_{\text{s}}$ is performed through fits of the Thrust and C-parameter distributions to perturbative QCD predictions at next-to-next-to-leading-order (NNLO) precision. The study further probes soft gluon dynamics through charged particle multiplicities and momentum distributions, comparing the energy evolution of their mean values with previous experiments. The learning from this phenomenological work provide a reference for future experimental QCD studies at high-energy electron-positron colliders.

</details>


### [47] [Constraining the Inert Doublet Model at the LHC](https://arxiv.org/abs/2511.23133)
*Jayita Lahiri,Tania Robens,Krzysztof Rolbiecki*

Main category: hep-ph

TL;DR: The paper examines how exclusion limits from a 2HDMa analysis apply to the IDM model, highlighting potential detection gaps and updating constraints using Run 2 data.


<details>
  <summary>Details</summary>
Motivation: To assess the transferability of existing exclusion bounds from the 2HDMa model to the IDM, which shares similar signatures, and to refine constraints using updated data for better parameter space coverage.

Method: The authors analyze the ATLAS 2HDMa search results adapted to the IDM, evaluate detection sensitivity under model-specific cuts, and update constraints from vector boson fusion and soft lepton searches using full Run 2 data.

Result: Shows that model-optimized cuts might miss IDM signals with higher rates, updates IDM parameter space restrictions, especially in off-shell regions, and emphasizes the importance of considering soft lepton channels.

Conclusion: Existing exclusion analyses require careful reinterpretation for model-independence, and comprehensive use of updated experiments (e.g., soft lepton searches and off-shell studies) is crucial for uncovering IDM's viable parameter regions.

Abstract: In this work, we analyze experimental exclusion bounds that have been derived within a specific new physics realization, the two Higgs-doublet model with a pseudoscalar singlet (2HDMa), and their application to a different model, the Inert Doublet Model (IDM), that features the same final state. In this context, we discuss the sensitivity of the ATLAS search for the 2HDMa in final states with leptons and missing energy. We demonstrate that, with cuts optimized for a specific model topology, other new physics scenarios with larger rates might yet escape detection. We also give an update on constraints from vector boson fusion production of the Standard Model-like scalar and subsequent invisible decay from full Run 2 data on the parameter space of the IDM, with a special emphasis on the off-shell region, as well as a search that specifically concentrates on soft lepton final states.

</details>


### [48] [Magnetic Dipole Portal Vector Dark Matter at Fixed-Targets](https://arxiv.org/abs/2511.23259)
*Avik Banerjee,Riccardo Catena,Taylor R. Gray*

Main category: hep-ph

TL;DR: The paper proposes a sub-GeV vector dark matter model extending the Standard Model with a dark SU(2)_D sector, including interactions through a kinetic mixing portal, and explores its phenomenology and viability against experimental bounds.


<details>
  <summary>Details</summary>
Motivation: To explore viable dark matter models in the sub-GeV regime that can explain the observed relic abundance while evading existing experimental constraints through specific interaction mechanisms.

Method: The model introduces a new non-Abelian SU(2)_D gauge group with scalar doublet and triplet fields, allowing for a dimension-5 kinetic mixing portal interaction. The authors calculate thermal relic abundance, analyze particle spectra, and confront predictions with experimental limits from direct detection, BBN, colliders, and cosmological observations.

Result: The model's dark sector naturally exhibits an inverted mass hierarchy between dark matter and Z' boson, predicting signals for fixed-target experiments. A significant parameter space survives experimental constraints, particularly in regions where relic abundance arises from forbidden annihilation or direct Standard Model decays.

Conclusion: The proposed model provides a consistent framework for sub-GeV dark matter with unique phenomenological signatures, remaining compatible with observed relic density and experimental data, emphasizing potential for discovery in upcoming fixed-target experiments.

Abstract: We present a model featuring a sub-GeV vector dark matter by augmenting the Standard Model with a new non-Abelian dark $SU(2)_D$, spontaneously broken by the vacuum expectation values of a scalar doublet and a triplet. Interactions between the dark and visible sectors arise through a dimension-5 non-Abelian avatar of kinetic mixing portal, inducing effective magnetic dipole couplings of the dark matter, with the photon and Z boson. The resulting spectrum of the dark gauge bosons naturally exhibits an inverse mass hierarchy between the dark matter and the $Z^\prime$, leading to interesting phenomenology at fixed-target experiments such as LDMX through dark off-shell bremsstrahlung, dark Higgs-strahlung, invisible vector meson decay, and visible decays. We compute the thermal relic abundance across sub-GeV dark matter masses, with regions of freeze-out proceeding via forbidden annihilation into dark sector states or direct annihilation into Standard Model states. Bounds from a broad set of laboratory probes, along with cosmological and astrophysical observations, are incorporated in our analysis. Among them, the most restrictive bounds originate from direct detection experiments, Big Bang Nucleosynthesis, collider searches, and the CMB. Our results demonstrate that a sizeable region of the parameter space remains consistent with the observed relic abundance and the existing experimental results.

</details>


### [49] [Vector-field spontaneous baryogenesis with Lorentz invariance violation](https://arxiv.org/abs/2511.23378)
*Mattia Dubbini,Orlando Luongo,Aniello Quaranta*

Main category: hep-ph

TL;DR: The paper proposes an extended spontaneous baryogenesis model using a complex vector field that breaks U(1)_B symmetry, leading to a Bumblebee model with spontaneous Lorentz violation. The pseudo-Nambu-Goldstone boson acts as the inflaton, generating baryon asymmetry via CP-violating interactions. Flavor oscillations among baryon/lepton fields, even for massless fermions, modify the baryon asymmetry and align with experimental data.


<details>
  <summary>Details</summary>
Motivation: To address the baryon asymmetry problem by extending spontaneous baryogenesis with a vector field that breaks U(1)_B symmetry, incorporating Lorentz violation and flavor mixing effects.

Method: Introduce a complex vector field interacting via vector-current coupling with baryons/leptons, which spontaneously breaks U(1)_B and Lorentz symmetry. The resulting pseudo-Nambu-Goldstone boson serves as the inflaton. Calculate baryon asymmetry considering flavor oscillations affecting spatial momenta, not masses, allowing larger coupling constants.

Result: The model predicts non-null flavor mixing even for massless fermions, enhancing baryon asymmetry calculations. The asymmetry aligns with experimental values using permissible coupling constants.

Conclusion: The framework provides a viable scenario for baryogenesis through vector field-driven symmetry breaking and Lorentz violation, with flavor oscillations playing a critical role in matching observed baryon asymmetry.

Abstract: We extend spontaneous baryogenesis by considering the spontaneous breaking of $U(1)_B$ through a complex vector field. This field interacts with baryons and leptons via a vector-current coupling and, by construction, acquires a nonzero vacuum expectation value. Accordingly, the theory also exhibits a spontaneous violation of Lorentz invariance, effectively realizing a Bumblebee model. In this picture, the pseudo-Nambu-Goldstone boson arising from spontaneous breaking of the $U(1)_B$ global symmetry is the global phase of the Bumblebee vector and, in the broken phase, it results minimally coupled with the baryonic current, guaranteeing the violation of the baryon number. Consequently, we assume that the pseudo-Nambu-Goldstone, arising from spontaneous breaking of $U(1)_B$, plays the role of the inflaton, leading to baryogenesis across the entire inflationary stage, up to when the inflaton decays into baryon-antilepton and antibaryon-lepton pairs through a CP-violating interaction that also violates the Lorentz symmetry. Afterwards, we address the issue of flavor oscillations among baryon and lepton fields, including the oscillation probability in the calculation of the baryon asymmetry. Remarkably, our framework predicts a non-null mixing factor even for massless fermions. This mixing acts on the spatial momenta rather than on the masses of the produced fermions, allowing larger values of the coupling constant even guaranteeing the production of light fermions. The net baryon asymmetry results accordingly modified, and may also reproduce the experimental data for allowed values of the coupling constant.

</details>


### [50] [Analytical Soft Functions for Heavy-Quark Final States at Hadron Colliders](https://arxiv.org/abs/2511.23280)
*Ze Long Liu,Pier Francesco Monni*

Main category: hep-ph

TL;DR: The paper presents the first complete two-loop, fully-differential soft function for heavy-quark pair production with a color-singlet system at hadron colliders, providing analytic results for various kinematic configurations and decomposing it into dipole and tripole color correlators to improve precision predictions for LHC experiments.


<details>
  <summary>Details</summary>
Motivation: To advance precision predictions for heavy-quark physics at the LHC by providing essential analytic computations and decomposition of soft functions needed for theoretical calculations involving multiple jets and heavy-quark systems.

Method: Computed the two-loop soft function in closed analytic form for generic multi-dimensional kinematics, decomposed it into dipole and tripole color correlators, and derived results for transverse-momentum-dependent and threshold soft functions.

Result: Successfully obtained the first complete two-loop soft function for this process, enabling novel analytic results for specific observables and decomposing the function into color components that can be used in broader collider physics analyses.

Conclusion: The achieved results are critical for improving the accuracy of theoretical predictions in heavy-quark production processes at hadron and lepton colliders, supporting experimental efforts at the LHC and future facilities.

Abstract: We present the first computation of the complete two-loop, fully-differential soft function describing the production of a heavy-quark pair in association with a color-singlet system at hadron colliders. This result constitutes one of the most complex soft functions known to date and it is obtained in closed analytic form for generic multi-dimensional kinematics. This allows us to obtain novel analytic results for the transverse-momentum-dependent and threshold soft functions in this class of processes. We further obtain a decomposition of the soft function into dipole and tripole color correlators, thereby supplying essential building blocks for processes involving a heavy-quark pair produced together with additional light jets at both hadron and lepton colliders. These results represent a key ingredient for advancing precision predictions for heavy-quark physics at the LHC.

</details>


### [51] [Quark mixing from muon collider neutrinos](https://arxiv.org/abs/2511.23288)
*David Marzocca,Francesco Montagno,Manuel Morales-Alvarado,Andrea Wulzer*

Main category: hep-ph

TL;DR: A muon collider's neutrino beam enables precise measurements of neutrino-nucleon scattering, improving CKM matrix determinations and PDF/fragmentation function knowledge, with potential for combined PDF analysis.


<details>
  <summary>Details</summary>
Motivation: To leverage the high-intensity, collimated neutrino beam from a muon collider for precision measurements of neutrino scattering, addressing CKM matrix uncertainties and enhancing PDF/fragmentation function accuracy.

Method: Assessing irreducible uncertainties from PDF and fragmentation function limitations in a muon collider's far-forward neutrino experiment setup, using deeply inelastic scattering data.

Result: Demonstrates significant improvement over current standards in reducing uncertainties, enabling better CKM matrix and PDF determinations, and highlighting the neutrino experiment's potential.

Conclusion: The proposed experiment could vastly enhance our understanding of quark mixing and parton distributions, warranting further detailed studies.

Abstract: A high energy muon collider naturally produces a collimated beam of neutrinos for a fixed-target experiment at a dedicated far-forward facility. The high intensity and energy of the beam makes it ideally suited for astonishingly precise measurements of neutrino scattering on nucleons in the deeply inelastic regime, enabling the determination of the Cabibbo--Kobayashi--Maskawa~(CKM) quark mixing matrix. We assess the floor to the attainable sensitivity set by irreducible sources of uncertainties from the imperfect knowledge of the parton distribution (PDF) and fragmentation functions, showing that a strong improvement is possible well above current standards. As a by-product, our analysis also outlines extraordinary perspectives for a combined determination of the PDF. The results demonstrate the potential of a parasitic neutrino experiment at the muon collider, motivating detailed future studies.

</details>


### [52] [Intrinsic $k_T$ and soft gluons in Monte Carlo event generators](https://arxiv.org/abs/2511.23291)
*Louis Moureaux,Aleksandra Lelek,Francesco Hautmann,Laurent Favart*

Main category: hep-ph

TL;DR: The paper analyzes experimental measurements of Drell-Yan lepton pair transverse momentum to study intrinsic $k_T$, comparing TMD parton branching calculations and collinear parton-shower MC generators. It highlights the impact of soft-gluon resolution scales, Sudakov region effects, and correlated uncertainties in TMD fits, with an application to forward rapidity intrinsic $k_T$ determination.


<details>
  <summary>Details</summary>
Motivation: To understand non-perturbative physics through intrinsic $k_T$ measurements, focusing on accurate modeling and uncertainty treatment in TMD analyses, especially in forward kinematics.

Method: Comparison of TMD parton branching and collinear parton-shower MC methods, analysis of soft-gluon scale effects, and correlated uncertainty treatment in TMD fits.

Result: Demonstrated influence of soft-gluon resolution and Sudakov effects on $k_T$ extraction; emphasized critical need for correlated uncertainties in fitting procedures, applied to forward rapidity region data.

Conclusion: Accurate intrinsic $k_T$ determinations require careful consideration of theoretical frameworks and systematic error correlations, with forward rapidity studies providing key constraints on non-perturbative TMD models.

Abstract: Experimental measurements of the transverse momentum of Drell-Yan lepton pairs are sensitive to non-perturbative physics associated with the intrinsic parton transverse momentum $k_T$. We discuss recent determinations of intrinsic $k_T$ in the context of transverse momentum dependent (TMD) parton branching calculations and collinear parton-shower Monte Carlo generators. We illustrate the influence of the soft-gluon resolution scale and the non-perturbative Sudakov region on the intrinsic $k_T$ extraction. We emphasize the relevance of the correct treatment of correlated uncertainties between different transverse momentum bins in TMD fits and present an application to the determination of the intrinsic $k_T$ in the forward rapidity region.

</details>


### [53] [Sensitivity of the FCC-ee to the decay of a dark photon into a $μ^+μ^-$ pair](https://arxiv.org/abs/2511.23337)
*Giacomo Polesello*

Main category: hep-ph

TL;DR: The paper investigates the production of a dark photon (A') at the CERN FCC-ee collider through associated production with a photon (e+e- → γA'), focusing on its decay into μ+μ- pairs. It evaluates the 95% CL sensitivity on photon-dark photon mixing across 0.4-360 GeV masses, considering both prompt and long-lived A' decay scenarios using the IDEA detector simulation.


<details>
  <summary>Details</summary>
Motivation: To probe the existence of dark photons, which are hypothetical particles mediating interactions between dark matter and visible matter. This study aims to establish constraints on their mixing with photons and explore their potential signatures in collider experiments.

Method: The analysis uses the IDEA detector's parametrised simulation to model e+e- collisions at FCC-ee, simulating both prompt and displaced decays of the dark photon. The production process e+e- → γA' is studied, with A'→μ+μ- decay channels, and sensitivity limits on the mixing parameter are calculated using statistical methods.

Result: The study provides 99% CL sensitivity limits on the photon-dark photon mixing parameter across the A' mass range. It demonstrates enhanced sensitivity compared to previous studies, especially in scenarios where the dark photon has a long lifetime (long-lived decays).

Conclusion: The FCC-ee collider with the IDEA detector offers a powerful platform to explore dark photon physics. The results constrain the parameter space of dark photons and open avenues for testing dark sector theories, particularly in regions inaccessible to current experiments.

Abstract: The production of a dark photon $A^{\prime}$ at the proposed CERN FCC-ee collider is investigated. The study addresses the associated production $e^+e^-\rightarrowγA^{\prime}$ followed by the decay $A^{\prime}\rightarrowμ^+μ^-$. The 95% CL sensitivity on the mixing between the photon and the dark photon is evaluated in the $m_{A^{\prime}}$ mass range 0.4-360 GeV, based on a parametrised simulation of the IDEA detector. The study is performed both for prompt and long-lived decays of the $A^{\prime}$.

</details>


### [54] [The Art of Counting: a reappraisal of the HEFT expansion](https://arxiv.org/abs/2511.23410)
*Ilaria Brivio,Ramona Gröber,Konstantin Schmid*

Main category: hep-ph

TL;DR: The paper provides a systematic approach to power counting in Higgs Effective Field Theory (HEFT) based on first principles, identifying two viable power counting rules depending on whether one or two low-energy scales (v or v < f) are considered. It offers methods for consistently truncating HEFT operators and demonstrates these with examples.


<details>
  <summary>Details</summary>
Motivation: To address ambiguities in HEFT power counting by deriving rules from first principles, ensuring predictions for observables are expansion compatible with small dimensionless scales. Existing power counting conventions lacked a unified derivation and were dependent on operator normalizations, leading to inconsistencies.

Method: Revisiting HEFT formulation by enforcing predictions follow a series expansion in small dimensionless quantities. The analysis considers two scenarios—using a single scale v or separate scales v < f. Quantitative truncation prescriptions for operators/amplitudes are developed based on these scalings.

Result: Two valid power counting schemes are identified: one for single scale v and another for two scales v < f. These schemes accommodate any operator normalization choices, ensuring consistent truncations. Examples demonstrate how these countings improve systematic control over HEFT predictions.

Conclusion: HEFT's predictive power and consistency under different scale assumptions are enhanced through these derived power counting rules, providing a robust framework for model-independent analysis of Higgs physics at low energies.

Abstract: We revisit the power counting of the Higgs Effective Field Theory (HEFT) from first principles, by requiring that predictions for physical observables follow a series expansion in small, dimensionless quantities. Depending on whether HEFT is formulated in terms of a unique low-energy scale $v$ or in terms of two scales $v<f$, this approach identifies two viable power counting rules that can accommodate any operator normalization choice. We provide quantitative prescriptions for the consistent truncation of HEFT operators, amplitudes and observable contributions and we illustrate our arguments with a number of examples.

</details>


### [55] [Higgs pair production in gluon fusion to higher orders in Higgs Effective Field Theory](https://arxiv.org/abs/2511.23411)
*Ilaria Brivio,Ramona Gröber,Konstantin Schmid*

Main category: hep-ph

TL;DR: The paper examines Higgs pair production in the Higgs Effective Field Theory (HEFT), showing that including higher-dimensional operators at next-to-leading order (NLO) is necessary for accurate predictions and questions the validity of commonly used benchmark scenarios for di-Higgs searches.


<details>
  <summary>Details</summary>
Motivation: To probe correlations among Higgs couplings and test the HEFT framework's consistency, especially since HEFT allows decoupling of couplings compared to SMEFT. The study aims to ensure proper power counting and account for NLO diagrams which necessitate higher-dimension operators.

Method: Analyzing gluon fusion Higgs pair production in HEFT, incorporating higher-dimensional operators beyond leading order. The authors perform a phenomenological analysis of these operators' effects and reassess existing di-Higgs search benchmarks.

Result: Higher-dimensional operators significantly impact predictions, showing that NLO calculations require their inclusion. Common experimental benchmark scenarios may not adequately capture these contributions, suggesting revisions to analysis strategies.

Conclusion: HEFT analyses must include higher-dimension operators at NLO for accuracy. Current di-Higgs search benchmarks need critical re-evaluation to properly account for these effects, emphasizing the importance of rigorous power counting in EFT frameworks.

Abstract: Higgs pair production offers the opportunity to probe correlations among the couplings of one or two Higgs bosons to fermions and gauge bosons. In this context, it serves as a powerful test of the underlying Effective Field Theory (EFT) framework. In particular, while such couplings remain correlated in the Standard Model Effective Field Theory (SMEFT) at dimension six, they can become fully de-correlated in Higgs Effective Field Theory (HEFT) already at leading order in the EFT expansion. In this work, we study Higgs pair production via gluon fusion within the HEFT framework. We demonstrate that adopting a consistent power counting in combination with next-to-leading order (NLO) diagrams necessitates the inclusion of higher-dimensional operators beyond the leading ones. We analyze their phenomenological impact and re-assess critically the kinematic benchmark scenarios commonly used in experimental non-resonant di-Higgs searches in light of these additional contributions.

</details>


### [56] [SU($N_c$) confinement and color $N_c$-ality](https://arxiv.org/abs/2511.23413)
*V. Tomas Mari Surkau,Urko Reinosa*

Main category: hep-ph

TL;DR: The paper extends previous findings on quark number screening in QCD phases to SU(Nc) gauge groups and various color representations, proposing a thermodynamic rationale linked to color Nc-ality and center symmetry.


<details>
  <summary>Details</summary>
Motivation: To explore how confinement mechanisms in different QCD phases relate to quark screening and to develop a thermodynamic approach that avoids complex Euclidean functional methods.

Method: Analyzes the net quark number gained by a quark-gluon plasma with external color probes in fundamental and non-fundamental representations, examining SU(Nc) symmetry and Nc-ality's role in screening and center symmetry.

Result: Demonstrates that net quark number screening aligns with meson/baryon states in confined phases, generalizes findings to SU(Nc), and connects Nc-ality to center symmetry via thermodynamic arguments.

Conclusion: Thermodynamic reasoning can explain quark screening without Euclidean frameworks, reinforcing Nc-ality as a key confinement indicator and highlighting center symmetry's role in QCD phase behavior.

Abstract: In a recent work, we have argued that the net quark number gained by a bath of quarks and gluons upon bringing an external static quark probe, while being equal to $1$ in the high temperature, deconfined phase, is equal to $0$ or $3$ in the low temperature, confined phase, depending on the value of the quark chemical potential. This establishes a clear-cut connection between the confinement of a medium as probed by order parameters such as the Polyakov loop, and the ability of that same medium to screen the quark probe into states with the same net quark content as mesons or baryons. In this work, we extend these findings to the SU($N_c$) color group, consider color probes in various (fundamental and non-fundamental) representations, and conjecture a rationale of how these results could be recovered from a purely thermodynamic argument that shortcuts the use of the Euclidean functional framework, while emphasizing the role of color $N_c$-ality as a proxy for center symmetry within the Minkowskian framework.

</details>


### [57] [IRC-safe jet flavour without modifying anything](https://arxiv.org/abs/2511.23423)
*Terry Generet*

Main category: hep-ph

TL;DR: The paper restores infrared-collinear safety for standard jet and jet flavour definitions at NNLO QCD without altering their definitions, enabling direct comparison with existing measurements using the anti-$k_T$ algorithm.


<details>
  <summary>Details</summary>
Motivation: To address the violation of infrared safety in jet flavour definitions at NNLO QCD, which arises from computational artifacts, and provide a solution that maintains compatibility with current experimental analyses.

Method: Explicitly examines the cause of infrared safety violation at NNLO, demonstrates that it is a calculational artifact, and shows that consistent flavour treatment restores safety. The approach avoids modifying jet definitions or cross-section calculations.

Result: Successful restoration of infrared-collinear safety without changing jet definitions, allowing predictions to align with standard experimental methods like anti-$k_T$ clustering.

Conclusion: The proposed method preserves existing jet flavour definitions and measurement protocols, avoiding the need for experimental adaptations required by previous solutions.

Abstract: In this work, we describe how infrared-collinear safety can be restored perturbatively for standard definitions of jets and jet flavour. We will explicitly study this approach at next-to-next-to-leading order in QCD, where we will discuss the cause of the violation of infrared safety, argue that it is merely an artefact of the calculation, and explain how a consistent treatment of flavour removes this violation. The most important feature of our approach is that it does not require any changes to the definition of the jet or its flavour, nor does it modify the definition of the cross section. Consequently, the predictions can be directly compared to measurements performed using the standard anti-$k_T$ jet clustering algorithm, without the need for experimental collaborations to adapt their analyses to some new, infrared-collinear-safe definition of jet flavour, as would be the case for most - if not all - solutions presented in the literature thus far.

</details>


### [58] [New Particles at the Z-Pole: Tera-Z factories as discovery and precision machines](https://arxiv.org/abs/2511.23461)
*Marco Drewes,Juraj Klarić,Yuan-Zhen Li*

Main category: hep-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Several proposed future lepton colliders are capable of producing trillions of Z-bosons, including FCC-ee, CEPC, LEP3 and LEP-Z. Such Tera-Z factories can discover new elementary particles with couplings to the Z-boson that are orders of magnitude smaller than current bounds. For couplings near the currently excluded parameter regions they could produce sufficiently large samples to study the new particles' properties in detail, hence acting as a discovery and precision machine in one. Using simple analytic estimates, we quantify the dependence of the expected event yield in long-lived particle searches on the number of produced Z-bosons and on the detector dimensions. From this, we derive estimates for both the discovery reach and the measurement precision attainable at such facilities. While the precision of such estimates of course falls short of proper simulations, the analytic approach is suitable for a quick assessment of the sensitivity for a given design. We illustrate this with two examples, heavy neutral leptons and axion-like particles. Under optimistic assumptions, these could be produced in the millions and billions, respectively, effectively turning future lepton colliders into exotics factories.
  We provide a code that quickly generates the sensitivity curves displayed in this work and can be extended to other models at https://github.com/liyuanzhen98/LLPatTeraZ.

</details>


### [59] [Gravitational Waves from Confining Dark Sectors with Self-Consistent Effective Potentials](https://arxiv.org/abs/2511.23467)
*Rachel Houtz,Martha Ulloa,Mia West*

Main category: hep-ph

TL;DR: The paper analyzes gravitational wave signals from confinement-induced phase transitions in SU(N) gauge theories, applying theoretical constraints to identify viable parameter spaces, but finds future detectors still face challenges in detecting the signal.


<details>
  <summary>Details</summary>
Motivation: To predict gravitational wave signals from hidden non-Abelian gauge theories while ensuring the reliability of effective field theory descriptions through perturbativity and unitarity constraints.

Method: Imposing perturbativity/unitarity constraints on the thermal effective potential; using Polyakov-loop-improved potentials for N=3 and N=4; computing gravitational wave spectra and viable parameter regions.

Result: Theoretical constraints limit phenomenologically viable parameter spaces, making detection of gravitational wave signals difficult even for future detectors.

Conclusion: Detection challenges persist despite refined models, indicating need for improved experimental sensitivity or theoretical approaches.

Abstract: In this work, we present a self-consistent prediction for the gravitational wave signal arising from confinement-induced phase transitions in hidden non-Abelian SU(N) gauge theories with F light flavors. To do this, we impose perturbativity and unitarity constraints on the thermal effective potential to identify the portion of parameter space that admits a reliable effective field theory description. We also include the Polyakov-loop-improved finite-temperature potential for both N=3 and N=4, where N is the number of dark colors, using an approximate computation of the mediating effects. We compute the resulting gravitational wave spectrum and delineate the regions of parameter space that remain phenomenologically viable after imposing theoretical consistency conditions. We find that these constraints make uncovering a stochastic background gravitational wave signal in this scenario more challenging, even for proposed future detectors.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [60] [Parity breaking reshapes black hole spectral dynamics](https://arxiv.org/abs/2511.21786)
*Han-Wen Hu,Chen Lan,Zong-Kuan Guo*

Main category: gr-qc

TL;DR: The paper introduces a method using dynamical amplification of quasinormal mode instabilities to detect symmetry breaking in black holes under dynamical Chern-Simons gravity. Key findings include topological reconnections, stabilized modes delaying transitions, and scalar mode dominance, offering new tests for modified gravity via gravitational waves.


<details>
  <summary>Details</summary>
Motivation: To explore observable effects of gravitational symmetry breaking in modified gravity theories, specifically through environmental spectral instabilities in black hole quasinormal modes, which could provide testable predictions for gravitational wave astronomy.

Method: Perturbing Schwarzschild spacetime with a localized potential bump in dynamical Chern-Simons gravity, analyzing non-Hermitian spectral physics of quasinormal modes, and observing emergent dynamical phenomena absent in general relativity.

Result: Observed three novel phenomena: mode branch reconnections, stabilized modes delaying overtaking transitions, and scalar dominance at intermediate couplings. These amplify weak static symmetry-breaking effects into detectable gravitational wave signatures.

Conclusion: Establishes a framework connecting gravitational symmetry breaking to non-Hermitian physics, providing observational pathways to probe modified gravity theories using gravitational wave detections.

Abstract: We propose a dynamical amplification mechanism for detecting symmetry breaking in black holes through environmentally driven spectral instabilities of quasinormal modes. Focusing on dynamical Chern-Simons gravity as a paradigm for parity violation, we perturb the Schwarzschild background with a localized potential bump. Our analysis reveals three distinctive phenomena absent in general relativity: 1) topological reconnections of mode branches, 2) counterintuitive mode stabilization that delays overtaking transitions, and 3) scalar mode dominance emerging at intermediate coupling strengths. These dynamical features amplify weak static splittings into observable signatures, establishing a connection between gravitational symmetry breaking and non-Hermitian spectral physics. Our framework provides new pathways for testing modified gravity theories through gravitational wave observations.

</details>


### [61] [Numerical study of hypershadows in higher-dimensional black holes](https://arxiv.org/abs/2511.21829)
*Jianzhi Yang*

Main category: gr-qc

TL;DR: The paper presents a numerical method to compute and visualize the hypershadow of five-dimensional black holes, validating spherical symmetry in Schwarzschild-Tangherlini and analyzing dependencies on observer position and spin in Myers-Perry geometries. It introduces quantitative measures for size and displacement, with extension potential to exotic objects.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a tool for visualizing and quantifying three-dimensional black hole shadows (hypershadows) in higher-dimensional spacetimes, enabling better understanding of their geometric properties and symmetries.

Method: The method uses backward ray tracing, flexible observer positioning, discrete sampling, surface contouring, and reflection difference maps to analyze hypershadows in Schwarzschild-Tangherlini and Myers-Perry geometries.

Result: Validates spherical symmetry in Schwarzschild-Tangherlini, shows dependence of hypershadow on observer position and black hole spin, quantitatively measures size reduction and displacement with monotonic trends, and establishes a framework for future studies.

Conclusion: The framework successfully characterizes hypershadows, providing tools for numerical studies of exotic higher-dimensional objects and suggesting extensions to black rings and toroidal hypershadows.

Abstract: We develop a fully numerical framework to compute and visualize the \emph{hypershadow}\cite{Novo:2024wyn}, the three-dimensional generalization of the black hole shadow in five-dimensional spacetimes. Our method is based on backward ray tracing and allows flexible control over observer position, enabling the reconstruction of the full shadow volume. For visualization, we combine discrete sampling with surface contouring and introduce reflection difference maps on central slices to quantify mirror symmetries. Applying this method to the Schwarzschild-Tangherlini and Myers-Perry geometries, we validate the former's spherical symmetry and systematically discuss the hypershadow's dependence on observer position and black hole spin parameters. We also provide compact quantitative measures for size reduction and global displacement, revealing clear monotonic trends.The framework is readily extendible to other metrics and opens the way to numerical studies of more exotic objects, such as black rings and their prospective toroidal hypershadows.

</details>


### [62] [Gravitational waves from the late inspiral, transition, and plunge of small-mass-ratio eccentric binaries](https://arxiv.org/abs/2511.21897)
*Devin R. Becker,Scott A. Hughes,Gaurav Khanna*

Main category: gr-qc

TL;DR: This paper studies the effects of eccentricity and orbital anomaly angle on gravitational waveforms from extreme mass ratio black hole binaries using the eccentric Ori-Thorne procedure and Teukolsky equation solutions. It finds that eccentricity amplifies late-time tails but variations with anomaly angle can make the quasinormal mode excitations similar to quasi-circular cases.


<details>
  <summary>Details</summary>
Motivation: To understand how eccentricity and orbital phase influence the gravitational wave signals for LISA, particularly focusing on quasinormal mode excitation and power-law tails during the ringdown phase.

Method: Applies an eccentric Ori-Thorne method to generate inspiral/plunge worldlines for small bodies around Kerr black holes. Uses a time-domain Teukolsky equation solver to compute gravitational waves, analyzing waveform properties.

Result: Eccentricity amplifies late-time power-law tails, but quasinormal mode excitations can resemble quasi-circular cases depending on anomaly angle. The interplay between eccentricity and anomaly complicates waveform predictions.

Conclusion: Eccentricity significantly impacts late-time waveforms, but its effects are moderated by orbital anomaly angles, necessitating careful modeling for accurate LISA data analysis.

Abstract: Black hole binaries with small mass ratios will be important sources for the forthcoming Laser Interferometer Space Antenna (LISA) mission. Models of such binaries also serve as useful tools for understanding the dynamics of compact binary systems and the gravitational waves they emit. Using an eccentric Ori-Thorne procedure developed in previous work, we build worldlines that describe the full inspiral and plunge of a small body on an initially eccentric orbit of a Kerr black hole. We now calculate the gravitational waves associated with these trajectories using a code that solves the Teukolsky equation in the time domain. The final cycles of these waveforms, the ringdown, contains a superposition of Kerr quasinormal modes followed by a power-law tail. In this paper, we study how a binary's eccentricity and orbital anomaly angle affect the excitation of both quasinormal modes and late-time tails. We find that the relative excitation of quasinormal modes varies in an important and interesting way with these parameters. For some anomaly angles, the relative excitations of quasinormal modes are essentially indistinguishable from those excited in quasi-circular coalescences. Consistent with other recent studies, we find that eccentricity tends to amplify the late-time power-law tail, though the amount of this amplification varies significantly with orbital anomaly. We thus find that eccentricity has an important impact on the late-time coalescence waveform, but the interplay of eccentricity and orbit anomaly complicates this impact.

</details>


### [63] [Bayesian analysis of late-time tails in spin-aligned eccentric binary black hole mergers](https://arxiv.org/abs/2511.21898)
*Tousif Islam,Guglielmo Faggioli,Gaurav Khanna*

Main category: gr-qc

TL;DR: Analyzed late-time tails in gravitational waves from high-eccentricity, spin-aligned binary black hole mergers with mass ratio q=1000. Found tails more pronounced for high e and negative spins, exponents align with asymptotic predictions. Public tools via gwtails package.


<details>
  <summary>Details</summary>
Motivation: To study the dynamics of gravitational wave tails in extreme mass ratio binaries with eccentricity and spin effects, which are critical for advanced gravitational wave astronomy and testing general relativity.

Method: Simulated 15 mergers using point-particle black hole perturbation theory, tracking six spherical harmonic modes up to t=9000M. Employed frequentist and Bayesian analyses to estimate tail amplitudes/exponents.

Result: Confirmed asymptotic exponent predictions (p=-ℓ-4), showed spin and eccentricity dependence of tail strength. Demonstrated mode behavior where ℓ determines exponent, m has no effect.

Conclusion: Gravitational wave tails' late-time behavior is governed by orbital eccentricity and black hole spin, with ℓ-index dependent asymptotics. The gwtails package enables systematic analysis of such phenomena.

Abstract: We present a comprehensive analysis of late-time tails in gravitational radiation from merging spin-aligned eccentric binary black holes, using high-accuracy point-particle black hole perturbation theory simulations. We simulate the late-time evolution of 15 binary black hole mergers with mass ratio $q = 1000$, dimensionless spins $χ= [-0.9, -0.6, 0.0, 0.6, 0.9]$ and eccentricity at the last stable orbit $e_{\rm LSO} = [0.8, 0.9, 0.95]$. We track the tail amplitudes and exponents up to a retarded time coordinate $t = 9000M$ after merger for the six spin-weighted spherical harmonic modes $(2,1)$, $(2,2)$, $(3,2)$, $(3,3)$, $(4,3)$, and $(4,4)$ employing both frequentist and Bayesian approaches. We note that the tails are increasingly pronounced for binaries with high eccentricity $e_{\rm LSO}$ and large negative spin $χ$. We find that the overall late-time exponents closely approach their predicted asymptotic values ($p=-\ell-4$ for Weyl curvature scalar $ψ_{4,\ell m}$ where $\ell$ is the spin-weighted spherical harmonic index), while estimates restricted to the latest portion of the data exactly recover them. We further verify numerically that modes with the same spherical index $\ell$ share identical tail exponents, while variations in $m$ do not affect the tail behavior. Our analysis framework is publicly available through the gwtails Python package.

</details>


### [64] [Searches for Post-Merger Gravitational Waves with CoCoA: Sensitivity Projections Across Large Template Banks for Current and Next-Generation Detectors](https://arxiv.org/abs/2511.21941)
*Tanazza Khanam,Alessandra Corsi,Robert Coyne,Michael St. Pierre*

Main category: gr-qc

TL;DR: This paper analyzes the sensitivity of triggered searches for intermediate-duration gravitational waves from post-merger neutron star-black hole remnants using the Cross-Correlation Algorithm (CoCoA) and current/future detectors, aiming to optimize search strategies for diverse merger outcomes.


<details>
  <summary>Details</summary>
Motivation: The nature of the remnant from GW170817 and its relation to gamma-ray burst engines remain unresolved. The study seeks to assess how different remnant types' gravitational wave signals can be detected to understand their diversity and implications for GRB engines.

Method: Developed a Python framework to estimate CoCoA's detection sensitivity for various post-merger bar-mode waveforms across detector networks, evaluating computational efficiency and parameter space coverage.

Result: The framework identifies optimal parameter regions for focused searches, balancing sensitivity and computational cost, aiding in designing future search strategies for ground-based GW detectors.

Conclusion: The tool provides a method to prioritize parameter spaces and optimize search strategies, enhancing detection prospects for intermediate-duration GWs and improving understanding of NS merger remnants and GRB engines.

Abstract: The multi-messenger detection of the binary neutron star (NS) merger GW170817 has revolutionized the field of gravitational wave (GW) astronomy. However, several important questions remain to be answered. One of these is the nature of the compact remnant leftover by GW170817 (short- or long-lived NS versus black hole). A key goal going forward is to understand the diversity of NS-NS merger remnants, and how such diversity maps onto their viability as gamma-ray burst (GRB) central engines. Here, we present a study aimed at assessing the sensitivity of triggered searches for intermediate-duration, post-merger GWs powered by long-lived GRB remnants using networks of current and future ground-based GW detectors and the Cross-Correlation Algorithm (CoCoA). We develop a Python-based framework to efficiently estimate CoCoA distance horizons for a broad range of post merger secular bar-mode waveforms and for different GW detector networks. This framework can be used to identify the most promising regions of parameter space in which to concentrate search efforts, helping design future search strategies to optimally balance search sensitivity and related parameter space gridding schema against computational cost.

</details>


### [65] [Measuring Cosmological Redshift Using Gravitational Waves from Compact Binaries with Mass Transfer](https://arxiv.org/abs/2511.22057)
*Zi-Han Zhang,Tan Liu,Shenghua Yu,Zong-Kuan Guo*

Main category: gr-qc

TL;DR: The study investigates how mass transfer in compact binary systems with white dwarfs affects gravitational wave signals and uses the Fisher matrix to predict DECIGO's ability to measure redshift, achieving 10% accuracy for z=0.01 and SNR~30.


<details>
  <summary>Details</summary>
Motivation: To determine if gravitational wave measurements from mass-transferring white dwarf binaries can break degeneracies in binary mass and redshift parameters, and to assess DECIGO's capability in measuring redshift accurately.

Method: Analytic modeling of mass transfer's effect on gravitational wave phase to first order post-Newtonian, combined with Fisher matrix analysis to forecast parameter estimation precision for DECIGO.

Result: DECIGO can measure redshift with 10% accuracy for z=0.01 when signal-to-noise ratio (SNR) is ~30, leveraging mass transfer effects to disentangle mass and redshift parameters.

Conclusion: Mass transfer in white dwarf binaries provides a viable method for breaking parameter degeneracies, enabling precise redshift measurements with future gravitational wave detectors like DECIGO.

Abstract: The mass transfer process is prevalent during the inspiral phase of compact binary systems. Detection of gravitational waves from the inspiral phase of binaries with white dwarfs will allow us to measure the mass transfer rate. Mass transfer effects provide additional contributions to the phase of gravitational waves, which can break the degeneracy between binary masses and redshift. Based on the analytic mass transfer rate to the first order post-Newtonian evolution of orbital angular frequency, we use the Fisher matrix to forecast the ability of DECIGO to measure the redshift of compact binaries with mass transfer. We conclude that for compact binary systems containing white dwarfs, the redshift can be determined to an accuracy of $10\%$ for $z=0.01$ with a $SNR\thicksim 30$.

</details>


### [66] [Abelian and non-Abelian mimetic black holes](https://arxiv.org/abs/2511.22062)
*Mohammad Ali Gorji,Susmita Jana,Pavel Petrov*

Main category: gr-qc

TL;DR: The paper explores black hole solutions in a mimetic extension of the Einstein-Yang-Mills system with a constrained Yang-Mills term, revealing stealth solutions with electric and magnetic hair in both U(1) and SU(2) cases, differing from conventional non-Abelian black holes.


<details>
  <summary>Details</summary>
Motivation: To investigate how the mimetic gravity framework affects black hole solutions in Yang-Mills systems, particularly focusing on the possibility of stealth solutions with electric/magnetic hair and their differences from standard Einstein-Yang-Mills black holes.

Method: Analyzing the mimetic Einstein-Yang-Mills equations under a constant Yang-Mills constraint, deriving static spherically symmetric solutions for U(1) and SU(2) gauge groups, and comparing their properties with known black hole solutions.

Result: Static solutions in U(1) include Schwarzschild/Reissner-Nordstrom as special cases, with an electric hair stealth Schwarzschild solution; SU(2) allows arbitrary magnetic parameters for genuinely non-Abelian configurations differing from conventional SU(2) solutions requiring unit magnetic parameters.

Conclusion: Mimetic extensions enable novel stealth black hole solutions with non-traditional hair properties, expanding possibilities for non-Abelian gauge field behavior in black hole physics beyond standard models.

Abstract: We investigate black hole solutions in the mimetic extension of the Einstein-Yang-Mills system, in which the Yang-Mills term is constrained to be constant. In the Abelian U(1) case, we find a static spherically symmetric solution that includes the Schwarzschild and Reissner-Nordstrom black holes as special cases. Moreover, we identify a stealth Schwarzschild solution with an electric hair. We show that it is impossible to have magnetic hair in the U(1) gauge case, while, in contrast, the non-Abelian SU(2) stealth solutions can sustain both electric and magnetic hair. Unlike the conventional SU(2) Einstein-Yang-Mills black hole, which requires a unit magnetic parameter to exhibit nontrivial non-Abelian contributions, the stealth mimetic SU(2) solution admits genuinely non-Abelian configurations with arbitrary integer magnetic parameter.

</details>


### [67] [Hotspot Image Driven by Magnetic Reconnection in Kerr-anti-de Sitter Black Holes](https://arxiv.org/abs/2511.22077)
*Xiao-Xiong Zeng,Ke Wang*

Main category: gr-qc

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Based on the Comisso-Asenjo mechanism, we investigate the kinematic images of plasma before and after magnetic reconnection in Kerr-Anti-de Sitter(Kerr-AdS) black holes. Following a brief review of the Comisso-Asenjo process in Kerr-AdS black holes, we introduce the hotspot model and the imaging method. Building upon these foundational theories, we obtain the trajectory of the plasma and the temporal evolution of the hotspot images. It is found that there are three flares within the observing time, which is driven by the Comisso-Asenjo mechanism. We also discuss the influence of the cosmological parameter on the hotspot imaging. The results indicate that the hotspot image enlarges as the absolute value of $Λ$ increases, demonstrating that the cosmological constant significantly affects the hotspot.

</details>


### [68] [Lorentz Violation in Emergent Gravity and Its Cosmological Consequences](https://arxiv.org/abs/2511.22221)
*Raymond Isichei,Joao Magueijo*

Main category: gr-qc

TL;DR: The paper proposes that General Relativity and other geometric gravity theories can be interpreted as a degenerate Otto cycle with only heat-exchange processes. Introducing work-producing elements leads to controlled violations of local Lorentz invariance and energy-momentum conservation, which could explain late-time cosmic acceleration. The implications for the cosmological constant problem, structure formation, and local observations are also explored.


<details>
  <summary>Details</summary>
Motivation: To address unresolved issues in cosmology, such as the late-time acceleration of the universe and the cosmological constant problem, by reinterpreting gravitational theories through thermodynamic cycles like the Otto cycle.

Method: The authors model gravitational theories as degenerate Otto cycles lacking work-producing legs, then analyze how adding such legs introduces controlled violations of fundamental symmetries (Lorentz invariance) and conservation laws (energy-momentum), linking these effects to cosmological acceleration and structure formation.

Result: The analysis shows that controlled symmetry violations can account for cosmic acceleration without dark energy, offering a new perspective on the cosmological constant problem and structure formation dynamics.

Conclusion: The thermodynamic cycle framework provides a novel approach to gravitational theories, suggesting that modifying these cycles could reconcile observed cosmic acceleration with theoretical predictions without exotic components like dark energy.

Abstract: We show that General Relativity and other geometrical theories can be viewed as a degenerate Otto cycle with only heat-exchange legs in emergent gravity. Including work-producing legs yields controlled violations of local Lorentz invariance and energy-momentum conservation, which produce late-time cosmological acceleration. Implications for the cosmological constant problem, structure formation and local observations are discussed.

</details>


### [69] [Accretion of matter of a new bumblebee black hole](https://arxiv.org/abs/2511.22266)
*Yuxuan Shi,A. A. Araújo Filho*

Main category: gr-qc

TL;DR: The paper examines how a static black hole in bumblebee gravity, characterized by a Lorentz-violating parameter, influences accreting matter's behavior and observable features like shadows, light deflection, and emission models.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of Lorentz-violating bumblebee gravity on black hole accretion phenomena and their observable signatures, such as shadow size, light trajectories, and emission intensities.

Method: Ray tracing analysis was employed to study light deflection, direct emission, lensing rings, and photon rings. Three thin-disk emission models (ISCO, photon sphere, event horizon) and both static and infalling spherical accretions were modeled.

Result: Larger Lorentz-violating parameters enlarge the shadow, shift optical features outward, reduce observed intensity via gravitational redshift, and cause additional dimming in infalling matter due to Doppler effects.

Conclusion: Lorentz violation in bumblebee gravity significantly alters black hole accretion dynamics and observables, offering testable predictions for distinguishing such theories from general relativity through astrophysical observations.

Abstract: We investigate how the newly obtained static black hole in bumblebee gravity affects the behavior of accreting matter and its observable signatures. The Lorentz-violating parameter that characterizes this geometry modifies photon trajectories and shifts the location of the critical curve that defines the shadow. Using ray tracing, we examine light deflection, the structure of direct emission, lensing rings, and photon rings, and we explore three thin-disk emission models--starting at the ISCO, at the photon sphere, and at the event horizon--together with static and infalling spherical accretions. Larger values of this parameter enlarge the shadow, move all optical features outward, and suppress the observed intensity through gravitational redshift, with additional dimming produced by Doppler effects for infalling matter

</details>


### [70] [Kerr black holes as circular polarizers](https://arxiv.org/abs/2511.22276)
*De-Chang Dai*

Main category: gr-qc

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study the retrograde second caustics of extremal Kerr black holes, where the intensity of the light beam is infinitely magnified. We find that the caustics of different polarized beams are split by as much as $10^{-3}$rad by an external black hole for a suitable range of parameters. A lensing black hole at several lys away separates the polarized beams about $10^{12}$m apart. This splitting is larger than the radius of the Earth. Therefore, an observer on Earth would see different circularly polarized light according to their location. The polarization will change while the detector is wandering around. Thus, the polarization of light beams can be an important quantity in retrolensing observations.

</details>


### [71] [Net Charge Accretion in Magnetized Kerr Black Holes](https://arxiv.org/abs/2511.22356)
*Ethan Berreby,Avner Okun,Shahar Hadar,Amos Ori*

Main category: gr-qc

TL;DR: The paper examines how a rotating Kerr black hole charges in a magnetic field, revisiting Wald's charge saturation theory. By modeling particle accretion, it finds discrepancies in absorption rates leading to a non-universal saturation charge exceeding Wald's prediction.


<details>
  <summary>Details</summary>
Motivation: Wald's assumption of zero injection energy assumes charge balance, but the actual charge depends on competition between positive and negative particle absorption. The authors aim to correct this by considering both charge signs' accretion dynamics.

Method: They model oppositely charged particles' accretion using electromagnetic fields of a magnetized Kerr black hole, combining numerical and analytic approaches to determine absorption regions and cross-section bounds.

Result: At Wald's Qw charge, absorption cross-section bounds show 'attracted' charges have higher absorption than 'repelled' ones, especially with strong B0. This implies net charge accretion surpassing Qw, indicating the saturation charge must be higher.

Conclusion: Wald's saturation charge is invalid under strong magnetic fields where charge accretion imbalance occurs. The true saturation charge exceeds Qw, showing dependence on magnetic field strength contrary to Wald's universality claim.

Abstract: We investigate the charging process of a rotating Kerr black hole of mass $M$ and angular momentum $J$ immersed in a stationary, axisymmetric, asymptotically uniform magnetic field of strength $B_{0}$. In Wald's classic analysis (Wald 1974), which was based on the assumption of vanishing injection energy, the black hole was predicted to acquire a universal "saturation charge" $Q_{\mathrm{w}}=2B_{0}J$. However, the physical mechanism that sets the saturation charge must ultimately be governed by the competition between the absorption rates of positively and negatively charged particles. Motivated by this observation, we revisit the problem in the framework of a simple accretion model, where two dilute, equivalent fluxes of charged particles of opposite signs are injected from infinity along the magnetic field lines. The problem then reduces to that of individual particle motion in the electromagnetic field of the magnetized Kerr black hole. Using a combination of numerical and analytical tools, we determine the domains of absorption and establish both lower and upper bounds on the corresponding absorption cross sections. At $Q=Q_\mathrm{w}$ these bounds reveal a systematic difference between the two charge signs. In particular, for sufficiently strong magnetic fields, the lower bound on the absorption cross section for the "attracted" charge exceeds the upper bound for the "repelled" one. This charge accretion imbalance (which we find to become extreme at the limit of large $B_{0}$) indicates a persistent net charge accretion at $Q=Q_{\mathrm{w}}$, implying that the actual saturation charge must differ from Wald's charge $Q_{\mathrm{w}}$.

</details>


### [72] [Gravitational Spectra and Wave Propagation in Regular Black Holes Supported by a Dehnen Halo](https://arxiv.org/abs/2511.22366)
*Bekir Can Lütfüoğlu,Abubakir Shermatov,Javlon Rayimbaev,Muhammad Matyoqubov,Otaboyev Sirajiddin*

Main category: gr-qc

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We investigate gravitational perturbations, quasinormal modes, grey-body factors, and absorption cross-sections of the recently proposed regular and asymptotically flat black hole supported by a Dehnen-type dark-matter halo. This geometry provides a remarkably simple analytic model of supermassive black holes embedded in galactic environments, having a lapse function $
f(r)=1-2 M r^{2}/(r+a)^{3}, $
[R. A. Konoplya, A. Zhidenko, 2511.03066]. The regularizing parameter $a$ is the characteristic scale of the halo. We compute the quasinormal spectrum for both axial "up" and "down" perturbations using the WKB method and verify the results through time-domain integration. The two sectors are no longer isospectral, and the deviations grow with the halo scale parameter. The grey-body factors and absorption cross-sections are extracted via standard scattering boundary conditions and the WKB approach, and their behaviour is fully consistent with the structure of the effective potentials. Altogether, our analysis demonstrates that a dark-matter halo imprint induces modifications in the gravitational response, while the employed approximation schemes remain sufficiently accurate for quantitative predictions. At asymptotically late times, the presence of the halo does not alter the Price-law decay, which remains identical to that of a Schwarzschild black hole in vacuum.

</details>


### [73] [Minimum spacetime length and the thermodynamics of spacetime](https://arxiv.org/abs/2511.22403)
*Valeria Rossi,Sergio Luigi Cacciatori,Alessandro Pesci*

Main category: gr-qc

TL;DR: This review connects quantum gravity's discrete spacetime structure with emergent gravity theories, showing how a minimum length scale and a bi-tensorial metric lead to gravitational field equations via thermodynamic principles.


<details>
  <summary>Details</summary>
Motivation: To establish a link between quantum gravity's microscopic spacetime discreteness and macroscopic gravitational phenomena, explaining how thermodynamic entropy of spacetime's information content gives rise to gravity.

Method: 1. Outlines quantum gravity's prediction of a minimum spacetime length.
2. Proposes a bi-tensorial metric q_{αβ}(x,x') to model finite geodesic distances in the coincidence limit.
3. Applies a thermodynamic variational principle to derive gravitational field equations from microscopic entropy.

Result: Demonstrates that a quantum-discrete spacetime structure with minimum length can naturally produce Einstein-like gravitational equations through entropy-based thermodynamic formulation.

Conclusion: Emergent gravity frameworks succeed in bridging quantum structures and classical gravity by encoding geometric properties in thermodynamic entropy of spacetime's information degrees of freedom.

Abstract: Theories of emergent gravity have established a deep connection between entropy and the geometry of spacetime by looking at the latter through a thermodynamic lens. In this framework, the macroscopic properties of gravity arise in a statistical way from an effective small scale discrete structure of spacetime and its information content. In this review we begin by outlining how theories of quantum gravity imply the existence of a minimum length of spacetime as a general feature. We then describe how such a structure can be implemented in a way that is independent from the details of the quantum fluctuations of spacetime via a bi-tensorial quantum metric $q_{αβ}(x, x')$ that yields a finite geodesic distance in the coincidence limit $x\rightarrow x'$. Finally, we discuss how the entropy encoded by these microscopic degrees of freedom can give rise to the field equations for gravity through a thermodynamic variational principle.

</details>


### [74] [Multipole moments do not uniquely characterise spacetimes beyond general relativity](https://arxiv.org/abs/2511.22405)
*Arthur G. Suvorov,George Pappas*

Main category: gr-qc

TL;DR: The paper demonstrates that in beyond-Einstein theories of gravity, distinct spacetimes can share the same Geroch-Hansen multipole moments, challenging their uniqueness across gravitational theories. This has implications for phenomena like black-hole shadows and universal relations.


<details>
  <summary>Details</summary>
Motivation: To investigate whether the uniqueness of multipole moment decomposition in general relativity extends to alternative theories of gravity, and to explore the consequences for astrophysical phenomena.

Method: The authors compare static, spherically-symmetric solutions from different gravitational theories to check if distinct spacetimes can have identical Geroch-Hansen moments and analyze implications for black-hole shadows and universal relations.

Result: Two different spacetimes in modified gravity theories can possess the same multipole moments, and metrics may match without agreeing on moments. This breaks the uniqueness property found in general relativity.

Conclusion: The non-uniqueness of multipole moments in beyond-Einstein theories complicates their use in characterizing black holes or testing gravity theories via observations like shadow imaging or universal relations.

Abstract: Spacetimes in general relativity can be uniquely decomposed into a set of multipole moments. Given the usefulness of moments in the categorisation of radiation patterns, tidal deformations, and other phenomena associated with compact objects, a number of studies have explored their construction in beyond-Einstein theories of gravity. It is shown here that uniqueness does not necessarily extend across theories: by comparing a few static and spherically-symmetric solutions in different theories, we find that two distinct objects can possess the same Geroch-Hansen moments. Moreover, two metrics can match and yet take different moments. Implications of this result are explored in the context of black-hole shadows and ``universal'' relations hinging on moment computations.

</details>


### [75] [Thermodynamics and Bouncing Cosmology in Rastall-like Gravity](https://arxiv.org/abs/2511.22501)
*José A. C. Nogales,K. Luz-Burgoa,Laysa G. Martins*

Main category: gr-qc

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this study, we explore the thermodynamic aspects of a modified version of Rastall's gravity theory and its implications for cosmological scenarios. We analyze the role of non-conserved energy-momentum tensor equations and investigate their influence on particle production within an irreversible thermodynamic framework. By introducing a novel Lagrangian, we derive modified field equations and establish their relationship with matter production, both with and without entropy generation. Our analysis focuses on ideal fluid models and extends to spatially flat LFRW cosmologies, providing key equations that govern energy density, pressure, and curvature dynamics. Furthermore, we propose a bouncing cosmological model, in which the universe undergoes cycles of contraction and expansion, avoiding the singularity associated with the Big Bang. Our results indicate that this bouncing scenario is feasible within the Rastall-like gravity framework, supported by particle production processes and stability conditions. The violation of energy conditions near the bounce point further confirms the consistency of this alternative cosmological model. The present work is focused on the theoretical foundations and internal consistency of the model; possible observational implications will be addressed in future investigations. We conclude that the proposed theory offers a coherent phenomenological approach to matter production and provides new insights into non-standard cosmological evolution.

</details>


### [76] [A Universal Smarr Formula via Coupling Constants](https://arxiv.org/abs/2511.22558)
*Kamal Hajian,Bayram Tekin,Onur Ucanok*

Main category: gr-qc

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In gravitational theories containing matter fields and higher-derivative corrections, the standard Smarr formula often fails unless all dimensionful couplings are incorporated consistently. Traditionally, parameters such as the cosmological constant or the coefficients of higher-derivative terms are regarded as immutable features of the theory and therefore excluded from the thermodynamic phase space. In our recent work, we developed a fully general framework that promotes every such coupling to a dynamical, freely varying parameter of black hole solutions. This is accomplished by introducing, for each coupling, an auxiliary scalar and gauge field, through which the coupling appears as a conserved charge associated with the global sector of an emergent gauge symmetry. The corresponding conjugate variables naturally arise as electric potentials evaluated at the black hole horizon. As a result, the first law and the Smarr relation acquire additional, systematically determined contributions, yielding a consistent and universal extension of black hole thermodynamics. We illustrate the validity of this construction by revisiting several black hole examples in the literature where the Smarr formula remains inconsistent even after treating the cosmological constant as a thermodynamic variable. Our analysis shows that only by including all dimensionful couplings in this generalized manner can one obtain an internally consistent Smarr relation, thereby providing the foundation for a truly universal formulation of black hole thermodynamics.

</details>


### [77] [Electromagnetic radiation from circular orbits in Schwarzschild--de Sitter spacetime](https://arxiv.org/abs/2511.22611)
*João P. B. Brito,Rafael P. Bernar,Luís C. B. Crispino*

Main category: gr-qc

TL;DR: The study examines electromagnetic radiation from charged particles around a 4D Schwarzschild-de Sitter black hole, finding that electromagnetic power is roughly double scalar fields with conformal coupling at low angular velocities, but scalar fields with minimal coupling can surpass it. The spectral analysis shows broader frequency ranges and significant low multipole contributions for electromagnetic emissions compared to scalar fields.


<details>
  <summary>Details</summary>
Motivation: To understand radiation mechanisms in curved spacetimes with cosmological constants, extending Schwarzschild results to include de Sitter effects and comparing electromagnetic vs scalar field emissions.

Method: Semiclassical approach to calculate photon emission probability amplitudes, deriving power and spectral distributions. Compared electromagnetic results against scalar fields (minimal and nonminimal coupling) in Schwarzschild-de Sitter and Schwarzschild spacetimes.

Result: Electromagnetic power is ~2x scalar conformal coupling at low angular velocities; scalar minimal coupling can exceed EM power. EM spectra are broader with stronger low multipole contributions, especially near photon spheres.

Conclusion: Demonstrates distinct radiation behaviors between electromagnetic and scalar fields in Schwarzschild-de Sitter spacetime, highlighting cosmological constant and coupling type effects on emission properties.

Abstract: We investigate the electromagnetic radiation emitted by a charged particle orbiting a four-dimensional Schwarzschild--de Sitter black hole using a semiclassical approach. We calculate the probability amplitude for the charged particle to emit a photon, from which we derive the emitted power and spectral distributions. The results are compared with those obtained for scalar fields, considering both minimal and nonminimal coupling, as well as the corresponding electromagnetic case in the Schwarzschild spacetime. It is found that the total electromagnetic power is approximately twice that of the scalar field with conformal coupling for orbits with low angular velocity, similar to the Schwarzschild case, whereas for minimal coupling, the scalar power can even exceed the electromagnetic power. We also investigate the spectral distributions and possible synchrotron emission by orbits near the photon sphere. The electromagnetic case has a much broader frequency spectrum, with a non-negligible lower multipole contribution.

</details>


### [78] [Echoes of Traversable Wormhole](https://arxiv.org/abs/2511.22671)
*Rajdeep Mondal,Abhishake Sadhukhan*

Main category: gr-qc

TL;DR: The paper studies linear scalar perturbations in a four-dimensional traversable wormhole solution, finding that higher angular momentum modes produce stronger echoes due to more reflective potential barriers in the throat's resonant cavity.


<details>
  <summary>Details</summary>
Motivation: To understand the dynamics of scalar perturbations in traversable wormholes, particularly how the near-horizon geometry and potential barriers affect wave trapping and echo generation.

Method: The authors analyze the wormhole's response using time-domain integration of an initial scalar wave packet, derived the effective scalar potential, and examined the resulting echoes for different angular momentum modes.

Result: Late-time echoes are observed with amplitudes dependent on the angular momentum number l, where higher l modes yield stronger echoes due to taller, more reflective potential barriers.

Conclusion: The near-horizon geometry creates a resonant cavity through sharp potential barriers, and higher l modes enhance echo strength by trapping waves more efficiently in the wormhole throat.

Abstract: We study linear scalar perturbations of the four-dimensional, traversable wormhole solution of Maldacena, Milekhin, and Popov(arXiv:1807.04726). The geometry is constructed by matching an asymptotically flat, near-extremal Reissner--Nordström region to a throat described by $AdS_2 \times S^2$, supported by charged massless fermions. We derive the effective scalar potential governing wave dynamics, which when viewed in the tortoise coordinate, exhibits two extremely sharp and widely separated barriers. These barriers form a resonant cavity and are a direct consequence of the near-horizon geometry of the wormhole mouths. Using time-domain integration, we analyze the wormhole's response to an initial scalar wave packet inside the throat. We find that the late-time signal contains a distinct train of echoes whose amplitude depends on the angular momentum number $l$. We show that higher $l$ modes produce significantly stronger echoes, as the corresponding potential barriers are taller and more reflective, which results in more efficient trapping of the wave within the wormhole throat.

</details>


### [79] [Asymptotics and Universality in Black Holes: from the quasinormal Weyl's law to the binary merger waveform](https://arxiv.org/abs/2511.22722)
*José Luis Jaramillo,Lamis Al Sheikh,Jérémy Besson,Badri Krishnan,Michele Lenzi,Rodrigo Panosso Macedo,Oscar Meneses-Rojas,Bernard Raffaelli,Carlos F. Sopuerta,Corentin Vitel*

Main category: gr-qc

TL;DR: The paper advocates an asymptotic reasoning approach to uncover universality patterns in black hole spacetimes, focusing on quasinormal modes (QNMs) and binary BH mergers. It identifies light-trapping and redshift effects as key mechanisms underlying QNM universality and proposes a hierarchical asymptotic model for binary BH dynamics connected to integrability theory.


<details>
  <summary>Details</summary>
Motivation: To address the blind spots in understanding mechanisms behind simplicity and universality in black hole dynamics, particularly in quasinormal modes and binary mergers, by employing an asymptotic reasoning approach that filters non-essential degrees of freedom.

Method: 1. Analyzing quasinormal mode (QNM) Weyl's law to derive asymptotic behavior of QNM counting functions. 2. Identifying light-trapping and redshift effects as key structural stability mechanisms. 3. Proposing a hierarchical asymptotic model for binary BH mergers using a wave-mean flow approach with connections to integrability theory in gravity.

Result: Unveils universality patterns in BH QNM spectra linked to spacetime structure and dimensionality. Establishes a framework connecting asymptotic models to integrability, potentially enabling observational tests of spacetime dimensions through QNM measurements. Sketches a program for applying this reasoning to binary BH mergers.

Conclusion: Asymptotic reasoning is powerful in revealing universal mechanisms in BH systems. Further development of hierarchical asymptotic models could deepen understanding of BH dynamics, including links to integrability and potential observational signatures of spacetime properties.

Abstract: Current state-of-the-art approaches to black hole (BH) dynamics, encompassing several effective approximation schemes, offer a remarkable control of the quantitative aspects of strong gravity. They also provide key insights into some qualitative aspects of the problem. In spite of this, there remain blind spots that hinder the understanding of the mechanisms underlying some observed phenomena, in particular concerning simplicity and universality in BH spacetimes. Adopting an 'asymptotic reasoning' approach, by filtering non-essential degrees of freedom, can potentially unveil universality patterns by identifying key underlying structural stability mechanisms. We first illustrate such an asymptotic approach by focusing on a BH quasinormal (QNM) Weyl's law, that accounts for the universal asymptotics of the QNM "counting function". This permits to identify light-trapping and the (local) redshift effect as the underlying mechanisms, also offering a bridge to the universal patterns found in BH QNM spectral instability. As a by-product, Weyl's law universality formally opens an observational access to spacetime (effective) dimensionality. More heuristically, we sketch a program recently put forward to apply such 'asymptotic reasoning' to address the observed simplicity and universality patterns in binary BH merger dynamics. This program is built as a hierarchy of asymptotic models, potentially making contact with integrability theory in gravity, namely through the background sector in a "wave-mean flow" approach to BH binary dynamics.

</details>


### [80] [Inflation, black holes with primary hair, and regular planar black holes from an infinite tower of regularized Lovelock-Proca corrections](https://arxiv.org/abs/2511.22798)
*Pedro G. S. Fernandes,Jingqian Gou,Lavinia Heisenberg,Nadine Nussbaumer*

Main category: gr-qc

TL;DR: Infinite towers of higher-order Proca corrections, inspired by dimensional regularizations of Lovelock invariants, replace the Big Bang singularity with an inflationary epoch and enable regular planar black holes and spherically symmetric black holes with primary hair.


<details>
  <summary>Details</summary>
Motivation: Address singularities in early-universe cosmology and black holes by extending previous proposals of using infinite towers of higher-order corrections to General Relativity.

Method: Implement an infinite tower of higher-order Proca corrections derived from dimensionally regularized Lovelock invariants to modify General Relativity.

Result: The Big Bang singularity is replaced by an inflationary period, and the theory supports regular planar black holes and spherically symmetric black holes with primary hair.

Conclusion: These results suggest that Lovelock-Proca towers offer a viable pathway to resolve spacetime singularities while enriching black hole solutions with novel features.

Abstract: Infinite towers of higher-order corrections to General Relativity have been proposed as a mechanism to resolve singularities in early-universe cosmology and black holes, in a variety of settings. In this work, we consider an infinite tower of higher-order Proca corrections inspired by dimensional regularizations of Lovelock invariants. We find that the Big Bang singularity present in General Relativity is replaced by an inflationary epoch. Furthermore, the Lovelock-Proca tower allows for regular planar black hole solutions and spherically symmetric black holes with primary hair.

</details>


### [81] [A New Approach to the Calculation of Particle Creation from Analog Black Holes](https://arxiv.org/abs/2511.22895)
*Yang-Shuo Hsiung,Pisin Chen*

Main category: gr-qc

TL;DR: The paper introduces the Inertial Replacement Method (IRM), a hybrid analytic-numerical approach to compute Bogoliubov coefficients for accelerating mirrors, enabling accurate predictions for analog Hawking radiation experiments like AnaBHEL.


<details>
  <summary>Details</summary>
Motivation: The need for accurate particle creation predictions in experiments such as AnaBHEL, where realistic mirror trajectories make traditional analytical methods intractable.

Method: The IRM method replaces asymptotically inertial trajectory segments with analytic extensions, reducing numerical computation to the finite accelerating region. Error bounds are derived for both perfect and imperfect mirrors, and validated against known cases.

Result: IRM accurately models plasma-mirror trajectories relevant to AnaBHEL, showing radiation spectra depend primarily on the accelerating region, not inertial parts. Validated through analytical comparisons.

Conclusion: IRM is a reliable, broadly applicable computational tool for modeling particle creation in realistic analog gravity systems, crucial for future experiments like AnaBHEL.

Abstract: Accurate prediction of particle creation from accelerating mirrors is crucial for interpreting forthcoming analog Hawking radiation experiments such as AnaBHEL. However, realistic experimental setups render the associated Bogoliubov integrals analytically intractable. To address this challenge, we introduce the Inertial Replacement Method (IRM), a hybrid analytic-numerical framework for computing Bogoliubov coefficients for general moving-mirror trajectories. The IRM replaces the asymptotically inertial portions of a trajectory with analytic inertial extensions, so that numerical evaluation is required only for the finite accelerating segment. We derive perturbative error bounds for both perfectly and imperfectly reflecting mirrors, providing controlled accuracy estimates and guiding the choice of segmentation thresholds. The method is validated against analytically solvable trajectories and then applied to a fully numerical, PIC-based Chen-Mourou plasma-mirror trajectory relevant to the planned AnaBHEL experiment. A key physical insight emerging from this analysis is that the radiation spectrum is determined almost entirely by the finite accelerating region, with negligible sensitivity to the far-past and far-future inertial motion. These results establish the IRM as a reliable and broadly applicable computational tool for modeling particle creation in realistic analog-gravity systems such as AnaBHEL.

</details>


### [82] [Strong-field Gravitational Wave Lensing in the Kerr Background](https://arxiv.org/abs/2511.23110)
*M. V. S. Saketh,Rajes Ghosh,Anuj Mishra*

Main category: gr-qc

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Gravitational lensing of gravitational waves (GWs) can encode valuable information about the properties of the intervening lens, yet most existing studies remain restricted to the small-deflection, weak-field regime. To bridge this crucial gap, this work presents the first systematic analysis of strong-field, wave-optical GW lensing by a Kerr black hole (BH), extending recent results for non-rotating lenses obtained in Chan et al. to the astrophysically more relevant case of spinning lens. Using the Mano-Suzuki-Takasugi formalism, we compute the strong-field scattering factor (SFSF) and show that the the spin produces characteristic modifications to the lensed waveform, and high-frequency incident radiation is not strongly absorbed by the BH lens, contrary to earlier claims in Chan et al. We further derive an explicit expression for the observed waveform for the general source-lens-observer configuration, showcasing the distortions produced by the scattering and quantifying their departure from the Schwarzschild case. Specializing to on-axis scattering, a mismatch analysis for a \texttt{GW150914}-like source lensed by a Kerr BH of mass $M=10^2M_\odot$ situated $100GM/c^2$ away from the source reveals percentage-level deviations from the direct (unscattered) wave at scattering angles near $30^\circ$, across a range of lens spin values. The mismatch exhibits a sharp decrease at larger scattering angles. For a fixed scattering angle, however, the mismatch shows only a weak dependence on the BH spin in the case of on-axis scattering, which may improve for more general off-axis considerations. The framework developed here offers a unified treatment of strong-field GW scattering in Kerr spacetime and provides tools for interpreting future high-precision observations of compact-object lenses in the wave-optics regime.

</details>


### [83] [Probing Observable Features of Lorentz violation in Low-Energy Hořava Gravity with Accretion Disk Images of Black Hole](https://arxiv.org/abs/2511.23117)
*Meng-Die Zhao,Yu-Yan Wang,Ke-Jian He,Guo-Ping Li*

Main category: gr-qc

TL;DR: The paper investigates Lorentz violation (LV) effects in low-energy Horava gravity by simulating images and polarization of rotating LV black holes using backward ray-tracing. Key findings include strong dependence of the inner shadow shape, brightness asymmetry, and polarization on the LV parameter l. The results highlight how varying l impacts black hole spin and observational features, suggesting EHT observations could test LV hypotheses.


<details>
  <summary>Details</summary>
Motivation: To explore observable signatures of Lorentz violation in Horava gravity, which could provide empirical tests for this modified gravity theory. Existing studies lack detailed analysis of LV effects on black hole observations through combined image and polarization data.

Method: Numerical simulations using backward ray-tracing method within a thin-disk accretion model and Zero Angular Momentum Observer (ZAMO) framework. Solved photon geodesic equations to generate synthetic images and polarization patterns of rotating LV black holes under varying LV parameter l.

Result: Large LV parameter l produces a leftward D-shaped critical curve and increased black hole angular velocity, while smaller l yields elliptical untilted shadows and reduced spin. Polarization intensity and direction distributions are significantly altered near critical curves, with clear observational distinctions based on l's sign.

Conclusion: LV effects alter observable black hole features in detectable ways. Future EHT observations combining imaging and polarization data could constrain or validate Lorentz violation predictions from Horava gravity, offering a novel testbed for quantum gravity phenomenology.

Abstract: In this paper, we study the observable signatures of Lorentz violation (LV) in low-energy Horava gravity by simulating the images and polarization features of rotating LV black holes using a backward ray-tracing method. Within a thin-disk accretion model and the ZAMO framework, we numerically solve the geodesics equation of photon and simulate the corresponding thin-disk images and polarization patterns. The results show that the LV parameter l strongly affects the inner shadow, brightness asymmetry, and polarization properties of the thin disk. The decrease of l leads to a more elliptical and untilted inner shadow, while increasing l produces a pronounced leftward D-shaped structure of the critical curve. In addition, the variation of l alters the distribution of polarized intensity and polarization direction, especially near the critical curve. Moreover, it also shows that a positive l enhances the black hole's angular velocity, while a negative one suppresses it, indicating that the sign of l determines the trend direction of the LV effect. These findings suggest that future high-resolution EHT observations combining the thin-disk images and polarization patterns could provide valuable tests of the LV effect.

</details>


### [84] [Internal structure of Hayward black holes](https://arxiv.org/abs/2511.23165)
*Caiying Shao,Jun-Qi Guo,Yu Tian,Hongbao Zhang*

Main category: gr-qc

TL;DR: The study explores scalar field collapse in Hayward regular black holes, finding that weak scalar perturbations keep the inner Cauchy horizon stable, while strong fields cause it to collapse into a spacelike singularity, transitioning the geometry to Schwarzschild-like with a critical exponent γ ≈ 0.5 near the threshold p_*.


<details>
  <summary>Details</summary>
Motivation: To investigate the stability of the Cauchy horizon in regular black holes like Hayward, addressing fundamental questions about spacetime structure and the endpoint of gravitational collapse.

Method: Numerical analysis of scalar field collapse in Hayward spacetime, varying scalar field strength parameters to observe inner horizon behavior and critical phenomena during phase transitions.

Result: Weak scalar fields maintain a stable inner horizon; strong fields collapse it to zero volume with a spacelike singularity. Critical exponent γ ≈ 0.5 describes radius scaling near threshold p_.

Conclusion: Scalar field strength decisively influences inner horizon dynamics, confirming Cauchy horizon instability under certain conditions and revealing universal critical behavior analogous to gravitational collapse scenarios in other spacetimes.

Abstract: Regular black holes, free of central singularities, provide an ideal laboratory for probing the geometric structure of spacetime. The global structure of some regular black holes, e.g. Hayward black hole, features an event horizon and a Cauchy horizon, raising fundamental questions about the latter's stability. In this work, we investigate collapse of a scalar field in Hayward spacetime. Under weak scalar perturbations, the inner horizon maintains a stable finite radius. In the circumstance of a strong scalar field, the inner horizon shrinks to zero volume, accompanied by the formation of a spacelike singularity. The Hayward geometry is effectively converted into a Schwarzschild-like geometry. Furthermore, the strength of the scalar field governs the contraction dynamics of the inner horizon. As the parameter $p$ of the initial profile for the scalar field approaches the critical threshold ${p_*}$, the radius of the inner horizon ${r_{-}}$ exhibits a universal scaling behavior: ${r_{-}}\propto{|p - {p_*}|^γ}$, with a critical exponent $γ\approx 0.5$.

</details>


### [85] [Linear and nonlinear late-time tails on dynamical black hole spacetimes via time integrals](https://arxiv.org/abs/2511.23242)
*Dejan Gajic,Lionor Kehrberger*

Main category: gr-qc

TL;DR: The paper establishes the global leading-order late-time asymptotic behavior of solutions to inhomogeneous wave equations on dynamical black hole exteriors approaching Schwarzschild backgrounds. It shows non-spherical solutions decay slower than Price's law by one power, using energy estimates linking time tails to spatial conformal irregularity.


<details>
  <summary>Details</summary>
Motivation: To understand how solutions to wave equations behave on dynamic black hole backgrounds that asymptote to Schwarzschild, especially deviations from Price's law for stationary backgrounds. Addresses the challenge of capturing precise late-time asymptotics with minimal decay assumptions.

Method: A physical-space energy estimate approach treating the dynamical wave operator as a Schwarzschild operator with inhomogeneous term. Exploits the connection between temporal decay tails and spatial conformal properties via analyzing differences from a chosen global tail function.

Result: Proves that non-spherically symmetric solutions exhibit slower decay by one power compared to Price's law. Valid for spacetimes converging to Schwarzschild with small decay. Demonstrates method robustness for nonlinear equations and extensions to Kerr and higher dimensions.

Conclusion: The findings refine understanding of black hole perturbations on dynamic backgrounds and validate a versatile method applicable to broader spacetime dynamics, including Kerr and higher-dimensional scenarios.

Abstract: We prove the global leading-order late-time asymptotic behaviour of solutions to inhomogeneous wave equations on dynamical black hole exterior backgrounds that settle down to Schwarzschild backgrounds with arbitrarily small decay rates. In particular, we show that for non-spherically symmetric solutions arising from compactly supported initial data, the late-time decay deviates from Price's law -- governing the decay for stationary black hole backgrounds -- by exhibiting slower time decay by exactly one additional power.
  Our proof is based around the observation that the emergence of late-time "tails", featuring inverse-polynomial decay in time, is intimately connected to conformal irregularity properties in space (towards future null infinity) of time integrals of the solutions. This relationship is exploited through a purely physical-space approach based around energy estimates, in which the dynamical wave operator is treated as a Schwarzschild wave operator with an inhomogeneous term. Going from almost-sharp decay to global asymptotics is achieved by exploiting this relation for the difference between the solution and a carefully chosen global tail function.
  We further apply our method to several examples of nonlinear wave equations and comment on its robustness to more general settings, such as dynamical spacetimes converging to sub-extremal Kerr spacetimes and higher-dimensional wave operators with even or odd spacetime dimensions.

</details>


### [86] [Constructing allowed complex metrics from black holes](https://arxiv.org/abs/2511.23338)
*Oscar Loaiza-Brito,J. L. López-Picón,Octavio Obregón*

Main category: gr-qc

TL;DR: The paper explores the connection between black hole metrics and complex solutions using diffeomorphic mappings, transforming static and rotating black holes into cosmological models within Quantum Field Theory.


<details>
  <summary>Details</summary>
Motivation: To bridge black hole physics with cosmological models by leveraging complex solutions allowed under the Kontsevich-Segal criterion, enabling quantum field theory applications.

Method: Diffeomorphic mappings are applied by swapping radial and time-like coordinates and using complex transformations to convert black hole interiors into Kantowski-Sachs and Gowdy-type cosmological models.

Result: Successfully mapped static and rotating black hole metrics into cosmological frameworks, interpreting the validity period of the Kontsevich-Segal criterion through this transformation.

Conclusion: The approach demonstrates a viable method to study black hole interiors and cosmological models within QFT, suggesting potential for further exploration into quantum gravity effects.

Abstract: We use diffeomorphic mappings to connect black hole metrics with complex solutions allowed by the Kontsevich-Segal criterion. By swapping radial and time-like coordinates and applying complex mappings, we derive dynamic metrics suitable for a Quantum Field Theory. This is shown for static and rotating black holes, mapping their interiors into the Kantowski-Sachs and a specific Gowdy-type cosmological model. We offer interpretations of the period during which the Kontsevich-Segal criterion holds.

</details>


### [87] [Multipartite entanglement features of primordial non-gaussianities](https://arxiv.org/abs/2511.23389)
*Alessio Belfiglio,Roberto Franzosi,Orlando Luongo*

Main category: gr-qc

TL;DR: The paper examines the emergence of multipartite entanglement in single-field inflation through cubic non-Gaussian perturbations. Using the Entanglement Distance metric, it demonstrates that quantum correlations from cubic interactions exceed standard squeezing effects and establishes their dependence on inflationary energy scales and e-folding numbers.


<details>
  <summary>Details</summary>
Motivation: To understand quantum entanglement dynamics during inflation, particularly how cubic non-Gaussian perturbations generate multipartite entanglement and quantify their impact compared to squeezing effects.

Method: Employed momentum-space techniques and the Entanglement Distance measure based on the Fubini-Study metric to analyze the comoving curvature perturbation in the interaction picture. Calculated entanglement bounds via displacement transformations and explored parameter dependencies.

Result: Found that entanglement from cubic interactions is significantly larger than squeezing contributions, with Entanglement Distance proportional to the number of excitations. Established that correlations scale with energy scales and e-folding duration.

Conclusion: Cubic gravitational interactions in inflation strongly influence quantum correlations, offering a geometric perspective through Entanglement Distance. Results underscore the importance of considering entanglement metrics beyond traditional entropy measures in cosmological perturbation analysis.

Abstract: We discuss some entanglement features associated with cubic non-Gaussian perturbations in single-field inflationary scenarios. We adopt standard momentum-space techniques to show how multipartite entanglement arises for inflationary perturbation modes, focusing on the dynamics of the comoving curvature perturbation. In particular, we quantify entanglement generation via the recently proposed Entanglement Distance, which introduces a geometric interpretation of quantum correlations in terms of the Fubini-Study metric. In the continuum limit, we show that the Entanglement Distance arising from displacement transformations is proportional to the total number of excitations in the quantum state for cubic perturbations, thus providing an upper bound on the von Neumann entanglement entropy of any reduced state compatible with such excitations. Within the interaction picture, we further observe that the quantum correlations arising from cubic gravitational interactions are typically much larger than the standard squeezing contribution, in agreement with previous studies focusing on von Neumann entropy generation across the Hubble horizon. We further show how the inflationary parameters affect the total amount of such correlations, highlighting in particular their dependence on the inflationary energy scales and the number of e-foldings during slow-roll.

</details>


### [88] [Cosmology in generalized hybrid metric-Palatini with matter-geometry coupling](https://arxiv.org/abs/2511.23396)
*Reza Jalali,Shahab Shahidi,Mohammad Hossein Zhoolideh Haghighi*

Main category: gr-qc

TL;DR: The paper explores a hybrid metric-Palatini gravity model with non-minimal matter-geometry coupling as an alternative to ΛCDM, using observational data and statefind analysis.


<details>
  <summary>Details</summary>
Motivation: To find alternatives to ΛCDM cosmology that explain cosmic acceleration without dark energy, considering the issue of matter conservation.

Method: Reformulated the theory as a bi-scalar-tensor model with non-minimal coupling, then tested with Cosmic Chronometers, DESI BAO, and Pantheon+ data. Performed statefinder analysis for dark energy behavior.

Result: The model matches observational data similarly to ΛCDM. It shows a quintessence-to-phantom dark energy transition at z≈0.86. Baryonic matter conservation only holds at background level.

Conclusion: The hybrid model offers a viable alternative to ΛCDM with unique dark energy dynamics, though matter conservation differs from standard models.

Abstract: Cosmological implications of a class of hybrid metric-Palatini gravity with a non-minimal matter-geometry coupling is considered. The theory contains a metric curvature tensor, together with a curvature tensor constructed from an independent affine connection. We will show that the model could be written as a bi-scalar-tensor gravity with a non-minimal coupling between matter sector and a scalar field. The theory will then be confronted with observational data from Cosmic Chronometers, BAO dataset from DESI and the Pantheon$^+$ dataset. We will show that the theory could be a good alternative to the $Λ$CDM model with the difference that the conservation of the baryonic matter sector holds only at the background level. The statefinder analysis will also be applied to the theory and it is observed that the DE behavior of the theory exhibits a quintessence to phantom transition occurs at redshifts around $z\approx0.86$.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [89] [The LMT 2 Millimeter Receiver System (B4R). I. Overview and Results of Science Demonstration](https://arxiv.org/abs/2511.21816)
*Ryohei Kawabe,Takeshi Sakai,Kunihiko Tanaka,Akio Taniguchi,Bunyo Hatsukade,Yoichi Tamura,Yuki Yoshimura,Tatsuya Takekoshi,Tai Oshima,Masato Hagimoto,Teppei Yonetsu,Kotomi Taniguchi,Kotaro Kohno,Hiroyuki Maezawa,David H. Hughes,Peter F. Schloerb,Edgar Colín-Beltrán,Miguel Chávez Dagostino,Víctor Gómez-Rivera,Arturo I. Gómez-Ruiz,Gopal Narayanan,Iván Rodríguez-Montoya,David Sánchez-Argüelles,Yoshito Shimajiri,Kamal Souccar,Min S. Yun,Tom J. L. C. Bakx,Kevin C. Harrington,Shinji Fujita,Fumitaka Nakamura,O. S. Rojas-García,Alfredo A. Montaña Barbano,Javier Zaragoza-Cardiel*

Main category: astro-ph.IM

TL;DR: The paper details the on-sky testing and science demonstration of the B4R receiver on the LMT, showing its ability to detect high-redshift galaxies and expand spectroscopic capabilities in the 2 mm band.


<details>
  <summary>Details</summary>
Motivation: To complement existing receivers and enable detection of multiple CO lines for redshift identification, while broadening LMT's scientific applications to include astrochemistry.

Method: Deployed B4R receiver with XFFTS spectrometer, conducted on-sky tests and observations of Orion Molecular Cloud 1 and high-redshift galaxies using on-the-fly mapping and position-switching techniques. Measured LMT's aperture efficiency across 130-160 GHz.

Result: B4R met performance specs, validated LMT's aperture efficiency aligning with design expectations, enabling sensitive 2 mm single-dish spectroscopic observations.

Conclusion: Successfully demonstrated B4R's operational capability, expanding LMT's observational prowess for studying distant galaxies and molecular processes via 2 mm spectral lines.

Abstract: We report on the results of the on-sky test and science demonstration conducted with the 2 mm receiver system, B4R, on the 50 m Large Millimeter Telescope (LMT), located at an altitude of 4600 m in Mexico. The B4R receiver was developed based on the dual-polarization sideband-separating mixer technology of the Atacama Large Millimeter/submillimeter Array, and is equipped with a fast Fourier transform digital spectrometer, XFFTS. The primary science objective is the spectroscopic redshift identification of high-redshift dusty star-forming galaxies, complementing the existing 3 mm Redshift Search Receiver by enabling the detection of multiple carbon monoxide lines. Additionally, the B4R receiver broadens the range of science cases possible with the LMT, including astrochemistry, as the 2 mm band encompasses unique molecular lines such as deuterated molecules and shock tracers. During on-site commissioning in 2018 and 2019, we successfully demonstrated on-the-fly mapping and position-switching observations toward the Orion Molecular Cloud 1 and bright high-redshift dusty star-forming galaxies, respectively. We confirmed that the installed B4R system largely met its basic performance specifications. Furthermore, we measured the LMT's aperture efficiencies across the entire B4R frequency range (130-160 GHz), finding them to be roughly consistent with expectations based on a surface accuracy of 100 $μ$m and the receiver optics design. These results with the B4R will enable the most sensitive single-dish spectroscopic observations at 2 mm using the LMT.

</details>


### [90] [A Catalogue of Mid-infrared Variable Sources from unTimely](https://arxiv.org/abs/2511.22071)
*Zihan kang,Jingyi Zhang,Yanxia Zhang,Changhua Li,Xiao Kong,Minzhi Kong,Jinghang Shi,Shirui Wei,Xue-Bing Wu*

Main category: astro-ph.IM

TL;DR: The authors created the first large-scale mid-infrared variability catalog using WISE/NEOWISE data, identifying millions of variable sources and rare objects through Bayesian analysis, advancing time-domain astronomy in this spectral range.


<details>
  <summary>Details</summary>
Motivation: WISE/NEOWISE missions provide unique mid-infrared all-sky time-domain data, but no comprehensive variability catalog existed. The need was to exploit this data for variability studies and discover new transient phenomena.

Method: Used unTimely coadded photometry with a Bayesian Gaussian mixture model and Dirichlet process for variable identification. Outlier detection algorithms were applied to find rare objects.

Result: Identified 8,256,042 variables in W1 and 7,147,661 in W2 bands. Discovered eruptive young stellar objects, variable AGN, and other transients through outlier detection.

Conclusion: This catalog expands mid-infrared time-domain astronomy, enabling studies of stellar evolution, accretion, and dust-obscured environments across galaxies. It complements optical surveys and opens new research avenues in astrophysics.

Abstract: The WISE and NEOWISE missions have provided the only mid-infrared all-sky time-domain data, opening a unique observational window for variability studies. Yet, a comprehensive and systematic catalog of mid-infrared variable sources has remained unavailable. In this work, we construct the first large-scale mid-infrared variability catalog based on the unTimely coadded photometry, covering tens of millions of sources. By employing a Bayesian Gaussian mixture model with a Dirichlet process, we identified 8,256,042 variable sources in the W1 band and 7,147,661 in the W2 band, significantly expanding the landscape of known mid-infrared variables. In addition to robust variability metrics, our analysis highlights rare and extreme outliers through dedicated outlier-detection algorithms, enabling the discovery of unusual classes of objects such as eruptive young stellar objects, highly variable active galactic nuclei, and other rare transients. This unprecedented dataset provides a new foundation for time-domain astronomy in the mid-infrared, offering complementary insights to optical and near-infrared surveys, and opening the door to systematic investigations of stellar evolution, accretion processes, and dust-enshrouded astrophysical environments on a Galactic and extragalactic scale.

</details>


### [91] [A Novel Comb Generator for Frequency Phase Transfer](https://arxiv.org/abs/2511.22161)
*Gabriella Montano,Cheukyu Edward Tong,Dan Marrone*

Main category: astro-ph.IM

TL;DR: The paper describes the development and testing of a millimeter-wave frequency comb generator for the BHEX mission, which uses a microwave phase modulator to produce phase-coherent comb signals for tracking instrumental delays in a dual-band receiver system.


<details>
  <summary>Details</summary>
Motivation: To enable precise delay tracking in the BHEX mission's dual-band receiver (90 and 270 GHz) for space VLBI, requiring a multi-octave bandwidth comb generator.

Method: A microwave phase modulator-based comb generator was designed and tested with astronomical receivers to validate its phase-coherent operation and bandwidth performance.

Result: Testing confirmed the comb generator's expected performance, demonstrating its capability to produce the required signals for delay tracking in the BHEX receiver system.

Conclusion: The developed comb generator meets mission specifications, providing a viable solution for phase-coherent delay tracking in space VLBI systems like BHEX.

Abstract: This paper presents the design and testing of a millimeter-wave frequency comb generator developed for the Black Hole Explorer (BHEX) mission, a space Very-Long-Baseline Interferometry (VLBI) mission concept. The heart of BHEX is a dual-band receiver, centered at 90 and 270 GHz. This novel comb generator is based on a microwave phase modulator producing phase-coherent comb signals with multi-octave bandwidth, which will be used to track instrumental delays when injected into the receiver system. The comb generator was tested with astronomical receivers, which confirms its expected operation.

</details>


### [92] [Recovering Intrinsic Pulsar Profiles and Scattering Parameters with a CLEAN-Based Algorithm for High-Precision Timing](https://arxiv.org/abs/2511.22299)
*Adarsh Bathula,M. A. Krishnakumar,S. Jena*

Main category: astro-ph.IM

TL;DR: The paper evaluates a CLEAN-based algorithm for recovering intrinsic pulsar profiles and parameters using simulated Gaussian profiles, assessing its capabilities and limitations.


<details>
  <summary>Details</summary>
Motivation: Accurate recovery of pulsar profiles and parameters like dispersion measure and scattering time is critical in high-precision pulsar timing, necessitating rigorous testing of algorithms.

Method: Simulated single and multi-component Gaussian profiles are used to test a CLEAN-based pipeline. Recovered profiles and parameters are compared to injected values from simulations.

Result: The pipeline's performance is evaluated, demonstrating its effectiveness in parameter recovery while identifying drawbacks and limitations.

Conclusion: The study highlights the algorithm's capabilities and areas needing improvement, offering insights for optimizing pulsar timing analysis.

Abstract: In high precision pulsar timing, the accurate recovery of intrinsic pulsar profiles and their associated scattering parameters is of paramount importance. In this paper, we present a comprehensive study focused on the retrieval of intrinsic pulsar profiles through the utilization of a CLEAN-based algorithm as described in Bhat et al. (2003). The primary objective of this study is to elucidate the capabilities of our pipeline in the context of recovering the intrinsic profiles and associated parameters, such as dispersion measure, frequency scaling index, scattering time, pulse broadening function, and time of arrival residuals. We use simulated profiles to rigorously test and validate the efficiency of our recovery pipeline. These simulated profiles encompass single and multi-component Gaussians, designed to emulate the diverse nature of pulsar profiles. By comparing the recovered profiles and parameters to their injected values, as derived from simulations, we provide a robust evaluation of the pipeline's performance along with its drawbacks and limitations.

</details>


### [93] [The NANOGrav 12.5-year Data Set: Chromatic Noise Characterization & Mitigation with Time-Domain Kernels](https://arxiv.org/abs/2511.22597)
*Jeffrey S. Hazboun,Joseph Simon,Jeremy Baier,Bjorn Larsen,Daniel J. Oliver,Paul T. Baker,Bence Bécsy,Siyuan Chen,Alberto Diaz Hernandez,Justin A. Ellis,A. Miguel Holgado,Kristina Islo,Aaron Johnson,Andrew R. Kaiser,Nima Laal,Alexander McEwen,Nihan S. Pol,Joey Shapiro Key,Min Young Kim,Matthew Samson,Brent J. Shapiro-Albert,Jerry P. Sun,Stephen R. Taylor,Caitlin A. Witt,Jeremy Volpe,Christine Ye,Harsha Blumer,Paul R. Brook,Shami Chatterjee,James M. Cordes,Fronefield Crawford,H. Thankful Cromartie,Megan E. DeCesar,Paul B. Demorest,Timothy Dolch,Robert D. Ferdman,Elizabeth C. Ferrara,William Fiore,Emmanuel Fonseca,Nathan Garver-Daniels,Peter A. Gentile,Deborah C. Good,Ross J. Jennings,Megan L. Jones,David L. Kaplan,Michael T. Lam,T. Joseph W. Lazio,Duncan R. Lorimer,Jing Luo,Ryan S. Lynch,Dustin R. Madison,Maura A. McLaughlin,Chiara M. F. Mingarelli,Cherry Ng,David J. Nice,Timothy T. Pennucci,Scott M. Ransom,Paul S. Ray,Xavier Siemens,Renée Spiewak,Ingrid H. Stairs,Daniel R. Stinebring,Kevin Stovall,Joseph K. Swiggum,Jacob E. Turner,Michele Vallisneri,Sarah J. Vigeland*

Main category: astro-ph.IM

TL;DR: The paper introduces time-domain Gaussian process models for chromatic noise in pulsar timing arrays (PTAs), demonstrating their effectiveness and computational advantages over traditional Fourier-domain methods, particularly at higher frequencies where scalability becomes critical.


<details>
  <summary>Details</summary>
Motivation: To address the varying techniques among PTA collaborations in modeling chromatic noise and improve computational efficiency as PTAs aim for higher-frequency sensitivity.

Method: Developed time-domain kernel-based Gaussian processes to model chromatic noise, implemented Bayesian model selection across different kernels and deterministic models for non-stationary events like the solar wind.

Result: Time-domain models are computationally efficient and equivalent to Fourier-domain models in mitigating chromatic noise, beneficial for future high-frequency PTA operations.

Conclusion: Time-domain approaches offer scalable solutions for upcoming PTA requirements, emphasizing their role in advancing the field beyond the GWB detection era.

Abstract: Pulsar timing arrays (PTAs) have recently entered the detection era, quickly moving beyond the goal of simply improving sensitivity at the lowest frequencies for the sake of observing the stochastic gravitational wave background (GWB), and focusing on its accurate spectral characterization. While all PTA collaborations around the world use Fourier-domain Gaussian processes to model the GWB and intrinsic long time-correlated (red) noise, techniques to model the time-correlated radio frequency-dependent (chromatic) processes have varied from collaboration to collaboration. Here we test a new class of models for PTA data, Gaussian processes based on time-domain kernels that model the statistics of the chromatic processes starting from the covariance matrix. As we will show, these models can be effectively equivalent to Fourier-domain models in mitigating chromatic noise. This work presents a method for Bayesian model selection across the various choices of kernel as well as deterministic chromatic models for non-stationary chromatic events and the solar wind. As PTAs turn towards high frequency (>1/yr) sensitivity, the size of the basis used to model these processes will need to increase, and these time-domain models present some computational efficiencies compared to Fourier-domain models.

</details>


### [94] [Identifying Transient Hosts in LSST's Deep Drilling Fields with Galaxy Catalogues](https://arxiv.org/abs/2511.22634)
*Josh G. Weston,David R. Young,Stephen J. Smartt,Matt Nicholl,Matt J. Jarvis,I. H. Whittam*

Main category: astro-ph.IM

TL;DR: This paper explores methods for identifying host galaxies of distant astrophysical transients detected by the LSST Deep Drilling Fields using pre-existing deep catalogues, evaluating accuracy and computational efficiency through DES supernova data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of associating transients with host galaxies in LSST DDFs where background galaxy catalogs are too shallow, necessitating efficient use of existing deep field-specific data.

Method: Compiled 70 deep catalogues for DDFs, created homogenized thin catalogues for matching, ranked catalogues by utilities like redshift/morphology data. Tested methods on DES supernova data with known hosts, applied machine learning (XGBoost) for contaminant filtering and confidence scoring.

Result: Demonstrated effective host galaxy association with high accuracy, developed lightweight processing methods suitable for real-time LSST data streams, validated contamination reduction techniques using DES datasets.

Conclusion: Proposes optimized catalogue integration and machine learning pipelines as viable solutions for LSST's transient-host association needs, emphasizing computational efficiency without sacrificing accuracy.

Abstract: The upcoming Vera C. Rubin Observatory Legacy Survey of Space and Time (LSST) will enable astronomers to discover rare and distant astrophysical transients. Host-galaxy association is crucial for selecting the most scientifically interesting transients for follow-up. LSST Deep Drilling Field observations will detect distant transients occurring in galaxies below the detection limits of most all-sky catalogues. Here we investigate the use of pre-existing smaller-scale, field-specific catalogues for host identification in the Deep Drilling Fields (DDFs) and a ranking of their usefulness. We have compiled a database of 70 deep catalogues that overlap with the Rubin DDFs and constructed thin catalogues to be homogenised and combined for transient-host matching. A systematic ranking of their utility is discussed and applied based on the inclusion of information such as spectroscopic redshifts and morphological information. Utilising this data against a Dark Energy Survey (DES) sample of supernovae with pre-identified hosts in the XMM-LSS and ECDFS fields, we evaluate different methods for transient-host association in terms of both accuracy and processing speed. We also apply light data-cleaning techniques to identify and remove contaminants within our associations, such as diffraction spikes and blended galaxies where the correct host cannot be determined with confidence. We use a lightweight machine learning approach in the form of extreme gradient boosting to generate confidence scores in our contaminant selections and associated metrics. Finally, we discuss the computational expense of implementation within the LSST transient alert brokers, which will require efficient, fast-paced processing to handle the large stream of survey data.

</details>


### [95] [Structure-Preserving Unpaired Image Translation to Photometrically Calibrate JunoCam with Hubble Data](https://arxiv.org/abs/2511.22668)
*Aditya Pratap Singh,Shrey Shah,Ramanakumar Sankar,Emma Dahl,Gerald Eichstädt,Georgios Georgakis,Bernadette Bucher*

Main category: astro-ph.IM

TL;DR: The paper introduces a novel method (SP-I2I) to address the lack of absolute photometric calibration in JunoCam images by translating them into Hubble Space Telescope (HST) equivalents using unpaired image-to-image translation, preserving high-frequency features for detailed Jupiter atmosphere analysis.


<details>
  <summary>Details</summary>
Motivation: To overcome JunoCam's photometric calibration limitations, enabling quantitative analysis of Jupiter's atmosphere using high-resolution data while leveraging HST observations as a reference.

Method: The proposed SP-I2I method uses unpaired image-to-image translation with explicit frequency-space constraints to maintain high-frequency features, ensuring fine-scale atmospheric structures are retained despite resolution differences between JunoCam and HST.

Result: Demonstrated that existing unpaired I2I methods fail in this task, while SP-I2I successfully preserves critical atmospheric details, enhancing JunoCam's observational data utility for scientific analysis and pansharpening applications.

Conclusion: SP-I2I provides an effective solution for calibrating JunoCam data, advancing studies of Jupiter's atmosphere and offering broader impacts for remote sensing data integration and pansharpening in planetary science.

Abstract: Insights into Jupiter's atmospheric dynamics are vital for understanding planetary meteorology and exoplanetary gas giant atmospheres. To study these dynamics, we require high-resolution, photometrically calibrated observations. Over the last 9 years, the Juno spacecraft's optical camera, JunoCam, has generated a unique dataset with high spatial resolution, wide coverage during perijove passes, and a long baseline. However, JunoCam lacks absolute photometric calibration, hindering quantitative analysis of the Jovian atmosphere. Using observations from the Hubble Space Telescope (HST) as a proxy for a calibrated sensor, we present a novel method for performing unpaired image-to-image translation (I2I) between JunoCam and HST, focusing on addressing the resolution discrepancy between the two sensors. Our structure-preserving I2I method, SP-I2I, incorporates explicit frequency-space constraints designed to preserve high-frequency features ensuring the retention of fine, small-scale spatial structures - essential for studying Jupiter's atmosphere. We demonstrate that state-of-the-art unpaired image-to-image translation methods are inadequate to address this problem, and, importantly, we show the broader impact of our proposed solution on relevant remote sensing data for the pansharpening task.

</details>


### [96] [The Parkes Observatory Pulsar Data Archive](https://arxiv.org/abs/2511.22702)
*Lawrence Toomey,George Hobbs,James Dempsey,Shane Majewski,Shi Dai,John Reynolds*

Main category: astro-ph.IM

TL;DR: The CSIRO Data Access Portal has made pulsar data from the Parkes radio-telescope highly accessible, housing nearly 2 million files from over 400 projects. The article details the archive's status, data processes, and new findings from these datasets.


<details>
  <summary>Details</summary>
Motivation: To enhance accessibility and utilization of pulsar observation data, ensuring long-term preservation and enabling new scientific discoveries through archived information.

Method: The paper outlines the infrastructure and processes for data acquisition, analysis, reduction, visualization, preservation, and dissemination via the CSIRO Data Access Portal. It also presents case studies or examples of new results derived from analyzing archival data.

Result: The archive successfully hosts a vast dataset facilitating global research. New scientific outcomes are showcased, demonstrating the archive's value in advancing pulsar studies.

Conclusion: Comprehensive archives like the CSIRO Data Access Portal are crucial for ongoing astronomical research, enabling both current and future discoveries through accessible, well-maintained datasets.

Abstract: Data from observations of pulsars made by Murriyang, the CSIRO Parkes 64-metre radio-telescope over the last three decades are more accessible than ever before, largely due to their storage in expansive long-term archives. Containing nearly 2 million files from more than 400 Parkes pulsar projects, CSIRO's Data Access Portal is leading the global effort in making pulsar data accessible. In this article, we present the current status of the archive, and provide information about the acquisition, analysis, reduction, visualisation, preservation and dissemination of these data sets. We highlight the importance of such an archive, and present a selection of new results emanating from archival data.

</details>


### [97] [The observation of Ground Level Enhancement GLE 77 by the neutron detectors of the Experimental Complex NEVOD](https://arxiv.org/abs/2511.23048)
*Evgenii Volkov,Kseniia Chelidze,Dmitrii Gromushkin,Semen Khokhlov,Evgenii Khomchuk,Anatoly Petrukhin,Ivan Shulzhenko*

Main category: astro-ph.IM

TL;DR: The paper reports the first detection of a Ground Level Enhancement (GLE) event using neutron detectors originally designed for studying extensive air showers at the NEVOD complex. The GLE was observed on November 11, 2025, through newly added neutron flux measurement channels in the PRISMA-36 and URAN arrays, showcasing the potential of modified setups for capturing such events.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the capability of modernized neutron detector arrays (PRISMA-36 and URAN) at NEVOD to detect GLE events, which are high-energy solar particles reaching near-Earth space. The motivation includes advancing space weather monitoring and understanding solar-terrestrial interactions.

Method: The study utilized neutron detectors at the NEVOD complex, which were recently upgraded with additional channels for neutron flux measurements. Data from the PRISMA-36 and URAN arrays recorded a sudden neutron flux increase on November 11, 2025. Preliminary analysis of the observed event was conducted using the collected data.

Result: A significant GLE event was detected for the first time via neutron detectors designed for air shower studies. The results confirm the effectivenessof the modernized setup in capturing such phenomena, providing new data for space weather events analysis.

Conclusion: The successful detection highlights the potential of repurposing air shower detector arrays for GLE observations, suggesting further modernizations could enhance space weather monitoring capabilities. This opens new avenues for studying solar particle events and their impacts on near-Earth environments.

Abstract: A Ground Level Enhancement event was observed by neutron detectors designed for the registration of extensive air showers at the Experimental Complex NEVOD. The potential for that was unlocked by a recent modernization of the experimental setup that included implementation of additional channels for measuring neutron flux variation. At 10:15 UT on November 11, 2025, a sudden and significant increase in the neutron flux was detected by two installations: PRISMA-36 and URAN arrays. For the first time, a GLE has been recorded using a set of neutron detectors oriented at the extensive air shower studies. We present the measured EC NEVOD data and the results of the preliminary analysis of the observed GLE.

</details>


### [98] [SlotFlow: Amortized Trans-Dimensional Inference with Slot-Based Normalizing Flows](https://arxiv.org/abs/2511.23228)
*Niklas Houba,Giovanni Giarda,Lorenzo Speri*

Main category: astro-ph.IM

TL;DR: SlotFlow is a deep learning architecture that efficiently performs trans-dimensional inference for time-series data, combining frequency and time-domain analysis to estimate the number of components and their parameters with high accuracy and speed, offering a significant improvement over traditional methods like RJMCMC.


<details>
  <summary>Details</summary>
Motivation: The challenge of inferring the number of components and their parameters in observations across various fields, with classical methods being computationally intensive and deep learning approaches often fixing component counts. SlotFlow aims to bridge this gap by providing a fast, scalable solution that handles variable component numbers.

Method: SlotFlow uses parallel encoders for time and frequency domains, a classifier for component count distribution, permutation-invariant Hungarian matching for parameter estimation via shared conditional normalizing flows. The model processes time-series data to estimate component counts and parameters through a factorized posterior with global context capture.

Result: In sinusoidal decomposition tasks, SlotFlow achieved 99.85% accuracy in component counting, well-calibrated parameter posteriors with minimal bias, and close agreement with RJMCMC (Wasserstein distances <0.03). It also offered a million-times speedup over RJMCMC, demonstrating practical utility in fields like gravitational-wave astronomy.

Conclusion: SlotFlow effectively addresses trans-dimensional inference challenges with high efficiency and accuracy, balancing modern deep learning scalability with the precision of Bayesian methods. Its applications span astronomy, neuroscience, and other domains requiring real-time analysis of complex time-series data.

Abstract: Inferring the number of distinct components contributing to an observation, while simultaneously estimating their parameters, remains a long-standing challenge across signal processing, astrophysics, and neuroscience. Classical trans-dimensional Bayesian methods such as Reversible Jump Markov Chain Monte Carlo (RJMCMC) provide asymptotically exact inference but can be computationally expensive. Instead, modern deep learning provides a faster alternative to inference but typically assume fixed component counts, sidestepping the core challenge of trans-dimensionality. To address this, we introduce SlotFlow, a deep learning architecture for trans-dimensional amortized inference. The architecture processes time-series observations, which we represent jointly in the frequency and time domains through parallel encoders. A classifier produces a distribution over component counts K, and its MAP estimate specifies the number of slots instantiated. Each slot is parameterized by a shared conditional normalizing flow trained via permutation-invariant Hungarian matching. On sinusoidal decomposition with up to 10 overlapping components and Gaussian noise, SlotFlow achieves 99.85% cardinality accuracy and well-calibrated parameter posteriors, with systematic biases well below one posterior standard deviation. Direct comparison with RJMCMC shows close agreement in amplitude and phase, with Wasserstein distances $W_2 < 0.01$ and $< 0.03$, indicating that shared global context captures inter-component structure despite a factorized posterior. Frequency posteriors remain centered but exhibit 2-3x broader intervals, consistent with an encoder bottleneck in retaining long-baseline phase coherence. The method delivers a $\sim 10^6\times$ speedup over RJMCMC, suggesting applicability to time-critical workflows in gravitational-wave astronomy, neural spike sorting, and object-centric vision.

</details>


### [99] [Tools and Advancements towards Data Standardization of the MAGIC Collaboration](https://arxiv.org/abs/2511.23244)
*C. P. Walther,C. Nigro,D. Elsässer,W. Rhode*

Main category: astro-ph.IM

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Gamma-ray astronomy is able to acquire large data volumes that astronomers use to draw scientific conclusions from. Ensuring the possibility of accessing and utilizing this data also after the lifetime of currently running experiments requires the use of a standardized data format. Following the data standardization format proposed by the gamma-ray astronomy community, we present 104 h of the first production of 166 h of data from the MAGIC Imaging Air Cherenkov Telescopes in standardized data format. Six datasets were processed from which three are presented, all of which have been analyzed and validated through comparison using the open-source software Gammapy and the MAGIC analysis software MARS.
  Furthermore, looking towards a large-scale production of standardized data and a legacy of the data taken by the MAGIC experiment, we have developed and implemented the automated database-driven MAGIC data reduction tool autoMAGIC which offers a reliable and reproducible way to produce high-level datasets. By utilizing the automatization of parameter configuration choices, the software allows for a reduction of human error as well as an acceleration in the production of standardized data. Here, we also show comparable results for data processed with manual and automatic methods.

</details>
