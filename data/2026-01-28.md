<div id=toc></div>

# Table of Contents

- [gr-qc](#gr-qc) [Total: 17]
- [hep-ph](#hep-ph) [Total: 29]
- [astro-ph.HE](#astro-ph.HE) [Total: 14]
- [astro-ph.IM](#astro-ph.IM) [Total: 7]


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [1] [The stealth Kerr solution in the bumblebee gravity](https://arxiv.org/abs/2601.18809)
*Rui Xu,Zhan-Feng Mai,Dicong Liang*

Main category: gr-qc

TL;DR: The paper explores solutions in the bumblebee model of vector-tensor gravity, showing that Kerr metrics with vector fields can be generated via the Newman-Janis algorithm from Schwarzschild solutions, offering a simple non-GR example where the algorithm applies.


<details>
  <summary>Details</summary>
Motivation: To investigate black hole solutions in the bumblebee model and validate the applicability of the Newman-Janis algorithm beyond general relativity.

Method: Derive Kerr-like solutions with vector fields in the bumblebee model by applying the Newman-Janis algorithm to Schwarzschild metrics with spherical vector fields.

Result: Successful generation of Kerr solutions coupled with vector fields using the Newman-Janis method in the bumblebee model, establishing a simple non-GR case where the algorithm works.

Conclusion: The bumblebee model provides a minimalist framework demonstrating the Newman-Janis algorithm's validity for producing rotating black holes beyond general relativity.

Abstract: In this paper, we find Kerr solution accompanied with a nontrivial vector field as a solution to one of the simplest vector-tensor theories of gravity, namely the bumblebee model with an intriguing coupling constant between the Ricci curvature tensor and the vector field. We also demonstrate that the accompanied vector field can be generated via the Newman-Janis algorithm from a simple spherical vector field, which together with the Schwarzschild metric constitutes a solution to the same bumblebee model. It is probably the simplest example of a theory and its black-hole solutions for the Newman-Janis algorithm to hold except for general relativity.

</details>


### [2] [Dark energy and a new realization of the matter Lagrangian](https://arxiv.org/abs/2601.18825)
*Shahab Shahidi,Sedigheh Farahzad*

Main category: gr-qc

TL;DR: The paper introduces a new matter Lagrangian modeling dark energy using non-standard baryonic matter thermodynamics, demonstrating independent properties from existing models, separate energy-momentum conservation, and testing with observational data.


<details>
  <summary>Details</summary>
Motivation: To explore alternative models of dark energy beyond standard approaches like ΛCDM, addressing the need for theories where dark energy interacts with baryonic matter through novel thermodynamic couplings while maintaining energy conservation.

Method: 1. Propose a new Lagrangian formulation coupling dark energy to baryonic matter's thermodynamic properties. 2. Compare with existing models to establish independence. 3. Analyze background dynamics and perturbations to constrain Lagrangian forms. 4. Test a logarithmic dark energy model using cosmic chronometers, Pantheon+, and fσ8 data.

Result: The new formulation shows distinct behavior from ΛCDM, satisfies independent energy conservation for baryonic and dark sectors, and observational data constrains model parameters effectively without ruling out the model. Differences in late-universe evolution predictions are identified.

Conclusion: This approach offers a viable alternative to ΛCDM by integrating dark energy with baryonic thermodynamics, potentially resolving some cosmological tensions through distinct evolutionary paths validated by observational evidence.

Abstract: A new realization of the matter Lagrangian is introduced which models the dark energy component as a non-standard combination of thermodynamics quantities of the baryonic matter. We will prove that the present realization is independent of existing models with matter-geometry couplings and has a property that the energy-momentum tensor of both baryonic matter and dark energy is conserved separately. We further show that two possible choices of the matter Lagrangian in the $Λ$CDM model are not totally equivalent and investigate the background and perturbative constraints on the form of matter Lagrangian. We will also investigate cosmological implications of a test model with logarithmic DE and obtain the model parameters by confronting the model with observational data on the cosmic chronometers, Pantheon$^+$ and $fσ_8$ datasets. We will also explain in details the predictions of the model on the late time behavior of the universe and compare the result with $Λ$CDM model.

</details>


### [3] [Reconsidering the consistent use of precessing, higher order multipole models for gravitational wave analyses](https://arxiv.org/abs/2601.18916)
*Charlie Hoy*

Main category: gr-qc

TL;DR: The paper proposes a selection criterion to use computationally cheaper gravitational-wave models for analyzing black hole binaries, reducing computational costs while maintaining accurate population property estimates. This is crucial as the number of observations grows, ensuring efficient resource use without biasing results on mass and spin distributions.


<details>
  <summary>Details</summary>
Motivation: The increasing number of gravitational-wave observations requires efficient analysis methods to avoid computational infeasibility when using the most accurate but expensive models. Direct measurements of general relativistic effects like spin precession are rare, making current methods unsustainable as detections rise.

Method: The authors developed a selection criterion to identify when less accurate but cheaper models can be used without biasing population-level inferences. They tested this criterion on a simulated 'worst-case' black hole population and an astrophysically motivated scenario.

Result: Using the criterion reduced computational cost by ~20% for the worst-case population and up to 78% for an astrophysical population, while yielding comparable estimates of mass and spin distributions. This allows maintaining analysis accuracy with significant resource savings.

Conclusion: The proposed criterion enables sustainable Bayesian inference at scale for gravitational-wave populations, ensuring continued scientific insights without compromising on results as observational data expands.

Abstract: The growing number of gravitational-wave (GW) observations allows for constraints to be placed on the underlying population of black holes; current estimates show that black hole spins are small, with binaries more likely to have comparable component masses. Since general relativistic effects, such as spin-induced orbital precession and higher order multipole moments, are more likely to be observed for asymmetric binary systems, a direct measurement remains unlikely. Nevertheless, we continue to consistently probe these effects by performing Bayesian inference with our most accurate and computationally expensive models. As the number of GW detections increases, it may soon become infeasible to consistently use these models for analyses. In this paper, we provide a selection criterion that determines when less accurate and computationally cheaper models can be used without giving biased estimates for the population properties of black holes in the Universe. We show that when using our selection criterion, comparable estimates can be obtained for the underlying mass and spin distribution of black holes for a simulated "worst-case" scenario population, while reducing the overall cost of performing Bayesian inference on our population by $\sim 20\%$. We anticipate a reduction of up to $78\%$ in the overall cost for an astrophysically motivated population, since there are fewer events with observable spin-precession and higher order multipole power.

</details>


### [4] [Inertial-to-Rindler Coordinates, with applications to the Twin Paradox, Radar Time and the Unruh Temperature](https://arxiv.org/abs/2601.18956)
*Paul M. Alsing*

Main category: gr-qc

TL;DR: This paper introduces a two-parameter family of Inertial-to-Rindler (I2R) coordinates in Minkowski spacetime, which interpolate between inertial and Rindler coordinates. It addresses the Twin Paradox scenarios, analyzes simultaneity hypersurfaces using radar time, and derives perturbative corrections to the Unruh temperature for observers under varying accelerations.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between inertial and uniformly accelerated reference frames, resolving discrepancies in the Twin Paradox interpretations and extending the Unruh effect to non-constant acceleration scenarios.

Method: Formulate I2R coordinates, apply them to the Twin Paradox cases, use radar time formulation to study simultaneity hypersurfaces, and analyze frequency content of plane waves for perturbative Unruh temperature corrections through velocity-dependent generalization.

Result: Successfully modeled acceleration transitions with I2R coordinates, validated both Twin Paradox scenarios, showed how simultaneity hypersurfaces evolve during acceleration, and derived modulated Unruh temperatures dependent on initial/final velocities.

Conclusion: I2R coordinates provide a unified framework for describing acceleration transitions in Minkowski spacetime, offering deeper insights into relativistic effects like the Twin Paradox and extending the Unruh effect's applicability beyond constant acceleration cases.

Abstract: In this work we formulate a two-parameter family of transformations in flat Minkowksi spacetime that smoothly interpolates between motion with constant initial/final velocity (inertial coordinates), and with constant acceleration (Rindler coordinates \cite{Rindler:1956}), which we term Inertial-to-Rindler (I2R) coordinates. We revisit the Twin ``Paradox" and show how the new I2R coordinates justify the ``immediate-" and ``gradual-turnaround" scenarios discussed in many texbooks and articles. We also examine the radar time formulation of hypersurfaces of simultaneity by Dolby and Gull \cite{Dolby_Gull:2001} for these new coordinates as we transition from zero to uniform acceleration. Finaly we re-examine the negative frequency content of a purely positive frequency Minkowski plane wave as observed by the I2R observer, and derive perturbative corrections to the Unruh \cite{Unruh:1976} temperature for the two cases of initial/final velocities slightly greater than zero, and slightly less than the speed of light - the latter of which characterizes constant acceleration motion. We argue for a proposed velocity-dependent generalization of the Unruh temperature that smoothly varies from zero at zero-acceleration, to the standard form at constant acceleration.

</details>


### [5] [Reconstructing inflation in Einstein-Gauss-Bonnet gravity in light of ACT data](https://arxiv.org/abs/2601.18958)
*Ramón Herrera,Carlos Ríos*

Main category: gr-qc

TL;DR: The paper investigates the reconstruction of background variables in Einstein-Gauss-Bonnet gravity during inflation, focusing on deriving effective potential and coupling function using scalar spectral index and tensor-to-scalar ratio. It challenges previous assumptions by showing $V(φ)$ is not proportional to $1/ξ(φ)$.


<details>
  <summary>Details</summary>
Motivation: To explore inflationary dynamics within Einstein-Gauss-Bonnet gravity by reconstructing cosmological potentials consistent with ACT observational data, addressing limitations of prior assumptions where $V \propto 1/ξ$.

Method: A general formalism reconstructs effective potential $V(φ)$ and coupling function $ξ(φ)$ as functions of scalar spectral index $n_s(N)$ and tensor-to-scalar ratio $r(N)$. An example with ACT-compliant attractors is analyzed to derive explicit expressions.

Result: Explicit $V(φ)$ and $ξ(φ)$ expressions are obtained, demonstrating $V(φ)$ ≠ proportional to $1/ξ(φ)$, contradicting past literature's assumptions about Einstein-Gauss-Bonnet cosmology.

Conclusion: Reconstruction methods reveal observational data in Einstein-Gauss-Bonnet gravity require abandoning previous proportional relationships between $V$ and $ξ$, necessitating revised theoretical frameworks for cosmic evolution studies.

Abstract: During the inflationary epoch, we investigate the reconstruction of the background variables within the framework of Einstein-Gauss-Bonnet gravity, considering the scalar spectral index $n_s(N)$ and the tensor-to-scalar ratio $r(N)$, where $N$ denotes the number of $e-$folds. Under a general formalism, we determine the effective potential and the coupling function associated with the Gauss-Bonnet term as functions of the cosmological parameters $n_s(N)$ and $r(N)$, respectively. To implement the reconstruction methodology for the background variables, we study an example in which the attractors for the index $n_s$ and the ratio $r$ are in agreement with Atacama Cosmology Telescope (ACT) data. In this context, explicit expressions for the effective potential $V(φ)$ and the coupling parameter $ξ(φ)$ are reconstructed. Moreover, the reconstruction based on observational parameters shows that $V(φ)\not\propto 1/ξ(φ)$, in contrast to the assumption adopted in the literature for the study of the evolution of the universe in Einstein-Gauss-Bonnet gravity.

</details>


### [6] [Classical emergence of the quantum-backreacted BTZ black hole from exponential electrodynamics](https://arxiv.org/abs/2601.18967)
*Julio A. Méndez-Zavaleta,Efraín Rojas,José Joaquín Suárez-Garibay*

Main category: gr-qc

TL;DR: The paper explores the classical interpretation of a quantum-corrected BTZ black hole in New Massive Gravity by connecting it to Einstein gravity with exponential nonlinear electrodynamics. It establishes a correspondence between quantum backreaction parameters and classical charges, providing thermodynamic analysis and hinting at a broader link between quantum gravity and nonlinear electrodynamics.


<details>
  <summary>Details</summary>
Motivation: To understand the classical origin of quantum-corrected geometries in New Massive Gravity and uncover deeper connections between higher-curvature quantum gravity effects and nonlinear electrodynamics in three dimensions.

Method: Reinterpretation of quantum-sourced BTZ geometry within classical Einstein gravity coupled to exponential nonlinear electrodynamics; analyzing the moduli space intersection of both theories to establish correspondence; Iyer-Wald formalism for thermodynamic analysis.

Result: Demonstrates that the quantum-corrected geometry is a classical solution with nonlinear electrodynamics, derives thermodynamic laws, and identifies a unique metric class linking the two theories.

Conclusion: The work establishes a classical analog of a semiclassical spacetime, suggesting a correspondence between quantum gravity corrections and nonlinear electrodynamics effects, opening avenues for studying quantum gravity through classical systems.

Abstract: In this work, we revisit a recently reported generalization of the Bañados--Teitelboim--Zanelli black hole arising in New Massive Gravity sourced by the quantum fluctuations of scalar matter, now examined through the lens of a purely classical framework. We show that the same geometry, distinguished by its logarithmic asymptotic structure, emerges as the unique static solution of Einstein gravity coupled to an exponential nonlinear electrodynamics. We trace the origin of this correspondence and prove that this geometry belongs to a unique class of metrics constituting the intersection of the moduli spaces of the static and circularly symmetric sectors of the two theories, thereby revealing a dynamical equivalence between them. An explicit mapping is established between the global charges of the nonlinearly charged black holes and the parameters governing the quantum backreaction in New Massive Gravity, allowing for a natural reinterpretation of the quantum imprints in terms of classical charges. A detailed analysis of the horizon structure of these spacetimes is presented. In addition, the full thermodynamics of the more general configurations is constructed using the Iyer--Wald formalism, from which we derive the first law and the associated Smarr relation. Altogether, our results provide a classical realization of a semiclassical spacetime and point toward a broader correspondence between higher-curvature corrections in quantum gravity and nonlinear effects in self-gravitating electrodynamics in three dimensions.

</details>


### [7] [Weak Lensing Approximation of Wave-optics Effects from General Symmetric Lens Profiles](https://arxiv.org/abs/2508.17486)
*Zhao-Feng Wu,Otto A. Hannuksela,Martin Hendry,Quynh Lan Nguyen*

Main category: gr-qc

TL;DR: The paper introduces a new method to model wave-optics effects in weak gravitational lensing of gravitational waves, enabling better analysis of lensing systems and insights into dark matter and low-mass halos.


<details>
  <summary>Details</summary>
Motivation: Gravitational waves (GWs) lensing, especially in the weak regime with higher optical depth, requires accurate modeling of wave-optics effects to extract lensing system information and improve applications like standard siren delensing.

Method: A novel, efficient framework approximates wave-optics effects caused by general symmetric lens profiles, validated against numerical calculations and asymptotic limits.

Result: The method accurately models weak lensing WO effects, recovering expected behaviors in high/low-frequency limits without computational intensity.

Conclusion: This approach enhances lens reconstruction, aids in studying low-mass halos with minimal baryons, and offers new avenues to probe dark matter properties through GW observations.

Abstract: Gravitational lensing of electromagnetic (EM) waves has yielded many profound discoveries across fundamental physics, astronomy, astrophysics, and cosmology. Similar to EM waves, gravitational waves (GWs) can also be lensed. When their wavelength is comparable to the characteristic scale of the lens, wave-optics (WO) effects manifest as frequency-dependent modulations in the GW waveform. These WO features encode valuable information about the lensing system but are challenging to model, especially in the weak lensing regime, which has a larger optical depth than strong lensing. We present a novel and efficient framework to accurately approximate WO effects induced by general symmetric lens profiles. Our method is validated against numerical calculations and recovers the expected asymptotic behavior in both high- and low-frequency limits. Accurate and efficient modeling of WO effects in the weak lensing regime will enable improved lens reconstruction, delensing of standard sirens, and provide a unique probe to the properties of low-mass halos with minimal baryonic content, offering new insights into the nature of dark matter.

</details>


### [8] [Spiral Density Waves and Torque Balance in the Kerr Geometry](https://arxiv.org/abs/2601.19123)
*Conor Dyson,Daniel J. D'Orazio*

Main category: gr-qc

TL;DR: The paper presents a relativistic calculation of disc-EMRI interactions using self-force theory, black hole perturbation theory, and relativistic stellar perturbation theory, deriving a relativistic torque-balance equation to study the effects of fluid perturbations on EMRI dynamics.


<details>
  <summary>Details</summary>
Motivation: Existing models of disc-EMRI interactions use non-relativistic methods (e.g., dynamical friction or Newtonian planetary migration) that fail to capture relativistic fluid responses. This gap needs addressing for accurate EMRI analysis with LISA.

Method: Applied self-force theory, black hole perturbation theory, and relativistic fluid dynamics to model the fluid response to EMRI's perturbing potential. Used linearized fluid equations and a master enthalpy variable to compute perturbations and spiral arm structures across Kerr spacetimes with varying spin parameters.

Result: Derived a relativistic torque-balance equation enabling comparison of local disc torques to advected angular momentum. Demonstrated the reconstruction of fluid perturbations and spiral arm structures in Kerr geometries with different spin values.

Conclusion: Established a framework linking disc torques from fluid perturbations to forces on EMRIs, providing a critical tool for future torque-balance studies of EMRIs in relativistic accretion discs.

Abstract: Extreme mass-ratio inspirals (EMRIs) in relativistic accretion discs are a key science target for the upcoming LISA mission. Existing models of disc-EMRI interactions typically rely on crude dynamical friction or Newtonian planetary migration prescriptions, which fail to capture the relativistic fluid response induced by the binary potential. In this work we address this gap by providing the relativistic calculation. We apply standard methods from self-force theory, black hole perturbation theory, and relativistic stellar perturbation theory to perform the full fluid calculation of the relativistic analogue of planetary migration for the first time. We calculate the response of a fluid in the perturbing potential of an EMRI consistently incorporating pressure effects. Using a master enthalpy-like variable and linearised fluid theory, we reconstruct the fluid perturbations and relativistic spiral arm structure for a range of spin values in the Kerr geometry. We conclude by deriving a relativistic torque-balance equation that enables computation and comparison of local torques with advected angular momentum through the disc. This opens a promising route towards establishing torque-balance relations between integrated disc torques arising from fluid perturbations and the forces acting on EMRIs embedded in matter.

</details>


### [9] [Wheeler-DeWitt Equation for Black Hole Interiors in Asymptotically Safe Gravity](https://arxiv.org/abs/2601.19227)
*Takamasa Kanai*

Main category: gr-qc

TL;DR: The paper examines the Wheeler-DeWitt equation with scale-dependent gravitational couplings under asymptotically safe gravity. It demonstrates that classical solutions remain unaffected by Newton's constant's running but shows the cosmological constant's running impacts classical behavior. Quantum effects in the UV regime suppress singularity formation regardless of scale identification and fixed point magnitudes.


<details>
  <summary>Details</summary>
Motivation: To investigate how scale dependence of gravitational couplings (Newton's constant and cosmological constant) influences the Wheeler-DeWitt equation and its solutions within the asymptotically safe gravity framework.

Method: Uses a renormalization-group improved Einstein-Hilbert action in Hamiltonian formulation to derive the Wheeler-DeWitt equation, analyzes solutions in minisuperspace context, and examines both classical and quantum regimes.

Result: Classical solutions are unaffected by Newton's constant's running but show sensitivity to cosmological constant running. Quantum UV behavior suppresses singularities universally, independent of scale identification or UV fixed point values.

Conclusion: Asymptotically safe gravity's framework consistently handles scale-dependent couplings, with quantum effects preventing singularities in all UV scenarios, highlighting the theory's viability in resolving cosmological singularities.

Abstract: In this work, we analyze the Wheeler-DeWitt equation with scale-dependent gravitational couplings within the framework of asymptotically safe gravity. In the Hamiltonian formulation based on a renormalization-group improved Einstein-Hilbert action, the consistency of the theory and the Poisson algebra of constraints have been clarified. Within this framework, we show that, despite the explicit scale dependence of Newton's constant, the classical solutions are generically unaffected by the running of the coupling.
  We then derive the Wheeler-DeWitt equation incorporating the scale dependence of the gravitational couplings and analyze its solutions in the minisuperspace framework. In the classical limit, while the scale dependence of Newton's constant does not affect the classical behavior, the running of the cosmological constant can contribute to the classical solutions. Moreover, we show that the quantum behavior in the ultraviolet regime acts toward suppressing singularity formation in all cases, independently of how the renormalization-group scale is identified with spacetime coordinates and of the relative magnitudes of the ultraviolet fixed points of the running Newton's constant and cosmological constant.

</details>


### [10] [Absorption and scattering of massless scalar waves by Frolov black holes](https://arxiv.org/abs/2601.19364)
*Jining Tang,Yang Huang,Hongsheng Zhang*

Main category: gr-qc

TL;DR: The study examines scalar wave absorption and scattering by Frolov black holes, showing that photon sphere geometry dominates interactions over core structure, validated through cross-section calculations and comparisons with other black hole models.


<details>
  <summary>Details</summary>
Motivation: To investigate how scalar waves interact with Frolov black holes, a regularized version of Reissner-Nordström spacetime, and understand the role of photon sphere geometry versus core structure in these interactions.

Method: Analyzing null geodesics to determine photon sphere radius and critical impact parameter, using partial-wave method for numerical computation of cross sections, and comparing results across Frolov, Reissner-Nordström, and Hayward black holes under horizon-radius normalization.

Result: Numerical results align with low/high-frequency approximations and glory scattering. Total absorption cross section increases with regularization parameter (linked to mass variation). Absorption/scattering patterns are similar when critical or glory impact parameters match across black hole types.

Conclusion: Photon sphere geometry primarily governs scalar field interactions, while core structure details are secondary. This highlights wave probes as potential tools for distinguishing regular black hole geometries and observing their effects.

Abstract: We comprehensively investigate the absorption and scattering of massless scalar waves by Frolov black holes, which is a class of regularization of the Reissner--Nordström spacetime. By analyzing the null geodesics, we determine the photon sphere radius and the critical impact parameter, deriving the geometric capture cross section and the classical differential scattering cross section. Utilizing the partial-wave method, we numerically compute the absorption and scattering cross sections across a broad frequency range. Our numerical results show excellent agreement with the low-frequency limit and the high-frequency sinc approximation, as well as with the semiclassical glory approximation. We analyze the dependence of the spectra on the charge and the regularization parameter (Hubble length). Under the horizon-radius normalization, we observe that the total absorption cross section increases with the regularization parameter, a behavior we attribute to the variation of the dimensionless mass. Furthermore, by comparing Frolov, Reissner--Nordström, and Hayward black holes, we demonstrate that their absorption and scattering patterns are nearly indistinguishable when their critical impact parameters or glory impact parameters are matched. This indicates that the photon sphere geometry predominantly governs scalar field interactions, while the detailed core structure plays a secondary role. Our work underscores the potential of wave-based probes to test regular black hole geometries and their observable imprints.

</details>


### [11] [Atomic clocks and gravitational waves as probes of non-metricity](https://arxiv.org/abs/2601.19407)
*Mohsen Khodadi,Emmanuel N. Saridakis*

Main category: gr-qc

TL;DR: This paper explores how spacetime non-metricity, modeled within Weyl geometry, can be tested using atomic clocks and gravitational waves. It shows that while gravitational waves don't directly create the Weyl field, its presence causes measurable effects in both systems, with current observations already limiting such non-metric degrees of freedom.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore experimental signatures of non-metricity in spacetime, an under-studied aspect of Riemannian geometry extensions, using precise measurements from atomic clocks and gravitational waves.

Method: The authors use Weyl geometry as a model for vectorial non-metricity. They formulate gauge-invariant observables linked to the Weyl field strength's path-dependent effects. They analyze constraints from atomic-clock data and study gravitational-wave backreaction caused by non-metricity.

Result: Atomic-clock experiments provide direct constraints on non-metricity parameters. Gravitational-wave observations reveal an anomalous strain due to Weyl field backreaction, leading to stronger constraints than previously known.

Conclusion: Current gravitational-wave data already imposes significant restrictions on dynamical non-metric degrees of freedom, highlighting the viability of using multi-messenger observations (clocks + GWs) to probe beyond-General-Relativity geometries.

Abstract: Non-metricity provides a natural extension of Riemannian geometry, yet its experimental signatures remain largely unexplored. In this work we investigate how spacetime non-metricity can be probed through high-precision observations, focusing on atomic clocks and gravitational waves as complementary tools. Working within Weyl geometry as a minimal realization of vectorial non-metricity, we formulate observable effects in a gauge-invariant manner and show that they are associated with path-dependent length transport governed by the Weyl field strength. We derive constraints from atomic-clock experiments and demonstrate that, although gravitational waves do not directly source the Weyl field at linear order, its dynamical contribution induces a backreaction on gravitational-wave propagation, leading to an anomalous strain. As a result, the absence of deviations from General Relativity in current gravitational-wave observations already places meaningful and strong constraints on dynamical non-metric degrees of freedom.

</details>


### [12] [Dynamical and observational properties of weakly Proca-charged black holes](https://arxiv.org/abs/2601.19409)
*Abylaikhan Tlemissov,Arman Tursunov,Jiří Kovář,Zdeněk Stuchlík*

Main category: gr-qc

TL;DR: The paper explores the effects of a small photon mass on neutral and charged particle motion around a Proca-charged black hole using perturbation theory. It derives constraints on the Proca parameter and highlights the significance of supermassive black holes in detecting such effects, particularly through black hole shadow observations and Galactic center flare analysis.


<details>
  <summary>Details</summary>
Motivation: To investigate the observable implications of a non-zero photon mass (via Einstein-Proca equations) on black hole physics and test theoretical predictions against astrophysical observations.

Method: Perturbative analysis of the Einstein-Proca equations with small Proca mass and black hole charge parameters. Analyzed gravitational lensing, black hole shadow structure, and Galactic center flare orbit data from GRAVITY.

Result: Detected photon mass effects only in extremely cold photons via shadow observations. Derived constraints on Proca parameter μ ≤ 0.125 for specific electric interaction ranges. Showed stronger Proca effects in supermassive vs stellar-mass black holes.

Conclusion: Proca black hole solutions offer testable predictions for astrophysical observations. Future GRAVITY data and black hole shadow imaging can improve constraints on photon mass. Non-perturbative analysis needed near black hole horizons.

Abstract: The simplest approach to include a mass into the electromagnetic vector potential is to modify the Einstein-Maxwell action to the Einstein-Proca form. There are currently no exact analytical solutions for this scenario. However, by using perturbation theory, where both the Proca mass and the black hole charge are small parameters, it is possible to find an exact analytical solution. In this solution, the metric tensor remains unchanged, but the vector potential deviates from the Coulomb potential. In particular, even if the Proca mass is limited by the value $m_γ<10^{-48}\text{g}$, which is the current experimental upper limit for photon mass, it makes a significant contribution to the dynamical equations. In this paper, we study the motion of neutral and charged particles in the vicinity of a weakly Proca-charged black hole, and test the observational implications of the solution of the Einstein-Proca equations for gravitational bending, the black hole shadow, and the fit to the orbits of the Galactic center flares observed by the near-infrared GRAVITY instrument. We find that only extremely cold photons, which are likely scattered before reaching a distant observer, could reveal the non-zero photon mass effect through the black hole shadow. For the Galactic center flare analysis we obtained constraints on the dimensionless Proca parameter to $μ\leq 0.125$ for the electric interaction parameter in the range $-1.1 < \mathrm{Q} < 0.5$, which can be potentially tested by future GRAVITY flare astrometry. Since the Proca parameter is coupled to the black hole mass, the effect of the Proca charge becomes more pronounced for supermassive black holes compared to stellar-mass objects. Our perturbative treatment remains valid essentially up to the horizon, with divergences appearing only in the immediate near-horizon region, where a fully non-perturbative analysis would be required.

</details>


### [13] [The diffusion equation is compatible with special relativity](https://arxiv.org/abs/2601.19464)
*Lorenzo Gavassino*

Main category: gr-qc

TL;DR: The paper demonstrates that the diffusion equation, despite its non-relativistic appearance, can be derived from a relativistic kinetic theory, resolving issues of causality and stability when boosted to relativistic speeds.


<details>
  <summary>Details</summary>
Motivation: To address the long-standing concerns about the compatibility of the diffusion equation with special relativity, particularly its acausal behavior and instability under Lorentz transformation.

Method: The authors show that smooth, localized solutions of the diffusion equation correspond to particle densities in the relativistic Vlasov-Fokker-Planck equation, thereby embedding diffusion into a fully relativistic framework.

Result: They prove the existence of a relativistic kinetic theory where diffusion equations govern hydrodynamics without causality violations or instabilities, disproving past assumptions of incompatibility.

Conclusion: The conclusions refute prior claims of relativistic incompatibility, showing diffusion is consistent with special relativity when properly framed within kinetic theory and signal definitions based on microscopic data.

Abstract: Due to its parabolic character, the diffusion equation exhibits instantaneous spatial spreading, and becomes unstable when Lorentz-boosted. According to the conventional interpretation, these features reflect a fundamental incompatibility with special relativity. In this Letter, we show that this interpretation is incorrect by demonstrating that any smooth and sufficiently localized solution of the diffusion equation is the particle density of an exact solution of the relativistic Vlasov-Fokker-Planck equation. This establishes the existence of a causal, stable, and thermodynamically consistent relativistic kinetic theory whose hydrodynamic sector is governed exactly by diffusion at all wavelengths. We further demonstrate that the standard arguments for instability arise from considering solutions that admit no counterpart in kinetic theory, and that apparent violations of causality disappear once signals are defined in terms of the underlying microscopic data.

</details>


### [14] [On the stability of the objects of limiting compactness: Black hole and Buchdahl star](https://arxiv.org/abs/2601.19476)
*Soumya Chakrabarti,Chiranjeeb Singha,Naresh Dadhich*

Main category: gr-qc

TL;DR: The paper demonstrates the stability of black holes and Buchdahl stars against any perturbations by analyzing their gravitational energy and mass relationship, showing these objects with limiting compactness are in stable equilibrium.


<details>
  <summary>Details</summary>
Motivation: To prove the inherent stability of black holes and Buchdahl stars, which are at critical compactness limits where gravitational energy equals or halves their mass.

Method: General Relativity framework analysis of gravitational energy's role in stability without relying on specific perturbation types.

Result: Confirmed stability of both objects under any perturbation through the inherent properties of their gravitational energy and mass-energy relationship.

Conclusion: Objects at gravitational-mass energy limits (black holes/Buchdahl stars) maintain stable equilibrium irrespective of perturbations due to their non-gravitational mass-energy sourcing.

Abstract: In General Relativity, there exist two objects of limiting compactness, one with a null boundary defining the horizon of a black hole and the other with a timelike boundary defining a Buchdahl star. The two are characterized by gravitational energy equal to or half the mass. Since non-gravitational mass-energy is the source of gravitational energy, both of these objects are manifestly stable. We demonstrate in this letter, in a simple and general way, that the equilibrium state defining the object is indeed stable, independent of the nature of the perturbation.

</details>


### [15] [Effect of noise characterization on the detection of mHz stochastic gravitational waves](https://arxiv.org/abs/2601.19741)
*Nikolaos Karnesis,Quentin Baghi,Jean-Baptiste Bayle,Nikiforos Galanis*

Main category: gr-qc

TL;DR: The paper explores improving the detection of a stochastic gravitational-wave background (SGWB) using LISA by refining noise modeling techniques.


<details>
  <summary>Details</summary>
Motivation: To address challenges in detecting SGWB with LISA due to stochastic instrumental noise and enhance the robustness of detection through advanced noise modeling.

Method: The authors use agnostic noise reconstruction methods with more realistic instrumental simulations, separate transfer functions for main noise sources, and spline fitting of power spectral densities to model noise more accurately.

Result: The study refines detectability bounds of SGWBs, showing dependence on noise model flexibility and prior probability assumptions.

Conclusion: Improved noise modeling enhances LISA's ability to detect SGWBs, requiring careful consideration of model flexibility and prior choices for accurate results.

Abstract: Pulsar timing arrays' hint for a stochastic gravitational-wave background (SGWB) leverages the expectations of a future detection in the millihertz band, particularly with the LISA space mission. However, finding an SGWB with a single orbiting detector is challenging: It calls for cautious modelling of instrumental noise, which is also mainly stochastic. It was shown that agnostic noise reconstruction methods provide robustness in the detection process. We build on previous work to include more realistic instrumental simulations and additional degrees of freedom in the noise inference model and analyze the impact of LISA's sensitivity to SGWBs. Particularly, we model the two main types of noise sources with separate transfer functions and power spectral density spline fitting. We assess the detectability bounds and their dependence on the flexibility of the noise model and on the prior probability, allowing us to refine previously reported results.

</details>


### [16] [Comment on "Multidimensional arrow of time" (arXiv:2601.14134)](https://arxiv.org/abs/2601.19819)
*Andrei Galiautdinov*

Main category: gr-qc

TL;DR: The paper addresses a potential conflict between the proposed geometric origin of time's arrow via extra-dimensional volume growth and the observed stability of Newton's constant G in Kaluza-Klein theory. It introduces the 'shape-dynamic arrow of time' using Perelman's nu-entropy to preserve the geometric arrow without violating observational bounds on varying G.


<details>
  <summary>Details</summary>
Motivation: To reconcile Rubin's geometric time's arrow mechanism with observed constraints on the stability of G, which standard Kaluza-Klein theories fail to satisfy due to excessive G variation from volume growth.

Method: Proposes a modified approach called 'shape-dynamic arrow of time', employing Perelman's nu-entropy's scale-invariant monotonicity under normalized Ricci flow. This allows time's arrow to emerge from geometric smoothing of extra dimensions while keeping their total volume fixed, avoiding G variation.

Result: Shows that the new mechanism successfully upholds observational constraints (BBN/LLR) on fundamental constants while maintaining a geometric origin for time's arrow. Demonstrates viability of Rubin's core idea within KK framework through this adjustment.

Conclusion: The shape-dynamic arrow.of time reconciles geometric time asymmetry with constant G in KK theories, offering a viable path forward for models linking cosmic time to extra-dimensional geometry.

Abstract: In a recent preprint [arXiv:2601.14134v1], Rubin argues that the arrow of time originates from the monotonic growth of the volume of extra dimensions. While the identification of a geometric origin for time's arrow is compelling in the case of brane-world models, we point out a possible tension between the proposed volume growth and the observational stability of the effective four-dimensional Newton's gravitational constant, G, that may arise in Kaluza-Klein (KK) theory. In standard KK approaches, such volume growth induces a time-variation of G that exceeds Big Bang Nucleosynthesis (BBN) and Lunar Laser Ranging (LLR) bounds by many orders of magnitude. To resolve this tension while preserving the author's key insight in the Kaluza-Klein case, we propose an extension: the "shape-dynamic arrow of time". By utilizing the scale-invariant monotonicity of Perelman's nu-entropy under normalized Ricci flow, we demonstrate how an arrow of time can emerge from the geometric smoothing of extra dimensions at fixed volume, thereby satisfying observational constraints on fundamental constants.

</details>


### [17] [Melvin--Bonnor and Bertotti--Robinson spacetimes with Baryonic charge](https://arxiv.org/abs/2601.19858)
*José Barrientos,Fabrizio Canfora,Adolfo Cisterna,Keanu Müller,Anibal Neira*

Main category: gr-qc

TL;DR: The paper establishes a novel dictionary connecting Einstein--Scalar--Maxwell solutions to gauged Skyrme--Maxwell--Einstein models in 3+1 dimensions, enabling the construction of configurations with nontrivial Baryonic charge and magnetic fields. It derives an analytic mass-Baryonic charge relation showing linearity at high mass but non-linear behavior at intermediate masses.


<details>
  <summary>Details</summary>
Motivation: To leverage the more tractable Einstein--Scalar--Maxwell system to generate new solutions with Baryonic charge and magnetic fields, which are harder to derive directly in the Skyrme--Maxwell--Einstein framework. This opens a systematic pathway for constructing physically relevant configurations.

Method: The authors use the established framework to embed compact scalar-field sources into electromagnetic backgrounds and construct dual Skyrme models with baryon charge. They express baryon charge in terms of seed spacetime parameters and derive a quantization condition linking mass and baryon charge through an analytic formula.

Result: A closed-form analytic equation relates black hole mass to baryonic charge and magnetic field. The relationship is linear at large masses but shows significant non-linear deviations at intermediate mass values, providing new insights into the parameter dependencies of these systems.

Conclusion: The dictionary approach successfully bridges two theoretical models, offering a tool for systematic exploration of baryonic charge in gravitational systems. The mass-baryon charge relation reveals critical non-linearities at lower mass scales, suggesting complex behaviors in intermediate regimes that require further study.

Abstract: Recently, a novel dictionary relating solutions of the Einstein--Scalar--Maxwell theory to solutions of gauged Skyrme--Maxwell--Einstein models in $(3+1)$ dimensions has been established. This development provides a clear and systematic route to constructing new configurations with nontrivial Baryonic charge and magnetic field, leveraging the fact that the Einstein--Scalar--Maxwell system is considerably more tractable, thanks to powerful solution-generating techniques. In this work, we exploit the framework that allows compact sources dressed by scalar fields to be consistently embedded in external electromagnetic backgrounds, and we construct their dual counterparts carrying Baryonic charge in the Skyrme sector. The resulting Baryonic charge is expressed directly in terms of the parameters characterizing the seed spacetime, and a corresponding quantization condition involving these parameters is explicitly derived. Consequently, the mass and the Baryonic charge are not independent parameters. These results provide a closed analytic formula for the black hole mass parameter in terms of the Baryonic charge and the magnetic field. This relation between the mass parameter and the Baryonic charge is linear for large values of the mass, while significant deviations from linearity arise if the mass takes intermediate values.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [18] [Thermoskyrmions](https://arxiv.org/abs/2601.18873)
*Mikael Chala,Juan Carlos Criado,Luis Gil*

Main category: hep-ph

TL;DR: The paper demonstrates that skyrmions unstable at zero temperature can become stable due to thermal effects in certain electroweak sector toy models, suggesting potential dark matter implications within the Standard Model without requiring new physics.


<details>
  <summary>Details</summary>
Motivation: To investigate the stability of skyrmions under thermal effects in electroweak models, exploring their possible role as dark matter candidates without needing beyond-Standard-Model physics.

Method: Analyzing toy models representing different limits of the electroweak sector to study thermal stabilization of skyrmions that are classically unstable at zero temperature.

Result: Skyrmions unstable at zero temperature are stabilized by thermal effects in these toy models, indicating a viable path for their existence in the Standard Model's quantum effective action.

Conclusion: The findings motivate further study of skyrmions in the Standard Model's quantum effective action as dark matter candidates, potentially eliminating the necessity for new physics beyond the SM.

Abstract: Skyrmions are stable and topologically non-trivial field configurations that behave like localized particles. They appear in the chiral effective theory for pions, where they correspond to the baryon states, and might also exist in the electroweak theory, in the presence of certain effective interactions. In this paper, focusing on toy models that capture different limits of the electroweak sector of the Standard Model (SM), we show that skyrmions not classically stable at zero temperature can be stabilized by thermal effects. This result motivates the study of skyrmions in the quantum effective action of the SM, potentially implying the existence of dark matter without new physics.

</details>


### [19] [Drell-Yan Production of New Particles at Fixed-Target Experiments: Heavy Neutral Lepton as a Case Study](https://arxiv.org/abs/2601.18874)
*Francis M. Burk,P. S. Bhupal Dev,Bhaskar Dutta,Tao Han,Aparajitha Karthikeyan,Doojin Kim*

Main category: hep-ph

TL;DR: This paper analyzes the sensitivity of Drell-Yan production processes in fixed-target experiments to detect heavy neutral leptons (HNLs) and other beyond-the-Standard-Model particles, demonstrating enhanced detection capabilities through energetic final-state particles and improved sensitivity limits over existing constraints.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of Drell-Yan processes in fixed-target experiments for probing BSM physics, focusing on HNLs as a key example. The goal is to extend search sensitivity into new parameter regions not accessible by current methods.

Method: The study uses Drell-Yan production of light vector bosons (2-20 GeV) decaying into HNLs, which then decay into observable final states like νπ⁰ and νe⁺e⁻. Analysis considers experiments (SBND, DarkQuest, DUNE ND, SHiP), evaluating their sensitivity to HNL mixing parameters (|U_{ℓ}|) and couplings (g_X) at 90% C.L.

Result: Drell-Yan enhances HNL sensitivity: SBND/DarkQuest reach |U_{ℓ}| ~3e-4 (g_X~1e-2) and DUNE ND/SHiP achieve ~1e-5 (g_X down to 5e-6). Benchmark scenarios show significant improvements over prior constraints, especially for light mediators and certain U(1) models.

Conclusion: Drell-Yan-based fixed-target experiments provide a powerful new method to probe HNL and dark sector BSM physics. The framework can be extended to other light mediators and models, offering a versatile discovery tool for future particle searches.

Abstract: We demonstrate the sensitivity of Drell-Yan production processes from deep inelastic scattering in searches for beyond-the-Standard Model (BSM) physics at fixed-target or beam-bump experiments. We take heavy neutral leptons (HNLs) as a case study, produced from the decay of a light vector boson mediator with mass in the range of $2-20$ GeV, which itself is generated via the Drell-Yan process. The produced HNLs subsequently decay into Standard Model final states. We consider several current and future experiments, including SBND, DarkQuest, DUNE Near Detector (ND), and SHiP. Utilizing $νπ^0$ and $νe^+e^-$ final states from HNL decays, we find that the Drell-Yan mechanism provides important contributions and significantly enhances the HNL search sensitivity, owing to the production of energetic final-state particles that are more readily detectable over the expected backgrounds. We find that at $90\%$ C.L. sensitivity, for gauge couplings $g_{X} \sim 10^{-2}\ (10^{-3})$ and kinematically accessible mass range, SBND and DarkQuest can probe the HNL flavor mixing $|U_{\ell}| \sim 3\times 10^{-4}\ (10^{-3})$, whereas DUNE ND and SHiP may extend the sensitivity down to the Type-I Seesaw prediction of $|U_{\ell}| \sim 10^{-5}$. Finally, for our chosen benchmark $|U_{\ell}| = 10^{-3}$ outside of the current experimental constraints, with a fixed mass ratio $m_{Z'}/m_N = 2.1$, and working within the $U(1)_{B-L}$, $U(1)_{B-3L_τ}$, and $U(1)_{B}$ parameter spaces, we find that both SBND and DarkQuest can probe $g_{X} \sim 10^{-3}$, DUNE ND can reach $g_{X} \sim 10^{-4}$, and SHiP can probe down to $g_{X}\sim 5\times 10^{-6}$. Our approach provides a powerful new technique to study HNL production at future fixed-target experiments and can readily be extended to other light BSM particle production within a broader class of dark sector models.

</details>


### [20] [Single-wave solutions of the neutrino fast flavor system. Part II. Weak instabilities and their resonant behavior](https://arxiv.org/abs/2601.18880)
*Damiano F. G. Fiorillo,Georg G. Raffelt*

Main category: hep-ph

TL;DR: The paper explores the nonlinear saturation of flavor instabilities in dense neutrino media using a single-wave solution model. It identifies periodic flavor reversal cycles in resonant neutrinos, forming a 'flavor pendulum' effect, which explains the dynamics of weak fast flavor instabilities. The model applies to early stages of instability when growth rates are low, except for two-beam systems.


<details>
  <summary>Details</summary>
Motivation: To better understand the nonlinear saturation of flavor instabilities in dense neutrino environments, as current knowledge is limited. The study aims to uncover the mechanisms behind the periodic behavior of resonant neutrinos and how they influence instability saturation.

Method: Analyzing a single-wave solution within an axially symmetric fast flavor conversion framework. Focus on scenarios with shallow angular crossings and small growth rates, examining resonant neutrinos' phase-synchronized evolution and their energy exchange with the unstable wave.

Result: Identification of a periodic 'flavor pendulum' dynamics where resonant neutrinos cyclically reverse flavor and exchange energy with the wave. This model describes weak, nearly monochromatic instabilities' early stages. Nonlinear solutions only emerge for weak instabilities or two-beam systems.

Conclusion: The flavor pendulum mechanism explains early nonlinear behavior of weak flavor instabilities. Stronger instabilities or complex angular distributions require different models, highlighting the model's applicability limits and guiding future research on instability saturation.

Abstract: Flavor instabilities in dense neutrino media trigger exponential growth of flavor waves, yet their nonlinear saturation remains poorly understood. We examine a simple proxy for this effect in the form of a single-wave solution of an axially symmetric fast flavor system. When the angular crossing is shallow and the growth rate of the instability correspondingly small, the flavor wave primarily affects resonant neutrinos that move in phase with it. The evolution of these resonant neutrinos becomes periodic, undergoing cycles of full flavor reversal. They feed power into the unstable wave, and subsequently return to their initial state, draining power back out. This new flavor pendulum captures the dynamics of weak, nearly monochromatic fast flavor instabilities. Since weakly unstable distributions always exhibit a narrow range of unstable wavenumbers, our model likely describes the earliest development of a flavor instability when it first appears. When the instability is not weak, the linear phase of a single-wave excitation does not connect to a regular nonlinear solution, unless the angle distribution consists of only two beams.

</details>


### [21] [Next-to-next-to-leading power corrections to unpolarized Semi-Inclusive Deep Inelastic Scattering](https://arxiv.org/abs/2601.18882)
*Ian Balitsky,Alexei Prokudin*

Main category: hep-ph

TL;DR: The paper extends rapidity factorization formalism to Semi-Inclusive Deep Inelastic Scattering (SIDIS) to derive next-to-next-to-leading power (NNLP) corrections of order $1/Q^2$. These corrections involve unpolarized and Boer-Mulders parton distributions convoluted with fragmentation functions, and are validated against experimental data.


<details>
  <summary>Details</summary>
Motivation: To improve the reliability of SIDIS phenomenology by systematically incorporating NLP and NNLP corrections, which are crucial for analyzing modern high-precision data and understanding nucleon substructure.

Method: Adapting the rapidity factorization approach from Drell-Yan processes to SIDIS, deriving analytical expressions for unpolarized structure functions at NNLP order, including convolutions of distributions like $f_1$, $h_1^\perp$ with fragmentation functions $D_1$, $H_1^\perp$.

Result: Derived NNLP corrections for SIDIS, performed numerical comparisons with HERMES/COMPASS data, and provided predictions for upcoming Jefferson Lab/EIC experiments. Confirms consistency with prior formulations while extending their scope.

Conclusion: The inclusion of NNLP corrections enhances precision in SIDIS analyses, offering improved tools for interpreting experimental results and guiding future high-energy physics experiments.

Abstract: Semi-Inclusive Deep Inelastic Scattering (SIDIS) is a key tool for exploring the three-dimensional structure of the nucleon through Transverse Momentum Dependent parton distributions and fragmentation functions. While leading-power contributions to the SIDIS cross-section are well established, next-to-leading (NLP) of order $1/Q$ and next-to-next-to-leading power (NNLP) corrections of order $1/Q^2$ to the hadronic tensor have only recently begun to be systematically investigated. These corrections are essential for the reliable phenomenology and interpretation of modern high-precision data. In recent papers by one of the authors, NNLP corrections to Drell-Yan process were derived using rapidity factorization formalism. In the present work we extend this approach to SIDIS and obtain analytic expressions for the unpolarized structure functions. We derive NNLP corrections that include convolutions of unpolarized distributions, $f_1$, with unpolarized fragmentation functions, $D_1$, and Boer-Mulders functions, $h_1^\perp$, with Collins fragmentation functions, $H_1^\perp$. We compare our results with previous formulations, provide numerical studies, confront our predictions with HERMES and COMPASS measurements, and present predictions for future experiments at Jefferson Lab and the Electron-Ion Collider.

</details>


### [22] [One-loop matching of the LEFT to the QCD gradient flow](https://arxiv.org/abs/2601.18883)
*Òscar L. Crosas,Peter Stoffer*

Main category: hep-ph

TL;DR: The paper presents a comprehensive one-loop matching of a baryon- and lepton-number-conserving low-energy effective field theory (LEFT) with the QCD gradient flow, ensuring precise calculations in lattice-QCD matrix elements beyond leading-logarithmic accuracy.


<details>
  <summary>Details</summary>
Motivation: To establish a consistent perturbative link between continuum LEFT calculations and gradient-flow-based lattice-QCD matrix elements, necessary for precision low-energy phenomenology requiring both divergent and finite counterterms.

Method: The study uses Euclidean conventions and the background-field formulation of the gradient flow to derive the short-flow-time expansion for the LEFT operator basis up to dimension six. Dimensional regularization in the 't Hooft-Veltman scheme is applied, with systematic treatment of evanescent operators and finite counterterms for chiral symmetry restoration.

Result: The authors verify the cancellation of spurious chiral-symmetry-violating terms, confirming the gradient flow's efficacy as a gauge-invariant UV regulator. They provide matching coefficients pre/post field redefinitions and power-divergent mixing data.

Conclusion: This work enables precise calculations in low-energy physics beyond leading order by bridging continuum LEFT and lattice-QCD frameworks, supporting accurate phenomenological predictions.

Abstract: We present the complete one-loop matching of the baryon- and lepton-number-conserving low-energy effective field theory (LEFT) to the QCD gradient flow. Using Euclidean conventions and the background-field formulation of the gradient flow, we derive the short-flow-time expansion for the full LEFT operator basis up to mass dimension six. The matching is performed in dimensional regularization in the algebraically consistent 't Hooft-Veltman scheme, including a systematic treatment of evanescent operators and the finite counterterms required to restore chiral symmetry in the spurion sense. Keeping fully generic flavor structures, we verify the cancellation of spurious chiral-symmetry-violating terms with the known finite symmetry-restoring counterterms. This demonstrates that the gradient flow as a gauge-invariant ultraviolet regulator enables an efficient extraction of both divergent and finite counterterms in addition to the matching contributions. We provide the matching coefficients both before and after field redefinitions that remove redundant operators, as well as power-divergent mixings into lower-dimensional operators. Our results establish a consistent perturbative link between continuum LEFT calculations and gradient-flow-based lattice-QCD matrix elements, enabling precision low-energy phenomenology beyond leading-logarithmic accuracy.

</details>


### [23] [Constraining axial non-standard interactions of neutrinos with long baseline experiments](https://arxiv.org/abs/2601.18888)
*Yasaman Farzan,Saeed Abbaslu*

Main category: hep-ph

TL;DR: The paper investigates axial non-standard neutrino interactions, suggesting they might hold new physics not found in vector NSI. It proposes using neutral current scattering data from MINOS, MINOS+, and DUNE experiments to detect axial NSI effects.


<details>
  <summary>Details</summary>
Motivation: While vector NSIs have been extensively constrained by neutrino oscillation and CEvNS experiments, the focus shifts to axial NSIs as a potential source of new physics.

Method: Analyzing neutral current scattering events in long baseline experiments (MINOS, MINOS+, DUNE) to study axial NSI impacts.

Result: Shows the feasibility of detecting axial NSI effects through these experiments, indicating a path forward for exploring beyond-standard-model physics.

Conclusion: Axial NSI interactions are viable candidates for new physics, and the proposed experimental analysis can help uncover their effects, offering a new direction for neutrino physics research.

Abstract: Thanks to a number of neutrino oscillation and Coherent Elastic neutrino Nucleus Scattering (CE$ν$NS) experiments, the vector Non-Standard Interactions (NSI) of neutrinos have been well studied and constrained. We show that the long-sought-after new physics may hide in the ``axial" non-standard interactions rather than in the vector NSI. We then show how by studying neutral current scattering events in the detectors of long baseline experiments, MINOS, MINOS$+$ and DUNE, the impact of the axial NSI can be discovered.

</details>


### [24] [Probing Lepton Flavor Violation at the ILC and CLIC](https://arxiv.org/abs/2601.18996)
*Pankaj Munbodh*

Main category: hep-ph

TL;DR: The study examines lepton flavor violation in the τμ sector using the SMEFT framework at future electron-positron colliders (ILC and CLIC), highlighting the role of beam polarizations in probing chirality and the superior sensitivity of high-energy collisions compared to Belle-II's tau decay experiments.


<details>
  <summary>Details</summary>
Motivation: To detect Beyond Standard Model physics through lepton flavor violation in the τμ sector, leveraging the capabilities of ILC and CLIC colliders.

Method: Utilizes the Standard Model Effective Field Theory (SMEFT) framework to analyze the process e+e-→τμ, incorporating beam polarization and high center-of-mass energy to study chirality of SMEFT operators.

Result: Beam polarizations at ILC/CLIC enable chirality discrimination of SMEFT operators. The high energy of these colliders significantly boosts sensitivity to four-fermion operators, surpassing some Belle-II tau decay projections.

Conclusion: Future e+e- colliders like ILC and CLIC offer unique capabilities to probe BSM physics via τμ lepton flavor violation, outperforming existing experiments in certain aspects.

Abstract: Lepton flavor violation in the $τμ$ sector would be a clear sign of Beyond Standard Model physics. We employ the SMEFT framework to study the process $e^+e^-\toτμ$ at the ILC and CLIC. We find that the $e^+e^-$ beam polarizations achievable at these machines allow us to probe the chirality structure of the SMEFT operators. In addition, the high center of mass energy leads to a substantial increase in sensitivity to the four-fermion operators that rivals, and in some cases, surpasses tau decay projections from Belle-II.

</details>


### [25] [Part II: Low Energy Galactic Neutrinos](https://arxiv.org/abs/2601.19015)
*Eduardo Flores,Elise Cantu,Ian Marano,Osvan Vivar-Garcia,Shabhaz Khalandar*

Main category: hep-ph

TL;DR: The paper explores low-energy galactic neutrinos as probes of fundamental gravity theories, showing they can either rule out quantum gravity's role in dark matter or support neutrinos as a dark matter candidate under general relativity, impacting our understanding of cosmic structure and matter-antimatter asymmetry.


<details>
  <summary>Details</summary>
Motivation: To determine how neutrinos interact with gravitational forces under different theoretical frameworks (quantum gravity vs. general relativity curvature) and their implications for dark matter and cosmic phenomena.

Method: The authors calculate neutrino mass distributions and dynamics under two gravity models: quantum gravity leading to neutrino-bound structures, and general relativity leading to collisionless neutrino orbits. They compare these results with Milky Way's dark matter mass requirements and rotation curves.

Result: Quantum gravity predicts negligible neutrino mass contribution (10^-29 of dark matter), making it implausible. General relativity allows neutrinos as viable dark matter, aligning with rotation curves and suggesting neutrino-antineutrino equilibrium affecting matter asymmetry.

Conclusion: Neutrinos critically test gravitational paradigms: quantum gravity requires direct detection of bound neutrino states, while general relativity retains neutrinos as dark matter candidates with potential insights into cosmological asymmetries.

Abstract: We study low energy galactic neutrinos in the Milky Way under two fundamentally different descriptions of gravity, showing that neutrinos provide a sensitive probe of gravity underlying nature. If gravity is a quantum interaction, its long range character leads to the formation of an atom like bound neutrino structure. We compute its mass distribution and find that, within a radius 292 kpc, the total mass is only ten to the minus 29 of the galaxy dark matter, ruling it out as a dark matter candidate. Nevertheless, experimental confirmation of this structure would constitute direct evidence for gravity as a quantum force mediated by gravitons. If gravity instead arises from spacetime curvature, neutrinos interact only via the short range weak force and are therefore effectively collisionless. In this regime, neutrinos behave as free classical particles orbiting the galaxy and experience no Fermi pressure. We show that such a population can be sufficiently compact to reproduce the Milky Way rotation curve, making neutrinos viable dark matter candidates. The extremely small neutrino antineutrino annihilation cross section further implies near equilibrium between neutrinos and antineutrinos, potentially addressing the matter antimatter asymmetry.

</details>


### [26] [Origin of the nucleon gravitational form factor $B_N(t)$](https://arxiv.org/abs/2601.19141)
*Xianghui Cao,Bheemsehan Gurjar,Chandan Mondal,Chen Chen,Yang Li*

Main category: hep-ph

TL;DR: The smallness of the nucleon's gravitational form factor $B_N(t)$ at finite momentum transfer is explained by a fundamental cancellation in the nucleon's wave functions, governed by an antisymmetric factor in longitudinal dynamics using light-front holographic QCD, highlighting the nucleon's S-wave dominance.


<details>
  <summary>Details</summary>
Motivation: To understand the physical origin of the suppression of $B_N(t)$ at finite $t$, as its smallness remains unexplained despite lattice QCD and phenomenological evidence.

Method: Employed light-front holographic QCD to analyze nucleon wave functions, demonstrating that $B_N(t)$ is controlled by an antisymmetric factor leading to cancellation in the symmetric limit, thus suppressing its value.

Result: Shown that $B_N(t)$ vanishes exactly in symmetric limits and is significantly suppressed for realistic nucleon structures, with the S-wave dominance of the nucleon justifying its negligible contribution in practical applications.

Conclusion: The suppression of $B_N(t)$ reflects the nucleon's S-wave character, validating its omission in applications like near-threshold $J/ψ$ production calculations.

Abstract: Recent lattice QCD simulations and phenomenological models indicate that the nucleon's gravitational form factor $B_N(t)$ remains remarkably small at finite momentum transfer $t$. While $B_N(0) = 0$ is a known consequence of the equivalence principle, the physical origin of its suppression at finite $t$ has not been fully elucidated. In this work, we demonstrate that the smallness of $B_N(t)$ arises from a fundamental cancellation within the nucleon's wave functions. Using light-front holographic QCD, we show that $B_N(t)$ is governed by an antisymmetric factor in the longitudinal dynamics that leads to an exact vanishing in the symmetric limit and significant suppression for realistic nucleon structures. Our results suggest that the smallness of $B_N(t)$ is a signature of the nucleon's dominant S-wave character, providing a formal justification for its frequent omission in practical applications like near-threshold $J/ψ$ production.

</details>


### [27] [Analysing Toponium at the LHC using Recursive Jigsaw Reconstruction](https://arxiv.org/abs/2601.19187)
*Aman Desai,Amelia Lovison,Paul Jackson*

Main category: hep-ph

TL;DR: The paper proposes using Recursive Jigsaw Reconstruction and angular variables in the RestFrames package to enhance sensitivity to toponium signals at the LHC, improving detection by 16% over current methods.


<details>
  <summary>Details</summary>
Motivation: To address combinatorial ambiguities and missing energy in detecting toponium states, enabling better discrimination between toponium signals and SM top quark pair backgrounds.

Method: The Recursive Jigsaw Reconstruction method combined with four RestFrames techniques is applied to analyze events with two b-jets, leptons, and missing neutrino energy. Angular variables are introduced to improve signal sensitivity.

Result: Preliminary results show a 16% improvement in sensitivity for toponium detection using the proposed method compared to current strategies in LHC Run 3 conditions.

Conclusion: The method offers a viable path to study top quark pair threshold physics, enhancing precision in analyzing toponium states and distinguishing them from background.

Abstract: Recent results from the ATLAS and the CMS experiments at the Large Hadron Collider indicate the presence of a top-quark pair bound state near the threshold region. We present a way to reconstruct a toponium state at the $t\bar{t}$ threshold region formed at the Large Hadron Collider using the Recursive Jigsaw Reconstruction. We have considered the Non-Relativistic QCD based toponium model implemented in MadGraph5\_aMC@NLO. The final states considered consist of two b-jets, two oppositely charged leptons, and missing energy that arises from two neutrinos. The goal of the Recursive Jigsaw Reconstruction is to make use of rules that can help resolve combinatorics ambiguity in preparing the decay tree for a given physics event. Additionally, missing energy coming from two neutrinos needs to be resolved in order to reconstruct the event. We apply four different methods within the RestFrames package and compare the reconstruction results resulting from each of the methods. Due to this method, one can also access kinematic variables in the rest frames belonging to intermediate particle states, providing additional means to discriminate the SM $\ttbar$ background from the toponium signal. We propose using two angular variables to enhance sensitivity to the toponium signal. Our preliminary results indicate that the improvement in sensitivity can be as much as 16\% over the current strategy in the LHC's Run 3 configuration. This method may be useful for gaining additional insight into the physics phenomenology in the $\ttbar$ threshold region.

</details>


### [28] [Confinement and Chiral Phase Transitions: The Role of Polyakov Loop Kinetics Terms](https://arxiv.org/abs/2601.19270)
*Banghui Hua,Jiang Zhu*

Main category: hep-ph

TL;DR: This paper investigates the kinetic term of the Polyakov loop in gravitational-wave predictions from QCD phase transitions, deriving it from first principles in SU(3) Yang-Mills theory and extending to more colors. It shows the term significantly alters GW energy spectra from confinement transitions and analyzes the chiral transition in the PNJL model, revealing the Polyakov-loop's disparate impacts on confinement vs. chiral transitions.


<details>
  <summary>Details</summary>
Motivation: To address the oversimplification of the Polyakov loop's kinetic term in gravitational-wave signal predictions and assess its true influence on phase transitions, particularly for confinement and chiral transitions.

Method: First-principles derivation of the Polyakov loop's kinetic term in finite-temperature SU(3) Yang-Mills theory with a field-dependent renormalization factor; comparison with three effective potentials (Haar-measure, polynomial, quasi-particle); PNJL framework analysis for chiral transition using quark condensate data.

Result: The kinetic term modifies GW energy spectra by 1-2 orders of magnitude for confinement transitions. In chiral transitions, it has negligible effect compared to fermion condensation.

Conclusion: The Polyakov-loop kinetic term critically affects gravitational waves from confinement transitions but plays no significant role in chiral transitions, highlighting its phase-dependent importance in early universe phase transition modeling.

Abstract: We studied a crucial but often oversimplified ingredient in predicting gravitational-wave signals from QCD-type phase transitions: the kinetic term of the Polyakov loop. For the first time, we derive this term from first principles in finite-temperature pure SU(3) Yang-Mills theory, incorporating a field-dependent renormalization factor--a calculation we also extend to theories with more colors. Employing this derived kinetic term alongside three commonly-used effective potentials (the Haar-measure, polynomial, and quasi-particle models), we demonstrate that it substantially modifies the predicted GW energy spectrum from confinement transitions by 1-2 orders of magnitude. Based on this, we provide the first complete analysis of the chiral transition within the Polyakov-Nambu-Jona-Lasinio (PNJL) framework, described by the quark condensate. Our results reveal a clear dichotomy: while the Polyakov-loop kinetic term critically shapes GWs from confinement transitions, it has a negligible impact on the dynamics of the chiral transition, which is dominated by fermion condensation effects.

</details>


### [29] [Constraints on Primordial Black Holes from Galactic Diffuse Synchrotron Emissions](https://arxiv.org/abs/2601.19386)
*Chen-Wei Du,Yu-Feng Zhou*

Main category: hep-ph

TL;DR: The paper explores using Galactic diffuse synchrotron emissions to constrain the abundance of primordial black holes (PBHs) with masses ≥10^15 g. By combining Hawking radiation and diffusive re-acceleration models with significant Alfvén velocities, the authors show that synchrotron observations (22 MHz–1.4 GHz) provide tighter constraints than previous methods for PBH masses ≥10^16 g.


<details>
  <summary>Details</summary>
Motivation: To address uncertainties in PBH abundance estimates and improve upon existing constraints (e.g., from Voyager-1 electron data and AMS-02 positron measurements), the study leverages synchrotron emissions as a novel probing mechanism. This approach accounts for CR electron/positron re-acceleration via Galactic magnetic fields, which amplifies their detectable energy range.

Method: 1. Model PBH Hawking radiation producing low-energy CR electrons/positrons (∼10 MeV). 2. Apply diffusive re-acceleration with Alfvén velocities (∼20 km/s) to boost particle energies to ∼100 MeV. 3. Link accelerated electron/positron spectra to synchrotron emission frequencies above 20 MHz. 4. Use AMS-02/Voyager-1 boron-to-carbon ratios to validate Alfvén velocity parameters. 5. Compare derived synchrotron-based constraints against existing PBH abundance limits.

Result: For PBH masses ≥1×10¹⁶ g, synchrotron constraints exceed Voyager-1 all-electron data constraints by >1 order of magnitude. For ≥2×10¹⁶ g, they also surpass AMS-02 positron-based limits. This establishes low-frequency synchrotron observations as a superior probe for high-mass PBHs.

Conclusion: Galactic synchrotron emissions at 22 MHz–1.4 GHz offer a powerful method to constrain PBH abundance, particularly effective for masses above 10¹⁶ g. Future low-frequency radio surveys (e.g., LOFAR, SKA) could further refine these constraints and test PBH theories in the cosmological mass range.

Abstract: We investigate the possibility of constraining primordial black holes (PBHs) with masses $M_\mathrm{PBH}\gtrsim 10^{15}\,\mathrm{g}$ through Galactic diffuse synchrotron emissions. Due to Hawking radiation, these types of PBHs are expected to be stable sources of cosmic-ray (CR) electrons and positrons with energies below $\mathcal{O}(10\,\mathrm{MeV})$. In many CR propagation models with diffusive re-acceleration characterized by a significant Alfvén velocity $V_a\sim \mathcal{O}(10)\,\mathrm{km/s}$, the energies of the evaporated electrons/positrons can be further enhanced to $\mathcal{O}(100)\,\mathrm{MeV}$ through their scattering with the Galactic random magnetic fields. Consequently, the observation of Galactic synchrotron emissions at frequencies above $\sim 20\,\mathrm{MHz}$ can provide useful constraints on the abundance of PBHs. Using the AMS-02 and Voyager-1 data on the boron-to-carbon nuclei flux ratio, we confirm that a significant Alfvén velocity $V_a \sim 20\,\mathrm{km/s}$ is favored in several benchmark diffusive re-acceleration models. We show that, in this scenario, the observed low-frequency synchrotron emissions (from 22 MHz to 1.4 GHz) can provide stringent constraints on PBH abundance. The obtained conservative constraints are stronger than those derived from the Voyager-1 all-electron (electron plus positron) data by more than one order of magnitude for $M_\mathrm{PBH}\gtrsim 1\times 10^{16}\,\mathrm{g}$, and also stronger than our previous constraints derived from the AMS-02 positron data for $M_\mathrm{PBH}\gtrsim 2\times 10^{16}\,\mathrm{g}$.

</details>


### [30] [Probing New Physics and CP Violation in $ν_τn \to Λ_c τ^- (π^- ν_τ)$ and $\barν_τp \to Λτ^+ (π^+ \barν_τ)$](https://arxiv.org/abs/2601.19397)
*E. Hernández,J. Nieves,J. E. Sobczyk*

Main category: hep-ph

TL;DR: The paper investigates neutrino interactions with nuclei to search for lepton flavor universality violation and CP violation beyond the Standard Model using pion angular distributions in tau decays.


<details>
  <summary>Details</summary>
Motivation: The motivation is to find evidence of physics beyond the Standard Model (SM) through lepton flavor universality violation and CP violation, using tau polarization studies in specific neutrino interactions.

Method: The authors analyze the processes ν_τn → Λ_c τ^- (π^- ν_τ) and ν̄_τp → Λ τ^+ (π^+ ν̄_τ), examining pion energy and angular distributions. They use an effective low-energy SM extension with dimension-six operators, incorporating complex Wilson coefficients to capture CP-violating effects. An azimuthal-angle asymmetry in pion decay is proposed as a CP-odd observable.

Result: The proposed observable, an azimuthal-angle asymmetry, is identified as a 'smoking-gun' signature of beyond-SM physics. The effect's strength is estimated using lattice QCD-derived nucleon-hyperon form factors.

Conclusion: The study demonstrates that measuring tau polarization via pion angular distributions can uniquely probe CP-violating new physics scenarios outside the Standard Model.

Abstract: We study the processes $ν_τn \to Λ_c τ^- (π^- ν_τ)$ and $\barν_τp \to Λτ^+ (π^+ \barν_τ)$, with particular emphasis on the pion energy and angular distributions, as a possible signal for lepton flavor universality violation, and in general of physics beyond the Standard Model (SM), as well as a sensitive probe of the $τ$ polarization vector. We work within an effective low-energy extension of the SM with all dimension-six four-fermion operators. In this framework, complex Wilson coefficients which encode new physics can generate CP-violating contributions. We propose an observable that provides a genuine CP-odd signal due to its sensitivity to particular transverse components of the $τ$ polarization vector. Namely, we show that the asymmetry in the azimuthal-angle distribution of the pion in the decay $τ^\pm\to π^\pm ν_τ$ constitutes a smoking-gun prediction of such a beyond the SM scenario. We estimate the strength of this effect extrapolating nucleon-hyperon form factors recently obtained from lattice QCD calculations.

</details>


### [31] [Lepton flavor violating signals driven by CP symmetry of order 4](https://arxiv.org/abs/2601.19398)
*Bei Liu,Igor P. Ivanov*

Main category: hep-ph

TL;DR: The paper explores the extension of the CP4 3HDM to the lepton sector, investigating constraints on lepton-Higgs couplings and suppressing lepton flavor violating (LFV) processes. It identifies a viable scenario compatible with experimental data and tests its interpretation of a CMS hint for a 146 GeV scalar decaying to eμ.


<details>
  <summary>Details</summary>
Motivation: To address the impact of CP4 symmetry on lepton sectors, ensuring that lepton flavor violating signals remain within experimental constraints, and to validate the model against recent CMS experimental hints.

Method: The authors extended CP4 to lepton sectors, considered two LFV process classes (Higgs decays and μ→eγ), performed Yukawa sector scans using physical lepton properties as inputs, and analyzed compatibility with experimental data.

Result: A viable CP4 3HDM scenario was found that suppresses LFV effects while accommodating the CMS 146 GeV scalar hint. This scenario is testable at future colliders.

Conclusion: CP4 3HDM can be consistently extended to leptons with controlled LFV effects, offering testable predictions for future experiments, particularly regarding the 146 GeV scalar and LFV decay signatures.

Abstract: CP4 3HDM is a curious version of the three-Higgs-doublet model built upon a CP symmetry of order 4 (dubbed CP4). When extended to fermions, CP4 leads to unusually tight correlations between the scalar and Yukawa sectors and induces tree-level flavor changing neutral couplings. Still, viable scenarios exist, in which quark flavor changing signals remain within experimental limits. In this work, we extend CP4 to the lepton sector and investigate whether the lepton-Higgs couplings and lepton flavor violating (LFV) signals can also be kept under control. We consider two classes of LFV processes: tree-level lepton decays of the 125 GeV Higgs boson and one-loop radiative decay $μ\rightarrow eγ$. For each CP4-invariant lepton Yukawa scenario, we perform a focused Yukawa sector scan that uses physical lepton properties as input and suppresses LFV effects. We identify a promising CP4 3HDM scenario compatible with the present-day experimental constraints, show that it can accommodate the recent CMS hint of a 146 GeV scalar decaying to $eμ$, and argue that this interpretation can be tested at future colliders.

</details>


### [32] [Predictions of effective Majorana neutrino mass under radiative corrections to $μ-τ$ reflection symmetry](https://arxiv.org/abs/2601.19419)
*Prokash Pegu,Chandan Duarah*

Main category: hep-ph

TL;DR: The paper investigates the impact of radiative corrections on the deviation from μ-τ reflection symmetry in predicting the effective Majorana neutrino mass ⟨m⟩_ee for neutrinoless double beta decay (0νββ).


<details>
  <summary>Details</summary>
Motivation: To explore how radiative corrections affect the μ-τ reflection symmetry and its implications on ⟨m⟩_ee, ensuring consistency with experimental bounds from KamLAND-Zen.

Method: Extends previous work by applying radiative corrections to the light Majorana neutrino mass matrix and mixing parameters. Estimates ⟨m⟩_ee using low-energy values derived from seesaw-scale inputs, then compares to experimental upper limits.

Result: The computed ⟨m⟩_ee values at the electroweak scale are found to be within the latest KamLAND-Zen upper bound (0.028–0.122 eV), validating the model's consistency with current experimental data.

Conclusion: Radiative corrections play a role in maintaining the μ-τ reflection symmetry scenario's viability, with predictions aligning with experimental constraints on ⟨m⟩_ee.

Abstract: The search for neutrinoless double beta decay ($0νββ$) is currently one of the key objectives in neutrino physics research. The decay rate of $0νββ$ decay depends on the effective Majorana neutrino mass $|\langle m \rangle_{ee}|$. In this work we study the numerical prediction of $|\langle m \rangle_{ee}|$ in the scenario of deviation from the $μ$-$τ$ reflection symmetry due to radiative corrections, as an extension of our earlier work \cite{pegu}. In \cite{pegu}, we consider an exact $μ$-$τ$ reflection symmetry in the light effective Majorana neutrino mass matrix and in the corresponding lepton mixing matrix as well at the seesaw scale. We choose numerical values of all the mixing parameters and neutrino mass eigenvalues at the seesaw scale as inputs and estimate the values of mass eigenvalues and mixing parameters at the electroweak scale due to radiative corrections. We find these low energy predictions consistent with global $3σ$ oscillation data. In the present work, we compute the effective Majorana neutrino mass $|\langle m \rangle_{ee}|$ using these low energy values at the electroweak scale. We find that the low energy predictions of $|\langle m \rangle_{ee}|$ are consistent with the latest upper bound $|\langle m \rangle_{ee}|<(0.028-0.122)\ eV$ provided by KamLAND-Zen Collaboration.

</details>


### [33] [Radiative Dirac neutrino masses and dark matter in a $U(1)_{B-L}$ extended model](https://arxiv.org/abs/2601.19454)
*Chayan Majumdar,Utkarsh Patel,Supriya Senapati,Sudhanwa Patra*

Main category: hep-ph

TL;DR: The paper explores a U(1)_{B-L} extension of the Standard Model, generating Dirac neutrino masses via one-loop radiative processes. It links neutrino mass generation to a dark sector stabilized by a Z_6 symmetry, and examines dark matter phenomenology, collider signals, and flavor violating processes, showing promising detection prospects at LHC and future muon colliders.


<details>
  <summary>Details</summary>
Motivation: To address the origin of neutrino masses and dark matter stability within a unified framework, connecting them through a gauge symmetry extension of the Standard Model.

Method: Proposes a U(1)_{B-L} model where neutrino masses arise radiatively at one-loop. Analyzes flavor violation constraints, dark matter relic density and direct detection limits, and predicts collider signatures for dark sector particles.

Result: Shows the model is consistent with observations and constraints, identifies viable dark matter candidates detectable in colliders even with lower luminosity.

Conclusion: The U(1)_{B-L} extension provides a viable pathway to link neutrino physics and dark matter, with testable signatures at existing and future colliders.

Abstract: We study a $U(1)_{B-L}$ extension of the Standard Model (SM) in which Dirac neutrino masses are generated radiatively at the one-loop level through the exchange of new beyond the SM fields. This framework establishes a direct connection between neutrino mass generation and the dark sector, with the stability of the dark matter ensured by a residual discrete $Z_6$ symmetry arising from the spontaneous breaking of $U(1)_{B-L}$. We investigate the resulting charged lepton flavor violating processes and dark matter phenomenology, saturating relic observations and direct-detection constraints, and analyze the collider signatures of the dark sector at the Large Hadron Collider and at a future muon collider. We have identified excellent prospects for observing the considered dark matter candidates in these colliders, even with lower integrated luminosities than the proposed one.

</details>


### [34] [Spectrum of radiation from global strings and the relic axion density](https://arxiv.org/abs/2601.19463)
*Richard A. Battye,Lukasz P. Bunio,Steven J. Cotterill,Pranav B. Gangrekalve Manoj*

Main category: hep-ph

TL;DR: The paper examines radiation from global strings and its effect on relic axion density, finding significant corrections needed, and obtains varying axion mass estimates via different emission scenarios.


<details>
  <summary>Details</summary>
Motivation: To address uncertainties in prior axion mass predictions by analyzing radiation mechanisms from global strings, particularly distinguishing axion emission from string self-fields.

Method: Used a simplified model analyzing radiation spectra from perturbed straight strings, comparing simulated vs self-field corrected data using Kalb-Ramond action predictions.

Result: Found axion radiation spectrum follows an exponential (non-'hard') profile; estimated axion masses of ~4-160 μeV and detection frequencies ~1-38 GHz depending on emission mechanism and corrections applied.

Conclusion: Highlights remaining uncertainties in relic density calculations despite methodological improvements, suggesting higher axion masses than the Initial Misalignment Mechanism predicts when using corrected spectra.

Abstract: We discuss key aspects of the nature of radiation from global strings and its impact on the relic axion density. Using a simple model we demonstrate the dependence on the spectrum of radiation emitted by strings. We then study the radiation emitted by perturbed straight strings paying particular attention to the difference between the overall phase of the field and the small perturbations about the string solution which are the axions. We find that a significant correction is required to be sure that one is analyzing the axions and not the self-field of the string. Typically this requires one to excise a sizeable region around the string - something which is not usually done in the case of numerical field theory simulations of string networks. We have measured the spectrum of radiation from these strings and find that it is compatible with an exponential, as predicted by the Nambu-like Kalb-Ramond action, and in particular is not a ``hard'' spectrum often found in string network simulations. We conclude by attempting to assess the uncertainties on relic density and find that this leads to a range of possible axion masses when compared to the measured density from the Cosmic Microwave Background, albeit that they are typically higher than what is predicted by the Initial Misalignment Mechanism. If the decay is via a ``soft spectrum'' from loops produced close to the backreaction scale we find that $m_{\rm a}\approx 160\,μ{\rm eV}$ and a detection frequency $f\approx 38\,{\rm GHz}$. If axions are emitted directly by the string network, and we use emission spectra reported in field theory simulations, then $m_{\rm a}\approx 4\,μ{\rm eV}$ and $f\approx 1\,{\rm GHz}$, however this increases to $m_a \approx 125\,μ{\rm eV}$ and $f\approx 30\,{\rm GHz}$ using our spectra for the case of an oscillating string. In all scenarios there are significant remaining uncertainties that we delineate.

</details>


### [35] [Event generation with exponential scaling in multiplicity using AmpliCol](https://arxiv.org/abs/2601.19483)
*Rikkert Frederix,Timea Vitos*

Main category: hep-ph

TL;DR: The paper introduces AmpliCol, a method that extends a two-step event generation strategy to general Standard Model processes, achieving efficient LHC event generation with stable exponential scaling in multiplicity, outperforming conventional methods.


<details>
  <summary>Details</summary>
Motivation: To overcome the computational challenges of generating high-multiplicity LHC events due to the rising cost of QCD matrix element evaluations, which grow factorially with increasing particles.

Method: AmpliCol uses a two-step approach: first generating unweighted events with the leading-colour approximation, then reweighting to full-colour accuracy. It leverages LC's efficiency while ensuring FC precision across various SM processes.

Result: Benchmark tests on multi-jet, tt̄+jets, ZZ+jets, and Drell-Yan+jets show runtime scales exponentially (not factorially) with multiplicity, enabling feasible generation at multiplicities previously computationally impossible.

Conclusion: AmpliCol provides a significant improvement for LHC event generation at high multiplicities, making previously intractable calculations computationally viable.

Abstract: Efficient generation of LHC events is hindered by the rapidly rising cost of evaluating QCD matrix elements with increasing multiplicity. We build on a recently proposed two-step strategy in which unweighted events are first generated using the leading-colour (LC) approximation and then reweighted to full-colour (FC) accuracy, utilising the LC integration efficiency while recovering the exact FC prediction. In this work we extend the method to general Standard Model processes and present AmpliCol, a standalone implementation designed for LHC collisions. We benchmark multi-jet, $t\bar{t}$+jets, $ZZ$+jets, and Drell-Yan+jets production, measuring the time required to obtain a fixed number of unweighted events at FC accuracy. Across all processes, the runtime exhibits a stable exponential scaling with multiplicity, far milder than the factorial growth of conventional matrix-element generators. This demonstrates that the AmpliCol code enables efficient event generation at multiplicities that are otherwise computationally prohibitive.

</details>


### [36] [Probing EFT breakdown in the tails of $W^+ W^-$ observables](https://arxiv.org/abs/2601.19495)
*Daniel Gillies,Andrea Banfi,Adam Martin*

Main category: hep-ph

TL;DR: The study evaluates clipping EFT simulations using $M_{WW} < Λ$ as a validity criterion, finding it insufficient for maintaining operator hierarchy. Alternative methods like bin-wise comparison, data cuts, and observables like $M_{T3}$ show better potential. The approach may introduce form factor-like modifications, risking EFT model independence.


<details>
  <summary>Details</summary>
Motivation: Assess the effectiveness of clipping EFT simulations by imposing $M_{WW} < Λ$ to ensure EFT validity, and compare it against bin-wise dimension comparisons and data cuts. Investigate correlations between observables to improve EFT fitting accuracy and maintain model independence.

Method: Compare three validation methods: (1) clipping EFT simulations with $M_{WW} < Λ$, (2) bin-by-bin comparison of dimension-6/8 contributions, and (3) data cuts. Analyze correlations between $M_{WW}$, $M_{eμ}$, and transverse mass observables ($M_{T1}, M_{T2}, M_{T3}$). Conduct sensitivity studies using $M_{T3}$ and $M_{eμ}$ distributions. Test $M_{T3}$ cuts instead of $M_{WW}$ clipping.

Result: The $M_{WW} < Λ$ clipping is insufficient to enforce EFT operator hierarchy due to correlations affecting bin validity. $M_{T3}$ and $M_{T1}$ better track $M_{WW}$ than $M_{eμ}$. Replacing $M_{WW}$ clipping with $M_{T3}$ cuts improves validation. Imposing $M_{WW}$ cuts on simulations may introduce form factors, compromising EFT model independence.

Conclusion: Clipping EFT simulations via $M_{WW} < Λ$ is inadequate; alternate methods like transverse mass observables and bin-wise analysis are preferable. Further caution is needed to preserve EFT's model independence as introduced biases could affect fit reliability.

Abstract: In this letter, we test clipping effective field theory (EFT) simulations as a method of ensuring EFT validity. The procedure imposes that, at the level of the simulation, the invariant mass of a $W^+W^-$ pair $M_{WW}$ is less than the new physics scale $Λ$. We compare this to two other methods, comparison bin by bin of dimension-6 and dimension-8 squared contributions and implementing a cut on data. We find that setting $M_{WW} < Λ$ is not strict enough to ensure that the hierarchy of EFT operators is respected for dimension-6 and dimension-8 contributions. We also show that, even when using a stricter cut on $M_{WW}$, due to different correlations between $M_{WW}$ and $M_{eμ}$ at different EFT orders, the bins in $M_{eμ}$ (the invariant mass of the leptons originating from $W$ decays) used in an EFT fit may not truly be in the regime of EFT validity when performing a dimension-6 fit with $M_{WW} < Λ$. We also explore the correlations of three transverse mass observables: $M_{T1}, M_{T2}$ and $M_{T3}$, finding that $M_{T1}$ and $M_{T3}$ follow the $M_{WW}$ distribution more closely than $M_{eμ}$. We present sensitivity studies using both the $M_{T3}$ distribution and $M_{eμ}$ distribution. We test implementing an experimental cut on $M_{T3}$ in place of clipping the EFT simulation at $M_{WW} < Λ$. We finally comment that adding $M_{WW} < Λ$ cuts only to the EFT simulation could be interpreted as modifying the SMEFT expansion by a form factor and could therefore impact the model independence of EFT fits under this procedure.

</details>


### [37] [Radiative return at NLOPS accuracy](https://arxiv.org/abs/2601.19530)
*Ettore Budassi,Carlo M. Carloni Calame,Marco Ghilardi,Andrea Gurgone,Guido Montagna,Mauro Moretti,Oreste Nicrosini,Fulvio Piccinini,Francesco P. Ucci*

Main category: hep-ph

TL;DR: The paper presents a precise calculation of next-to-leading order QED corrections for radiative return processes in flavor factories, implemented in an updated Monte Carlo generator to improve precision measurements of the pion form factor relevant for the muon g-2.


<details>
  <summary>Details</summary>
Motivation: The pion form factor is a critical input for calculating the leading-order hadronic contribution to the muon anomalous magnetic moment. Existing methods require improved precision, necessitating accurate radiative return and energy scan techniques employment.

Method: Exact NLO QED corrections are computed for e+e- → X+X-γ (X=π, μ), including initial/final state radiation and their interference. A novel parton shower matched to NLO (NLOPS) approach handles exclusive photon emission in 2→3 processes. Validation tests compare against existing NLO results and incorporate factorised scalar QED for hadronic channels.

Result: The implementation in BabaYaga@NLO provides NLOPS-accurate simulations meeting flavor factory experimental criteria. Cross-checks validate the approach, and numerical results demonstrate improved precision for pion form factor measurements.

Conclusion: The developed framework enhances theoretical predictions for radiative return experiments, enabling more accurate pion form factor determinations necessary for improving the muon g-2 determination through dispersive calculations.

Abstract: The radiative return, together with the energy scan, is the method used at flavour factories to measure the pion form factor, which is a crucial input for the data-driven dispersive computation of the leading-order hadronic contribution to the muon anomalous magnetic moment. We consider the radiative hadronic and leptonic channels of main experimental interest, namely the processes $e^+e^-\to X^+X^-γ$, with $X = \{π\, , μ\}$. For such processes, we compute the exact next-to-leading order (NLO) corrections matched to a Parton Shower (PS) to describe exclusive multiple photon emission. All sources of radiative corrections from initial-state and final-state radiation, as well as their interference, are considered according to QED for $e^+e^-\toμ^+μ^-γ$ and QED$\oplus$F$\times$sQED (Factorised scalar QED) for $e^+e^-\toπ^+π^-γ$. We describe in detail the novel features of our PS approach to compute the fixed-order corrections in association with higher-order contributions to $2\to3$ processes, with a hard photon in the final state. We present validation tests and comparisons with NLO predictions available in the literature to cross-check various ingredients of our formulation. We also show numerical results at NLOPS accuracy according to realistic event selection criteria for precision measurements at flavour factories. Our calculation is implemented in an updated version of the Monte Carlo event generator BabaYaga@NLO, which can be used for fully exclusive simulations and data analysis in radiative return experiments.

</details>


### [38] [Probing Solar Neutrino Deficit via Torsion-Induced Flavor Change in f(T) Gravity](https://arxiv.org/abs/2601.19550)
*H. Yazdani Ahmadabadi,H. Mohseni Sadjadi*

Main category: hep-ph

TL;DR: The paper explores how spacetime torsion in f(T) gravity affects neutrino flavor oscillations and uses solar neutrino data to constrain model parameters.


<details>
  <summary>Details</summary>
Motivation: To probe modified teleparallel gravity and its effects in astrophysical environments by studying neutrino oscillations influenced by torsion.

Method: Deriving an effective coupling between torsion vector and neutrino current using the Dirac action in teleparallel geometry. Calculating torsion-induced phase shifts and mass-squared differences in weak-field limits around spherical masses. Analyzing vacuum oscillations and MSW resonance effects.

Result: Both vacuum oscillations and MSW resonance are modified by torsion. Parameters of teleparallel models and neutrino-torsion coupling are constrained using solar neutrino data.

Conclusion: Spacetime torsion in f(T) gravity significantly impacts neutrino oscillation patterns, providing a viable probe for testing modified gravity theories using astrophysical observations.

Abstract: We investigate whether the spacetime torsion can modify neutrino flavor oscillations in f(T) gravity. This offers a probe of modified teleparallel gravity in astrophysical environments. By using the Dirac action in teleparallel geometry, we derive an effective coupling between the torsion vector and neutrino current. In the weak-field limit around a spherical mass, we obtain analytical expressions for torsion-induced phase shifts and effective mass-squared differences. Our results indicate that both vacuum oscillations and the Mikheyev-Smirnov-Wolfenstein (MSW) resonance in matter are affected by these torsion-based modifications. Using solar neutrino data from Super-Kamiokande, SNO, Borexino, and KamLAND, we constrain the teleparallel model parameters and also the neutrino-torsion coupling.

</details>


### [39] [How well does NRQCD describe quarkonium production?](https://arxiv.org/abs/2601.19619)
*Mathias Butenschoen*

Main category: hep-ph

TL;DR: The paper reassesses the NRQCD factorization framework by rigorously fitting color octet LDMEs to LHC data, demonstrating its robustness across a wide range of quarkonium production observables, including high-p_T J/psi and η_c production, Upsilon production through NRQCD relations, and low-p_T J/psi in photon-induced processes (excluding high z regions). The results show better agreement than previously thought, with remaining discrepancies addressable via proposed resummation methods.


<details>
  <summary>Details</summary>
Motivation: To resolve longstanding debates about NRQCD's validity by systematically evaluating its predictive power against experimental data, especially addressing scale uncertainties and expanding observable coverage beyond prior studies.

Method: A next-to-leading order NRQCD analysis with systematic scale uncertainty treatment, fitting CO LDMEs to LHC data for J/ψ and η_c production. Includes evaluation of Υ(nS) production via NRQCD-derived relations and J/ψ production in photoproduction/gamma-gamma collisions while excluding high-z regions.

Result: Surprisingly good agreement between NRQCD predictions and data up to highest measured p_T for J/ψ, accurate Υ(nS) descriptions via NRQCD links, and successful low-p_T J/ψ reproduction in photon collisions. Remaining discrepancies lie in regions where resummation approaches are proposed solutions.

Conclusion: NRQCD factorization performs far better than conventionally acknowledged when systematic uncertainties and broader observables are considered. Most remaining issues have proposed resolution pathways, strengthening NRQCD's foundational status in quarkonium physics.

Abstract: The question how well nonrelativistic QCD (NRQCD) factorization can describe quarkonium production has been subject to debate since its invention. We review our recent reanalysis of a classic next-to-leading order color octet (CO) long distance matrix element (LDME) fit to large transverse momentum $p_T$ $J/ψ$ and $η_c$ LHC production data. Our analysis differs from previous analyses of this kind not only by implementing for the first time a systematic treatment of scale uncertainties, but also by scrutinizing a much broader range of observables. Surprisingly, $J/ψ$ hadroproduction is well described up to the highest measured values of $p_T$. Potential NRQCD based relations nontrivially lead to a perfect description of $Υ(nS)$ production data. Furthermore, $J/ψ$ production in $γp$ and $γγ$ collisions is, contrary to prevailing conceptions, reproduced down to $p_T=1$ GeV, as long as the region of large inelasticity $z$ is excluded. The overall picture is much rosier than usually perceived, the more so as the remaining discrepancies appear in phase space regions where solutions via varying kinds of resummations have been proposed.

</details>


### [40] [Discriminating QCD Compton and Quark-Antiquark Annihilation Processes in $γ$ + Jets Using Interpretable Machine Learning](https://arxiv.org/abs/2601.19645)
*Monalini Samal,Nihar Ranjan Sahoo*

Main category: hep-ph

TL;DR: The study examines the effectiveness of jet substructure techniques in distinguishing QCD Compton and quark-antiquark annihilation processes in photon-jet production. Key observables like jet multiplicity and girth show strong discrimination, while jet charge is ineffective. Results indicate classifier performance is constrained by QCD radiation, not algorithm complexity, offering insights for precision jet measurements in various collision systems.


<details>
  <summary>Details</summary>
Motivation: To quantify how well jet substructure methods can preserve information about the underlying hard process through hadronization and reconstruction, establishing a baseline for precision measurements in different collision environments.

Method: Trained boosted decision trees and multilayer perceptrons on labeled quark/gluon-initiated jets from dijet events. Applied these classifiers to photon-jet samples using IR/C Stable observables like jet multiplicity, girth, mass, and charge.

Result: Jet multiplicity and girth were most discriminatory, jet mass contributed less, and jet charge showed no significant power. The separation limit was QCD-radiation dominated rather than classifier-related.

Conclusion: Findings demonstrate the survival of hard process information post-hadronization, providing physics-driven benchmarks for future precision studies in proton-proton, lepton-ion/heavy-ion collisions.

Abstract: We investigate how effectively final-state jet substructure can discriminate between QCD Compton and quark-antiquark annihilation processes from photon-jet production in $pp$ collisions at $\sqrt{s}=13$ TeV. Using infrared- and collinear-safe jet observables, multivariate classifiers -- boosted decision trees and multilayer perceptrons -- are trained on labeled quark- and gluon-initiated jets from dijet events and applied to photon-jet samples. Observables probing soft and wide-angle radiation, in particular jet multiplicity and jet girth, dominate the discrimination. The jet mass provides a complementary but weaker contribution, while the jet charge exhibits negligible discriminating power. A comparison of the two classifiers demonstrates that the achievable separation is limited primarily by QCD radiation effects rather than by classifier complexity. These findings quantify the extent to which information about the underlying hard process survives hadronization and realistic jet reconstruction, providing a physics-driven baseline for precision jet measurements in $pp$, $ep/$A, and heavy-ion collisions.

</details>


### [41] [From $B_c$ mesons to the baryon asymmetry: a unified $B$ Mesogenesis Framework](https://arxiv.org/abs/2601.19651)
*M. Burgos Marcos,A. Verheyden,K. K. Vos*

Main category: hep-ph

TL;DR: This paper refines B mesogenesis by including all B meson channels post-reheating, updating neutral meson contributions, exploring B_c+ decays with new mechanisms, and showing viable baryogenesis through combined mechanisms.


<details>
  <summary>Details</summary>
Motivation: To address the baryon asymmetry problem by improving the B mesogenesis model, resolving previous tensions in neutral-meson contributions and incorporating unexplored B_c+ decay channels.

Method: The study updates neutral B meson (B_d^0 and B_s^0) contributions using time-integrated decay rates; introduces a systematic analysis of B_c+ decays with branching ratio predictions using factorization and data-driven methods; examines two B_c+ mesogenesis channels involving direct CP violation and neutral-meson oscillations; combines all mechanisms to assess viability under early-universe conditions.

Result: The B_d^0 contribution is reduced by ~40%, alleviating prior tension; B_c+ decays offer competitive contributions to baryogenesis; combined mechanisms successfully achieve baryogenesis across broad parameter spaces dependent on CP asymmetry and mixing parameters.

Conclusion: B mesogenesis remains viable through combined neutral and B_c+ contributions, with future B_c+ measurements essential to further validate the model.

Abstract: $B$ mesogenesis offers an interesting mechanism to generate the baryon asymmetry of the universe by converting the CP violation of the Standard Model into a net baryon number asymmetry. In this work we refine and extend the $B$ mesogenesis framework by incorporating all relevant $B$ meson channels active after low-temperature reheating. We first update the known neutral-meson contribution using time-integrated decay rates. While the $B_s^0$ contribution remains essentially unchanged, we find a suppression of the $B_d^0$ term by a factor $\sim 0.4$ with respect to previous analyses, alleviating the tension associated with its expected negative sign.
  We then perform a systematic study of $B_c^+$ decays, which are basically unexplored. We provide branching-ratio predictions using both leading-order factorization and a data-driven approach inspired by $D\to M_1M_2$ decays. These estimates allow us to quantify two $B_c^+$ sources of mesogenesis: the previously discussed $B_c^+ \to B^+ M^0$ channel and a new mechanism introduced in this work, $B_c^+ \to B_q^0 M$, in which the asymmetry is generated by combining direct CP violation with neutral-meson oscillations. Interestingly, in these channels the charm quark decays. Therefore, these decays give access to {\it charm CP violation} in modes with percent level branching ratios. With moderate assumptions, we find that $B_c^+$ mesogenesis can match or exceed the neutral contribution.
  Finally, we combine all three mechanisms and explore their viability in terms of the direct CP asymmetry, neutral-meson mixing parameters and early-universe fragmentation fractions. We find that successful baryogenesis can be achieved in a broad parameter space, showing the viability of $B$ mesogenesis. Future measurements of $B_c^+$ modes are thus highly anticipated to further probe the viability of unified $B$ mesogenesis.

</details>


### [42] [The leading Lyapunov exponent in the glasma](https://arxiv.org/abs/2601.19679)
*Pooja,Dana Avramescu,Tuomas Lappi*

Main category: hep-ph

TL;DR: The paper demonstrates that small perturbations in boost-invariant glasma color fields exhibit exponential growth proportional to the square root of time, interpreting this as a Lyapunov exponent linked to entropy production and thermalization in early heavy-ion collisions. For SU(2), growth follows ~exp(0.4√(g²μτ)), showing insensitivity to initial perturbation details.


<details>
  <summary>Details</summary>
Motivation: To understand the dynamics of thermalization and entropy production in the earliest stages of heavy-ion collisions by analyzing the behavior of perturbations in glasma fields.

Method: Linear analysis of small perturbations in boost-invariant color fields, extracting time dependence of unstable mode growth and examining dependence on initial conditions and momentum scales.

Result: The perturbations exhibit sqrt(time)-dependent exponential growth with a Lyapunov exponent-like rate, independent of initial conditions but dependent on system parameters (g²μ). The unstable mode couples to all initial momentum scales.

Conclusion: The exponential growth rate indicates a universal thermalization mechanism in gluonic matter, with the Lyapunov exponent providing insight into entropy production timescales in relativistic collisions.

Abstract: We show that small perturbations in the boost-invariant color fields of the glasma exhibit an exponential growth with the square root of time. We interpret this growth rate as a Lyapunov exponent, related to entropy production and the thermalization timescale in the earliest stage of heavy-ion collisions. Working in a regime that is linear in this perturbation, we extract the time dependence of this mode as $\sim \exp(0.4\sqrt{g^2μτ})$ for SU($2$), where $g^2μ$ is proportional to the saturation scale and the square-root dependence is caused by the boost-invariant expansion of the system. We show that the growth rate of this mode is, unlike its amplitude, remarkably insensitive to the details of how the perturbations are initialized. In particular, we show that the unstable mode couples to all momentum scales present in the initial perturbation.

</details>


### [43] [Glueball mass from RGZ-inspired infrared gluodynamics: a Euclidean Bethe-Salpeter approach](https://arxiv.org/abs/2601.19727)
*Rodrigo Carmo Terin*

Main category: hep-ph

TL;DR: The paper solves the Euclidean Bethe-Salpeter equation for the lightest scalar glueball in Yang-Mills theory using an infrared-complete gluon propagator, yielding a mass of ~1.9 GeV consistent with lattice and correlator-based results.


<details>
  <summary>Details</summary>
Motivation: To study the scalar glueball spectrum and validate the refined Gribov-Zwanziger framework against lattice data by solving the Bethe-Salpeter equation with an improved infrared gluon propagator.

Method: Minimal ladder truncation of Bethe-Salpeter equation with constant kernel strength g_C^2, using RGZ gluon propagator for input. Focus on s-wave component to calculate glueball mass.

Result: Glueball mass found between 1.7-2.3 GeV, with optimal value at ~1.9 GeV when g_C^2=0.54. Results align with RGZ infrared moment analyses and lattice expectations.

Conclusion: Confirms RGZ infrared gluodynamics predictions through bound-state analysis, serving as a cross-check for the framework's validity in describing gluon dynamics.

Abstract: We formulate and solve a Euclidean Bethe-Salpeter equation for the lightest scalar glueball (0++) in pure Yang-Mills theory, using the refined Gribov-Zwanziger gluon tree-level propagator as an infrared-complete input. In a minimal ladder truncation with an effective constant kernel strength g_C^2 and the dominant s-wave component, we extract scalar glueball masses in the range 1.7-2.3 GeV for representative values of g_C^2, with a preferred value around 1.9 GeV near g_C^2 = 0.54. The result is consistent with RGZ correlator-based infrared moment analyses and with lattice expectations, providing a cross-check of RGZ-inspired infrared gluodynamics from a bound-state viewpoint.

</details>


### [44] [Strong CP and the QCD Axion: Lecture Notes via Effective Field Theory](https://arxiv.org/abs/2601.19735)
*Francesco Sannino*

Main category: hep-ph

TL;DR: The paper provides a graduate-level introduction to the strong CP problem and QCD axion physics using effective field theory (EFT). It covers chiral EFT of QCD, the Witten-Veneziano relation, extraction of CP-odd amplitudes, generalization to SU(N) theories, and analysis of the Peccei-Quinn mechanism.


<details>
  <summary>Details</summary>
Motivation: To explore the strong CP problem and axion physics systematically through EFT approaches, clarify the theoretical framework's predictions, address recent claims about resolving the CP problem without axions, and review solutions like the Peccei-Quinn mechanism.

Method: Uses chiral EFT to construct a theta-dependent potential, applies the Veneziano-Yankielowicz Lagrangian for supersymmetric insights, and analyzes CP-odd observables like neutron EDM. Critiques a recent 'no strong CP' proposal by showing it introduces an extra degree of freedom. Derives axion potential and mass via EFT.

Result: Demonstrates how EFT explains vacuum alignment and theta periodicity via topological susceptibility. Derives CP-odd mesonic/baryonic amplitudes and validates the Peccei-Quinn mechanism as a valid solution. Shows flawed assumptions in the contested 'no strong CP' claim requiring unnatural terms.

Conclusion: EFT provides a robust framework for QCD CP studies. The Peccei-Quinn mechanism with an axion remains the most natural solution. Recent claims avoiding axions fail to respect QCD requirements, reinforcing the axion's necessity in resolving the strong CP problem.

Abstract: These lecture notes provide a self-contained, graduate-level introduction to the strong $CP$ problem and QCD axion physics from an effective field theory (EFT) viewpoint. We review the construction of the chiral EFT of QCD yielding a $θ$-dependent potential, from which vacuum alignment, $θ$ periodicity and branch structure follow. We further show how the framework leads to the Witten-Veneziano relation highlighting the role of the pure-glue topological susceptibility in organizing $θ$-dependent hadronic observables. Using these tools, we show how to extract representative $CP$-odd mesonic and baryonic amplitudes, including the chiral estimate underlying the neutron EDM bound, and how to generalize the effective framework to confining SU(N) theories with fermions in arbitrary representations. We further show how to employ the Veneziano-Yankielowicz effective Lagrangian for N=1 supersymmetric Yang-Mills theory to extract salient information on the $θ$-dependent physics of one-flavour QCD via orientifold planar equivalence. We also revisit a recent no strong $CP$ claim based on an ordering of limits in the sum over topological sectors and show, in the EFT language, that it amounts to introducing an extra non-propagating axion-like degree of freedom not required by QCD. We then present the standard dynamical resolution to the strong $CP$ problem, i.e. the Peccei-Quinn mechanism, the resulting axion potential and mass from chiral EFT and briefly review associated time-honored UV completions, and the axion quality problem from gravitational corrections.

</details>


### [45] [Oscillating Resonances: Imprints of ultralight dark matter at colliders](https://arxiv.org/abs/2601.19844)
*Martin Bauer,Sreemanti Chakraborti*

Main category: hep-ph

TL;DR: The paper explores how ultralight dark matter models produce oscillations in fundamental constants via effective operators. Mediator fields, when integrated out, induce these effects, leading to 'oscillating resonances' in colliders. The study proposes analysis techniques using time-stamped data to detect such signals through Fourier transforms and mass-binned data, offering a new collider signature for ultralight dark matter even with stringent atomic clock constraints.


<details>
  <summary>Details</summary>
Motivation: To investigate observable collider signatures of ultralight dark matter scenarios where the misalignment mechanism causes oscillations in fundamental constants. The goal is to provide a testable alternative to atomic clock constraints and to identify methods to detect these effects through resonance searches in experiments like Belle II, LHCb, and SHiP.

Method: The authors derive effective operators from models with mediator fields interacting with Standard Model particles. They analyze how the oscillating mediator mass smears resonance peaks in collider data. Techniques are proposed to reconstruct the resonance using mass-binned data over oscillation periods and time-resolved Fourier analysis to detect periodic signal variations.

Result: Oscillating resonances can be identified in colliders by observing peak smearing over time or periodic signal fluctuations. Calculations show sensitivity to mediator parameters within reach of current/future experiments despite tight atomic clock bounds. Specific analysis frameworks for existing data (Belle II, LHCb) and future projections (SHiP) are quantified.

Conclusion: Collider experiments can probe ultralight dark matter by searching for oscillating resonances, providing complementary evidence beyond astrophysical/dark matter signals. Time-stamped data analysis methods could uncover these phenomena, opening a novel experimental pathway even with existing constraints on fundamental constant variations.

Abstract: In models where ultralight fields constitute dark matter, the misalignment mechanism leads to coherent, low-amplitude oscillations in fundamental constants. This effect arises from effective operators that couple dark matter to Standard Model fields. We present different models that can induce these effective operators by integrating out a mediator field. For mediator masses within the reach of collider searches, an alternative way to discover ultralight dark matter is to search for a resonance. Due to being a mediator to dark matter, the mass of the mediator oscillates. The resonance therefore, should not appear as a single isolated peak, but is smeared out once data is averaged over an oscillation cycle or more. Remarkably, the oscillation period and amplitude are in the range of current and future collider searches, even though constraints from atomic clocks probing variations of the fine-structure constant and the electron mass are very strong. We recast existing searches and projections from Belle II, LHCb and SHiP for an `oscillating resonance', and discuss how the periodicity of the signal can be used to reconstruct the peak from mass-binned data. We further show that time-stamped data would allow to unfold the signal via a fast Fourier transform and determine the significance of the signal for different background levels. The discovery of an oscillating resonance at a collider, with characteristics as predicted in ultralight dark matter scenarios, would constitute a powerful probe of dark matter's underlying nature.

</details>


### [46] [A Comprehensive Effective Field Theory Framework for Coherent Elastic Neutrino-Nucleus Scattering](https://arxiv.org/abs/2601.19883)
*Gang Li,Chuan-Qiang Song,Feng-Jie Tang,Jiang-Hao Yu*

Main category: hep-ph

TL;DR: This paper introduces a comprehensive effective field theory (EFT) framework for coherent elastic neutrino-nucleus scattering (CEνNS) to bridge high-energy physics and low-energy experimental data. It systematically analyzes operators up to dimension 8, incorporates QCD renormalization effects, matches to the chiral Lagrangian, and enables combined analyses of experimental data to constrain EFT parameters and neutrino interactions.


<details>
  <summary>Details</summary>
Motivation: To address the need for a systematic theoretical framework that connects high-energy physics scenarios with low-energy CEνNS observations, facilitated by recent experimental measurements from COHERENT, CONUS+, PandaX-4T, and XENONnT.

Method: Develops an end-to-end EFT framework considering dimension-8 operators in the low-energy EFT (LEFT), QCD renormalization group running, spurion method for matching to chiral Lagrangian, nuclear response function power counting with nucleon number scaling, and SM EFT operator completions.

Result: Establishes a consistent top-down workflow for calculating CEνNS cross sections, incorporates systematic power counting for nuclear effects, and enables combined experimental constraints on EFT scales and neutrino non-standard interactions.

Conclusion: The framework provides a robust foundation for precision tests of the Standard Model, neutrino property studies, and new physics searches via integrated analysis of CEνNS experimental data.

Abstract: Coherent elastic neutrino-nucleus scattering (CE$ν$NS) stands out as a pivotal process for precision tests of the Standard Model electroweak sector, investigations of neutrino properties, and searches for new physics (NP). Recent experimental measurements by COHERENT, CONUS+, and ton-scale xenon detectors--including PandaX-4T and XENONnT--underscore the need for a systematic theoretical framework to bridge high-energy physics scenarios with low-energy observational data. In this work, we develop a comprehensive end-to-end effective field theory (EFT) framework for CE$ν$NS, encompassing the complete energy scale hierarchy spanning the ultraviolet (UV) regime down to the nuclear sector. We consider the low-energy EFT (LEFT) operators up to dimension 8, incorporating their QCD renormalization group running effects, and employ the systematic spurion method to achieve the matching between these operators and the chiral Lagrangian. A full power counting analysis is performed, extending to nuclear response functions, which evaluates contributions from LEFT operators up to dimension 8 while accounting for the nucleon number enhancement effect intrinsic to CE$ν$NS. Moreover, we match the relevant LEFT operators for CE$ν$NS onto operators up to dimension 8 within the Standard Model EFT. By also providing their complete tree-level ultraviolet completions, this procedure establishes a consistent top-down theoretical workflow. Leveraging a broad suite of CE$ν$NS experimental data, this framework enables a combined analysis to extract constraints on the scales of EFT operators and neutrino non-standard interaction parameters.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [47] [Energetic Ceilings of Astrophysical Gravitational-Wave Backgrounds](https://arxiv.org/abs/2601.18859)
*Chiara M. F. Mingarelli*

Main category: astro-ph.HE

TL;DR: The paper establishes an upper energy limit for astrophysical stochastic gravitational wave backgrounds (GWB) across all frequencies. It applies this limit to various GW sources and finds that the observed GWB is consistent with being produced by ultramassive black holes, with a total combined background from all sources not exceeding Ω_gw ~ 10^-7.


<details>
  <summary>Details</summary>
Motivation: The motivation is to determine an absolute energetic ceiling for stochastic GW backgrounds to constrain astrophysical sources and validate observations. This helps in understanding the energy budgets of different populations and matching theoretical predictions with experimental results like the NANOGrav, EPTA, and PPTA GWB amplitudes.

Method: Deriving a population-agnostic scaling law based on rest mass-energy conversion efficiency for gravitational radiation. Applying this to specific sources (e.g., supermassive black hole binaries, IMRIs, binary neutron stars) to compute upper bounds. Comparing computed ceilings with observational data from pulsar timing arrays.

Result: The supermassive black hole binary background has a ceiling of A ≈ 1.6×10^-15 at 1 yr⁻¹, aligning with NANOGrav results. The total combined GWB from all sources cannot exceed Ω_gw ~ 10^-7, suggesting ultramassive black holes could power the observed signal.

Conclusion: The derived energetic limits are consistent with current observations, supporting ultramassive black holes as the source. These bounds provide a framework for future GW detection experiments and constrain astrophysical models without overproducing gravitational wave energy.

Abstract: Every astrophysical stochastic gravitational wave (GWB) is limited by the amount of rest mass available to be converted into gravitational radiation. Here we derive a population-agnostic scaling law that places an absolute energetic ceiling on stochastic backgrounds across the entire GW frequency spectrum, from nanoHertz to kilohertz. We apply this framework to bound the backgrounds from supermassive black hole binaries, intermediate-mass black hole captures by supermassive black holes in AGN disks, extreme mass-ratio inspirals, binary neutron stars, Population III remnants, and stellar-mass binary black holes. We find that the energetic ceiling for supermassive black hole binaries is $A \leq 1.6^{+0.3}_{-0.3} \times 10^{-15}$ at a reference frequency of $1\,{\rm yr}^{-1}$. This astrophysical GWB ceiling is within $1σ$ with the GWB amplitude reported by NANOGrav, EPTA, and PPTA, implying that the current observed signal is consistent with being powered by a population of ultramassive black holes ($M_\bullet \gtrsim 10^{10}\,M_\odot$). Finally, we demonstrate that the total astrophysical background from all channels combined cannot exceed $Ω_{\rm gw} \sim 10^{-7}$.

</details>


### [48] [Induced Scattering of Fast Radio Bursts in Magnetar Magnetospheres](https://arxiv.org/abs/2601.18865)
*Rei Nishiura,Shoma F. Kamijima,Kunihito Ioka*

Main category: astro-ph.HE

TL;DR: The paper studies Compton/Brillouin scattering in magnetized electron-positron plasmas using kinetic theory and simulations, applying this to FRBs in magnetars. The scattering enters a linear growth phase despite magnetic suppression. A critical density determines if scattering continues fully or saturates, allowing FRBs to escape. This resolves issues with compact emission regions and explains FRB diversity linked to X-ray bursts.


<details>
  <summary>Details</summary>
Motivation: To address discrepancies between theoretical models and observations of FRBs, particularly their compact emission regions and diversity in associations with X-ray bursts. The study aims to understand scattering mechanisms in magnetar magnetospheres under strong magnetic fields which prior work may have oversimplified.

Method: Kinetic theory analysis combined with Particle-in-Cell (PIC) simulations. The team modeled magnetized electron-positron pair plasmas to study induced Compton and Brillouin scattering effects on electromagnetic waves relevant to FRB propagation.

Result: Scattering processes initiate linear growth despite magnetic suppression. A critical plasma density threshold was identified: above it, full scattering occurs; below it, saturation allows FRB escape. This explains how FRBs can originate from small regions and why some are linked to X-ray bursts while others are not.

Conclusion: Magnetic field effects modify but do not prevent scattering's linear growth phase. The critical density mechanism resolves observational tensions by allowing FRB escape under certain conditions, naturally explaining their observational diversity in emission scale and associations with transient events.

Abstract: We investigate induced Compton/Brillouin scattering of electromagnetic waves in magnetized electron and positron pair plasma by verifying kinetic theory with Particle-in-Cell simulations. Applying this to fast radio bursts (FRBs) in magnetar magnetospheres, we find that the scattering--although suppressed by the magnetic field--inevitably enters the linear growth stage. The subsequent evolution bifurcates: full scattering occurs when the density exceeds a critical value, whereas below it the scattering saturates and the FRB can escape. This eases the tension with observations of compact emission regions and may explain the observed diversity, including the presence or absence of FRBs associated with X-ray bursts.

</details>


### [49] [AT2018cow Powered by a Shock in Aspherical Circumstellar Media](https://arxiv.org/abs/2601.18887)
*Taya Govreen-Segal,Ehud Nakar,Kenta Hotokezaka,Christopher M Irwin,Eliot Quataert*

Main category: astro-ph.HE

TL;DR: The paper presents a quantitative model explaining the luminous fast blue optical transient AT2018cow through shock interactions in an aspherical circumstellar medium, accounting for observed X-ray and optical emissions, their coordinated evolution, and related phenomena.


<details>
  <summary>Details</summary>
Motivation: To address unresolved puzzles of AT2018cow, such as coordinated optical/X-ray evolution after day 20, the disappearing hard X-ray hump, and other anomalies like X-ray fluctuations and NIR excess.

Method: Developed a physical model where shocks in an asymmetric CSM produce emissions. Modeled X-ray reprocessing into optical/UV, explored radiative shock instabilities for X-ray variability, and analyzed CSM structure impacts on light curves and energy parameters.

Result: Quantitatively reproduced observational data including bolometric luminosity, X-ray spectra, luminosity ratios, and explained features like X-ray variability and NIR excess. Derived explosion energy (~1-5e50 erg), ejecta velocity (~0.1c), and CSM mass (~0.3 Msun).

Conclusion: The model's success confirms asymmetric CSM and radiative shock dynamics as key to AT2018cow's behavior. The inferred parameters provide constraints on progenitor systems and mechanisms for such transients.

Abstract: We present a quantitative model for the luminous fast blue optical transient AT2018cow in which a shock propagating through an aspherical circumstellar medium (CSM) produces the X-ray and UV/optical/NIR emission. X-rays are emitted from hot post-shock electrons, and soft X-ray photons are reprocessed into optical/UV emission in the cool downstream. This naturally explains two previously puzzling features: (i) the coordinated evolution of the optical and soft X-ray after day 20, (ii) the hard X-ray hump above 10 keV that disappears around day 15 as the Thomson optical depth transitions from $τ_T \gg1$ to $τ_T \sim 1$.
  Our model is over-constrained, and it quantitatively reproduces the bolometric luminosity evolution, soft X-ray spectrum, and time-dependent soft/hard X-ray and soft X-ray/optical luminosity ratios. It also explains additional puzzles: X-ray fluctuations with $\sim4-10$ day timescales arise from a global radiative shock instability, while the NIR excess and the apparent receding blackbody radius result from reprocessed X-rays in matter far from thermodynamic equilibrium. The radio is naturally explained as originating from a shock driven by the same ejecta in the more dilute CSM. The light curve steepening after $\sim 40$ days likely indicates the shock reaches the edge of the dense CSM at $\sim {\rm few} \times 10^{15}$ cm. We infer explosion energy $\sim 1-5 \times 10^{50}$ erg, carried by an ejecta at $\sim 0.1c$ and a mass of $0.01-0.05 M_\odot$, in a dense asymmetric CSM with $\sim 0.3 M_\odot$, embedded in a more dilute CSM.

</details>


### [50] [Multiwavelength Analysis of Six Luminous, Fast Blue Optical Transients](https://arxiv.org/abs/2601.18926)
*Cassie Sevilla,Anna Y. Q. Ho,Nayana A. J.,Steve Schulze,Daniel A. Perley,Michael Bremer,Igor Andreoni,Ivan Altunin,Thomas G. Brink,Poonam Chandra,Ping Chen,Ashley A. Chrimes,Michael W. Coughlin,Kaustav K. Das,Andrew Drake,Alexei V. Filippenko,Christoffer Fremling,James Freeburn,Avishay Gal Yam,Mary Gerhart,Matthew J. Graham,George Helou,K-Ryan Hinds,Natalya Johnson,Mansi M. Kasliwal,Harsh Kumar,Russ R. Laher,Natalie LeBaron,Maggie L. Li,Chang Liu,Ben Margalit,Yu-Jing Qin,Nabeel Rehemtulla,Sophia Risin,Sam Rose,Rupak Roy,Ben Rusholme,Genevieve Schroeder,Jesper Sollerman,Kailai Wang,Jacob L. Wise,Yi Yang,Yuhan Yao,WeiKang Zheng*

Main category: astro-ph.HE

TL;DR: The analysis of six luminous fast blue optical transients (LFBOTs) observed by the ZTF survey reveals common characteristics, including fast light-curve evolution, blue colors, high luminosity, and X-ray/radio detections. Radio peaking at 50-100 days (except AT2024aehp) suggests a dense environment with fast shocks. Host galaxies are star-forming with non-nuclear offsets, suggesting progenitor scenarios involving massive stellar mergers.


<details>
  <summary>Details</summary>
Motivation: To investigate the nature of LFBOTs through multiwavelength observations and determine their progenitor systems and environments by analyzing their light curves, spectral properties, and host galaxy characteristics.

Method: Multiwavelength observations (optical, X-ray, radio) of six LFBOTs from ZTF data were analyzed. Radio emissions were modeled using synchrotron radiation for shock dynamics. Host galaxy photometry and spectroscopy were used to determine environments. Comparisons were made between transient behaviors and existing theories.

Result: LFBOTs show consistent radio peaks, dense circumburst medium, and X-ray variability. AT2024aehp's optical plateau and radio brightening highlight unique properties. Host galaxies are star-forming with non-nuclear offsets, supporting progenitor scenarios like massive star-compact object mergers.

Conclusion: The observed features suggest progenitors involving recent mass loss in a dense environment, favoring scenarios like mergers of massive stars with compact objects over models requiring circumburst winds.

Abstract: We present multiwavelength observations and analysis of six luminous fast blue optical transients (LFBOTs) discovered in Zwicky Transient Facility (ZTF) survey data. We identified these LFBOTs from their fast light-curve evolution ($t_{1/2}\leq 12 $d), blue colors at peak brightness ($g-r\leq-0.5 $mag), a visible host galaxy, high optical luminosity ($M_g<-20$), and an X-ray or radio detection.
  With the exception of AT2024aehp (ZTF24abygbss), these transients exhibit peaks in their $10\,$GHz radio light curves at $t_{\text{rest}} \approx 50-100$ d, with peak radio luminosities ranging from $10^{38}-10^{40}$ erg s$^{-1}$. Modeling the radio emission as synchrotron radiation indicates a fast ($v=0.1-0.3c$) shock in a dense ($n_e\approx10^{3}-10^{4}$ cm$^{-3}$) medium. The X-ray emission varies by $\approx2$ orders of magnitude in luminosity ($10^{42}-10^{44}$ erg s$^{-1}$) at $t_{\text{rest}}\sim20 $d.
  Analysis of the host-galaxy photometry and spectroscopy for each transient shows that they are predominantly nonnuclear (a few kpc offset) with star-forming host galaxies of stellar masses $10^{9}-10^{11} ,M_\odot$.
  Unlike all other LFBOTs to date, AT2024aehp exhibited a luminous ($M<-19 $mag) plateau in the optical light curve; spectra during this plateau phase showed a featureless blue continuum. The $6-15$ GHz radio emission of AT2024aehp brightened by over an order of magnitude from $t_{\text{rest}} \approx70 $d to $t_{\mathrm{rest}} \approx130 $d.
  The mostly consistent radio behavior between optically selected LFBOTs implies a similar circumburst medium, leading us to prefer a progenitor scenario in which mass is lost in a consistent way shortly prior to the terminal event, such as a massive star merging with a compact object.

</details>


### [51] [Progenitor of the recoiling super-massive black hole RBH-1 identified using HST/JWST imaging](https://arxiv.org/abs/2601.18986)
*Tousif Islam,Tejaswi Venumadhav,Digvijay Wadekar*

Main category: astro-ph.HE

TL;DR: The detection of a runaway supermassive black hole (RBH-1) with a high velocity provides constraints on the merger history of its progenitor black holes, suggesting a precessing, unequal-mass system with a highly spinning primary black hole. This merger could be a significant source for future LISA observations and indicates a major 'wet' galaxy merger.


<details>
  <summary>Details</summary>
Motivation: To understand the merger dynamics of supermassive black holes that overcome the final-parsec problem, using the observed runaway velocity of RBH-1 to infer progenitor properties and implications for galaxy formation.

Method: Combining Hubble and JWST imaging data with gravitational-wave recoil models from numerical relativity and perturbation theory to constrain mass ratios, spins, and merger scenarios.

Result: The progenitor black holes were precessing with a mass ratio ≤6:1, the more massive having a spin ~0.75, merged ~70 Myr ago. The host galaxy GX likely formed from a major gas-rich merger with mass ratio ≤4:1.

Conclusion: Such mergers are promising for LISA detection (SNR ~1000) and highlight the role of dynamical interactions in supermassive black hole evolution.

Abstract: Using a combination of \textit{Hubble Space Telescope} and \textit{James Webb Space Telescope} imaging, a runaway supermassive black hole (RBH-1) was recently identified with an inferred velocity of $954^{+110}_{-126}\,\mathrm{km\,s^{-1}}$, likely ejected from a compact star-forming galaxy (denoted as GX) at $z \approx 0.96$. Assuming the runaway black hole was the outcome of the gravitational-wave-driven merger of two black holes, we use its measured runaway velocity together with gravitational-wave recoil predictions from numerical relativity and black hole perturbation theory to constrain the mass ratio and spin configuration of the progenitor SMBHs that overcame the final-parsec problem and merged $\sim 70$~Myr ago. We find that the progenitor binary must have been precessing, with a mass ratio $m_1/m_2\lesssim 6$, and that the more massive SMBH must have possessed a high spin (dimensionless spin magnitude $\sim 0.75$) in order to generate a recoil of this magnitude. This has important astrophysical implications as similar SMBH mergers can be an interesting source population for the upcoming LISA mission with signal-to-noise ratios $\gtrsim$ 1000. Furthermore, the progenitor SMBH properties imply that GX was likely formed through a major, gas-rich (``wet'') merger between two galaxies of comparable mass, with a mass ratio $\lesssim 4$.

</details>


### [52] [A multiwavelength view of the nearby Calcium-Strong Transient SN 2025coe in the X-Ray, Near-Infrared, and Radio Wavebands](https://arxiv.org/abs/2601.19018)
*Sahana Kumar,Raphael Baer-Way,Aravind P. Ravi,Maryam Modjaz,Poonam Chandra,Stefano Valenti,Lindsey A. Kwok,Samaporn Tinyanont,Ryan J. Foley,D. Andrew Howell,Daichi Hiramatsu,Jennifer E. Andrews,K. Azalee Bostroem,Collin Christy,Noah Franz,Brian Hsu,Jeniveve Pearson,David J. Sand,Manisha Shrestha,Nathan Smith,Bhagya Subrayan*

Main category: astro-ph.HE

TL;DR: SN 2025coe is a nearby Calcium-strong transient (CaST) with He-rich features and interaction with CSM, suggesting mass-loss mechanisms common to this subclass.


<details>
  <summary>Details</summary>
Motivation: To understand the origin of CaSTs, which exhibit characteristics of both thermonuclear and core-collapse SNe, by studying the closest observed example.

Method: Multi-wavelength observations (NIR spectroscopy, X-ray, radio) to analyze SN 2025coe's spectral properties and CSM interaction over time.

Result: Detection of SN Ibs-like spectral signatures, X-ray emission from CSM interaction (~0.12 M☉ at 2e15 cm), and radio non-detections constraining CSM extent to ~4e15 cm. Consistency with other X-ray detected CaSTs.

Conclusion: CaSTs likely involve progenitors with significant mass-loss or polluting mechanisms, consistent with He-star core-collapse scenarios, but some uncertainties remain about progenitor systems.

Abstract: Calcium-strong transients (CaSTs) are a subclass of faint and rapidly evolving supernovae (SNe) that exhibit strong calcium features and notably weak oxygen features. The small but growing population of CaSTs exhibits some aspects similar to thermonuclear supernovae and others that are similar to massive star core-collapse events, leading to intriguing questions on the physical origins of CaSTs. SN 2025coe is one of the most nearby CaSTs discovered to date, and our coordinated multi-wavelength observations obtained days to weeks post-explosion reveal new insights on these enigmatic transients. With the most robust NIR spectroscopic time-series of a CaST collected to date, SN 2025coe shows spectral signatures characteristic of Type Ib SNe (SNe Ib, i.e. He-rich stripped-envelope SNe). SN~2025coe is the third X-ray detected CaST and our analysis of the \textit{Swift} X-ray data suggest interaction with 0.12 $\pm\,0.11\ M_{\odot}$ of circumstellar material (CSM) extending to at least $2 \times 10^{15} $cm ($\sim 30,000\ R_{\odot}$), while our analysis of the 1-240 GHz radio non-detections gives an outer radius of that CSM of at most $\sim 4\times 10^{15}$ cm. This inferred nearby high-density CSM extending out to $3\pm 1 \times10^{15}$ cm is similar to that seen in the other two X-ray detected CaSTs, and its presence suggests that either intensive mass-loss or some polluting mechanism may be a common feature of this subclass. Our work also expands upon recent studies on the optical properties of SN 2025coe and explores our current understanding of different progenitor systems that could possibly produce CaSTs.

</details>


### [53] [Maximum Energy of Particles Accelerated in GRB Afterglow Shocks](https://arxiv.org/abs/2601.19135)
*Zhao-Feng Wu,Sofía Guevara-Montoya,Paz Beniamini,Dimitrios Giannios,Daniel Grošelj,Lorenzo Sironi*

Main category: astro-ph.HE

TL;DR: The study uses particle-in-cell (PIC) simulation-based models to analyze gamma-ray burst (GRB) afterglows, showing that low-energy bursts in sparse environments exhibit a GeV synchrotron cutoff early post-burst. Current Fermi-LAT data can't distinguish between PIC-derived acceleration mechanisms and the Bohm limit, but future MeV-TeV observations will improve constraints on relativistic shock acceleration.


<details>
  <summary>Details</summary>
Motivation: To test predictions from PIC simulations about electron acceleration in relativistic shocks, particularly the maximum electron energy below the Bohm limit, and to determine observational signatures like synchrotron cutoffs in GRB afterglows.

Method: The authors model the spectral evolution of GRB afterglows during the relativistic deceleration phase, incorporating acceleration prescriptions from PIC simulations. They compute synchrotron and synchrotron self-Compton emission self-consistently, applying the framework to GRB 190114C and GRB 130427A.

Result: Low-energy GRBs in low-density environments show a pronounced GeV synchrotron cutoff shortly after the burst. Current Fermi-LAT observations lack sufficient resolution to differentiate between PIC-based models and Bohm-limit assumptions due to observational uncertainties.

Conclusion: Future MeV-TeV observations of GRB afterglows are necessary to break model degeneracies and constrain particle acceleration mechanisms in relativistic shocks more effectively than current data.

Abstract: Particle acceleration in relativistic collisionless shocks remains an open problem in high-energy astrophysics. Particle-in-cell (PIC) simulations predict that electron acceleration in weakly magnetized shocks proceeds via small-angle scattering, leading to a maximum electron energy significantly below the Bohm limit. This upper bound manifests observationally as a characteristic synchrotron cutoff, providing a direct probe of the underlying acceleration physics. Gamma-ray burst (GRB) afterglows offer an exceptional laboratory for testing these predictions. Here, we model the spectral evolution of GRB afterglows during the relativistic deceleration phase, incorporating PIC-motivated acceleration prescriptions and self-consistently computing synchrotron and synchrotron self-Compton emission. We find that low-energy bursts in low-density environments, typical of short GRBs, exhibit a pronounced synchrotron cutoff in the GeV band within minutes to hours after the trigger. Applying our framework to GRB 190114C and GRB 130427A, we find that current observations are insufficient to discriminate between PIC-motivated acceleration and the Bohm limit, primarily due to large uncertainties in the Fermi-LAT band. Nevertheless, future MeV-TeV afterglow observations can break model degeneracies and place substantially tighter constraints on particle acceleration in relativistic shocks.

</details>


### [54] [Latent characterisation of the complete BATSE gamma ray bursts catalogue using Gaussian mixture of factor analysers and model-estimated overlap-based syncytial clustering](https://arxiv.org/abs/2601.19140)
*Fan Dai,Ranjan Maitra*

Main category: astro-ph.HE

TL;DR: The paper analyzes 1,150 gamma-ray bursts (GRBs) using a Gaussian factor analyzers mixture model and identifies five ellipsoidal-shaped groups differentiated by duration, fluence, and spectrum. By applying MOBSynC clustering, these merge into three then two groups, resolving discrepancies with previous two- or three-class studies. The approach uses latent factors to characterize GRBs and reveals a multi-layered structure in their variability.


<details>
  <summary>Details</summary>
Motivation: To address discrepancies in previous GRB classification studies (which suggested 2-3 groups) and provide a comprehensive, multi-layered characterization of GRBs using advanced statistical methods, explaining their variability through latent features derived from nine observed parameters.

Method: The study employs a Gaussian factor analyzers mixture model on nine parameters (T50, T90, F1-F4, P64, P256, P1024) from 1,150 BATSE GRBs to identify five ellipsoidal groups. Model-estimated overlap-based syncytial clustering (MOBSynC) is then used to merge groups into three and eventually two clusters, focusing on duration differences. Latent factors from the parameters help characterize each group.

Result: Five initial groups are identified (short-faint-hard to long-bright-intermediate), which merge into two main groups separated by duration. Latent factors reveal spectral and brightness characteristics tied to the nine parameters, resolving previous classification discrepancies by showing hierarchical structure in GRB populations.

Conclusion: The analysis demonstrates a multi-layered classification of GRBs (5→3→2 groups), reconciling prior conflicting results. The latent factor approach provides deeper insight into GRB variability patterns, suggesting that both intrinsic properties and observational biases contribute to the observed diversity. This hierarchical structure offers a framework for future studies linking GRB classifications to physical mechanisms.

Abstract: Characterising and distinguishing gamma-ray bursts (GRBs) has interested astronomers for many decades. While some authors have found two or three groups of GRBs by analyzing only a few parameters, recent work identified five ellipsoidally-shaped groups upon considering nine parameters $T_{50}, T_{90}, F_1, F_2, F_3, F_4, P_{64}, P_{256}, P_{1024}$. Yet others suggest sub-classes within the two or three groups found earlier. Using a mixture model of Gaussian factor analysers, we analysed 1150 GRBs, that had nine parameters observed, from the current Burst and Transient Source Experiment (BATSE) catalogue, and again established five ellipsoidal-shaped groups to describe the GRBs. These five groups are characterised in terms of their average duration, fluence and spectrum as shorter-faint-hard, long-intermediate-soft, long-intermediate-intermediate, long-bright-intermediate and short-faint-hard. The use of factor analysers in describing individual group densities allows for a more thorough group-wise characterisation of the parameters in terms of a few latent features. However, given the discrepancy with many other existing studies that advocated for two or three groups, we also performed model-estimated overlap-based syncytial clustering (MOBSynC) that successively merges poorer-separated groups. The five ellipsoidal groups merge into three and then into two groups, one with GRBs of low durations and the other having longer duration GRBs. These groups are also characterised in terms of a few latent factors made up of the nine parameters. Our analysis provides context for all three sets of results, and in doing so, details a multi-layered characterisation of the BATSE GRBs, while also explaining the structure in their variability.

</details>


### [55] [Constraining FRB Microstructure with Polarised Shot Noise](https://arxiv.org/abs/2601.19254)
*J. C. F. Balzan,A. Bera,C. W. James,B. Meyers*

Main category: astro-ph.HE

TL;DR: FIRES is a polarised shot-noise framework modeling FRB dynamic spectra as incoherent Gaussian microshots. Applied to FRB 20191001A and FRB 20240318A, it explains scattering's suppression of PA variability on trailing edges, quantifies parameters like microshot number and intrinsic PA dispersion through R_ψ and Π_L vs σ_ψ analysis, and constrains viable microphysical models without emission mechanism assumptions.


<details>
  <summary>Details</summary>
Motivation: To explain the observed polarimetric behaviors of FRBs (e.g., PA variability suppression), quantitatively constrain model parameters, and explore the role of microshot superposition in FRB emission without relying on specific emission mechanisms.

Method: Developed FIRES framework treating FRB bursts as incoherent superpositions of Gaussian microshots. Analyzed CRAFT FRBs using PA variance ratio R_ψ and Π_L vs σ_ψ plots to constrain N (microshot count), σ_ψ (intrinsic PA dispersion), and Π_{L,0} (intrinsic polarization fraction). Accounted for scattering, signal-to-noise, finite sampling, and noise effects.

Result: FRB 20191001A shows wide parameter degeneracies (σ_ψ ~10-30°, N ~5-1000). FRB 20240318A has narrower constraints (N ≤20, σ_ψ ~15-23°) linked to its PA variability. FIRES successfully reproduces observed phenomena and provides direct constraints on microphysical models.

Conclusion: Microshot superposition naturally explains diverse FRB polarimetry. FIRES offers an emission-agnostic tool to refine FRB emission models by combining minimal assumptions with observational data, highlighting parameter interdependencies and scattering effects.

Abstract: We present FIRES, a polarised shot-noise framework that models fast radio burst (FRB) dynamic spectra as the incoherent superposition of Gaussian microshots. Applied to the CRAFT bursts FRB 20191001A and FRB 20240318A, FIRES reproduces key spectro-polarimetric behaviours: scattering suppresses position-angle (PA) variability on the trailing edge, while the leading edge preferentially retains intrinsic structure when sufficient signal-to-noise is present. We quantify this behaviour using the PA variance ratio $\mathcal{R}_ψ$ and explore the joint plane of measured linear polarisation fraction $Π_L$ versus PA variance to constrain the allowed parameter space of microshot number $N$, intrinsic PA dispersion $σ_ψ$, and intrinsic linear fraction $Π_{L,0}$ at fixed signal-to-noise. For FRB~20191001A, the data are consistent with an extended region spanning $σ_ψ\sim 10^\circ$--$30^\circ$ and $N \sim 5$--$1000$, reflecting degeneracies between intrinsic PA structure, microshot superposition, scattering, finite sampling, and noise. FRB~20240318A occupies a more restricted region, favouring fewer microshots ($N \lesssim 20$) and larger intrinsic PA dispersion ($σ_ψ\sim 15$--$23^\circ$), depending on $Π_{L,0}$, consistent with its observed PA variability. By combining an emission-mechanism-independent framework with minimal assumptions and observational constraints, FIRES provides direct, quantitative constraints on the space of viable FRB microphysical models and demonstrates that microshot superposition offers a natural explanation for the diverse polarimetric behaviours observed in FRBs.

</details>


### [56] [STONKS first results: Long-term transients in the XMM-Newton Galactic plane survey](https://arxiv.org/abs/2601.19328)
*Robbie Webbe,E. Quintin,N. A. Webb,Gabriele Ponti,Tong Bao,Chandreyee Maitra,Shifra Mandel,Samaresh Mondal*

Main category: astro-ph.HE

TL;DR: The STONKS pipeline enables detection of faint X-ray transients in XMM-Newton data, identifying 70 astrophysical sources including new discoveries and classifications.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current instruments in detecting faint X-ray transients and enable faster follow-up studies of extreme astrophysical events.

Method: Analysis of over 200 XMM-Newton observations using the STONKS pipeline, screening for instrumental effects, followed by temporal and spectral analysis of 78 alerts from 70 sources.

Result: 70 astrophysical sources identified, 32 confidently classified (including X-ray binaries, γ-Cas analogues, magnetar candidates), 23 newly detected in X-rays. Demonstrates STONKS' effectiveness in detecting fainter transients.

Conclusion: STONKS significantly enhances the ability to detect and classify variable X-ray sources, offering advantages over existing systems and enabling community-driven transient studies.

Abstract: The study of astronomical transients at high energies provides insights into some of the most extreme physical events in the universe; however, carrying out their detection and fast follow-up studies are limited by instrumental constraints. Search for Transient Object in New observations using Known Sources (STONKS) is a near-real-time transient detection system for XMM-Newton offering the capability to detect transients in XMM-Newton observations at fainter fluxes than can be achieved with wide survey instruments. We present the transients detected with the STONKS pipeline found in an XMM-Newton multi-year heritage survey of the Galactic plane to identify and classify highly variable X-ray sources that have recently been reported in this region. We examined the alerts created by the STONKS pipeline from over 200 XMM observations of the Galactic plane, screening for instrumental effects. The 78 alerts associated with 70 real astrophysical sources were then subjected to further temporal and spectral analysis. From the 70 sources we identified, we were able to classify 32 with a high degree of confidence, including 7 X-ray binaries, 1 $γ$-Cas analogue, and 1 magnetar candidate. Of the 70 sources, 23 were detected for the first time in X-rays. This systematic analysis of publicly available data has shown the value and potential of STONKS in the application to XMM-Newton observations. It will enable the community to detect transient and highly variable sources at fainter fluxes than with any other X-ray transient detection systems.

</details>


### [57] [Numerical simulations of black hole-neutron star mergers with equal and near-equal mass ratios](https://arxiv.org/abs/2601.19405)
*Ivan Markin,Mattia Bulla,Tim Dietrich*

Main category: astro-ph.HE

TL;DR: The study investigates black hole-neutron star mergers with near equal mass ratios, finding discrepancies in gravitational waveform models and potential detectability of kilonova emissions by observatories like Vera C. Rubin and DECam.


<details>
  <summary>Details</summary>
Motivation: The detection of GW230529_181500 highlights the need for better models of symmetric mass ratio mergers to avoid misclassifying events and missing electromagnetic counterparts. Limited numerical simulations exist for such systems, impacting waveform accuracy and understanding remnant properties.

Method: Performed numerical-relativity simulations of black hole-neutron star mergers with mass ratios q = 1, 1/2, 1/3. Compared results with existing gravitational waveform models and fitting formulas for remnant properties (mass, spin, disk mass, ejecta). Analyzed disk dynamics and modeled kilonova emissions.

Result: Waveform models showed significant dephasing (~1 rad) at merger. Fitting formulas accurately predicted dynamical ejecta and disk masses but poorly estimated BH spin. Identified disk oscillations modulating accretion rates. Predicted detectable kilonova signals by Rubin Observatory and DECam within days at 200 Mpc distances.

Conclusion: Current waveform models require improvement for symmetric mergers. Accurate simulations are critical to prevent observational misinterpretation and enhance detection of electromagnetic counterparts. Future observatories can effectively observe these events, aiding in multi-messenger astronomy.

Abstract: The detection of GW230529_181500 suggested the existence of more symmetric black hole-neutron star mergers where the black hole mass can be as low as 2.6 times that of the neutron star. Black hole-neutron star binaries with even more symmetric mass ratios are expected to leave behind massive disks capable of driving bright electromagnetic transients like kilonovae. Currently, there is only a limited number of numerical-relativity simulations of black hole-neutron star mergers in this regime, which are vital for accurate gravitational waveform models and analytical fitting formulas for the remnant properties. Insufficient accuracy of these may lead to misclassification of real events and potentially missed opportunities to locate their electromagnetic counterparts. To fill this gap in the parameter space coverage, we perform simulations of black hole-neutron star mergers with mass ratios $q \in \{1, 1/2, 1/3\}$. We find the gravitational waveform models do not show good agreement with the numerical waveforms, with dephasing at the level of around 1 rad at the merger. We find that the masses of the dynamical ejecta and disk are in good agreement with the available fitting formulas. The analytical formulas for the remnant black hole are in excellent agreement for the black hole mass, but are less accurate with the predictions for its spin. Moreover, we analyze the remnant disk structure and dynamics, deriving the rotation law and identifying global trapped $g$-mode density oscillations. We distinguish three types of accretion in the postmerger and find modulation of the accretion rate by the global oscillations of the disk. Finally, we model the kilonova emission these systems would produce and find that most of them are potentially detectable by Vera C. Rubin Observatory within four days after merger, and by DECam within two days after merger if located at a distance of 200 Mpc.

</details>


### [58] [A highly ionised outflow in the X-ray binary 4U 1624-49 detected with XRISM](https://arxiv.org/abs/2601.19480)
*M. Díaz Trigo,E. Caruso,E. Costantini,T. Dotani,T. Kohmura,M. Shidatsu,M. Tsujimoto,T. Yoneyama,J. Neilsen,T. Yaqoob,J. M. Miller*

Main category: astro-ph.HE

TL;DR: The study detects an accretion disc wind in the high inclination LMXB 4U 1624-49 using XRISM spectroscopy, confirming a thermal-radiative pressure origin with velocities of 200-320 km/s and column density above 1e23 cm-2.


<details>
  <summary>Details</summary>
Motivation: To resolve the disputed origin of accretion disc winds by investigating the presence and properties of a wind in 4U 1624-49, which exhibits persistent accretion and highly ionised plasmas observed in X-ray spectra.

Method: Phase-resolved spectroscopy with XRISM/Resolve across the full binary orbit (excluding absorption dips) to analyze radial velocity curves, line profiles, and plasma characteristics.

Result: Detection of an outflow with velocities ~200-320 km/s, column density >1e23 cm-2, and narrow line profiles (50-100 km/s), suggesting low turbulence and potential increase near absorption dips due to mixing.

Conclusion: The observed wind characteristics align with thermal-radiative pressure launching from the disc outskirts, supporting this mechanism over others like magneto-centrifugal processes.

Abstract: The origin of accretion disc winds remains disputed to date. High inclination, dipping, neutron star Low Mass X-Ray Binaries (LMXBs) provide an excellent testbed to study the launching mechanism of such winds due to being persistently accreting and showing a nearly ubiquitous presence of highly-ionised plasmas. We aim to establish or rule out the presence of a wind in the high inclination LMXB 4U 1624-49, for which a highly ionised plasma has been repeatedly observed in X-ray spectra by Chandra and XMM-Newton, and a thermal-radiative pressure wind is expected. We leverage the exquisite spectral resolution of XRISM to perform phase-resolved spectroscopy of the full binary orbit to characterise the highly ionised plasma at all phases except during absorption dips. An outflow is clearly detected via phase-resolved spectroscopy of the source with XRISM/Resolve. Based on analysis of the radial velocity curve we determine an average velocity of ~200-320 km/s and a column density above 10$^{23}$ cm$^{-2}$. The line profiles are generally narrow, spanning from ~50 to ~100 km/s, depending on the orbital phase, pointing to a low velocity sheer or turbulence of the highly ionised outflow and a potential increase of turbulence as the absorption dip is approached, likely due to turbulent mixing. The line profiles, together with the derived launching radius and wind velocity are consistent with a wind being launched from the outskirts of the disc and without stratification, pointing to a thermal-radiative pressure origin.

</details>


### [59] [On the rarity of rocket-driven Penrose extraction in Kerr spacetime](https://arxiv.org/abs/2601.19616)
*An T. Le*

Main category: astro-ph.HE

TL;DR: The study uses Monte Carlo simulations to explore energy extraction from rotating Kerr black holes via the Penrose process with rocket propulsion, finding that successful extraction requires extremely high black hole spin and ultra-relativistic exhaust velocities, but occurs in only ~1% of cases without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The Penrose process is a theoretical mechanism for extracting energy from black holes, but its practical viability under realistic conditions (like rocket propulsion mechanisms) has not been well quantified. The study aims to determine under what precise conditions energy extraction is kinematically possible and efficient.

Method: Over 250,000 trajectory simulations using Monte Carlo methods to model rocket propulsion within the Kerr ergosphere, analyzing exhaust energy, velocity thresholds, and thrust strategies (single-impulse vs continuous).

Result: Successfully achieving energy extraction with escape to infinity occurs rarely (1% baseline), requiring a/M ≥ 0.89, exhaust velocities ≥ 0.91c. Optimized 'sweet spot' conditions achieved 88.5% success, while single-impulse thrust at periapsis yielded higher efficiency (19% vs 2-4%).

Conclusion: Penrose process energy extraction via rockets requires extreme fine-tuning and is statistically rare, supporting the observational dominance of electromagnetic mechanisms in astrophysical energy extraction from black holes.

Abstract: We present a Monte Carlo study of energy extraction from rotating (Kerr) black holes via the Penrose process using rocket propulsion. Through over 250,000 trajectory simulations, we establish sharp constraints on when Penrose extraction with escape to infinity succeeds. The mechanism requires that exhaust ejected inside the ergosphere carries negative Killing energy, which is kinematically accessible only via ultra-relativistic ejection deep within the ergosphere. We find that successful extraction with escape is statistically rare ($\sim$1% in broad parameter scans) and is governed by strict thresholds: it requires high black hole spin (empirically $a/M \gtrsim 0.89$) and ultra-relativistic exhaust velocity (onset at $v_e \approx 0.91c$). When conditions are highly tuned to a specific "sweet spot," success rates can reach 88.5%, representing a narrow extraction window rather than generic behavior. Furthermore, single-impulse thrust at periapsis achieves significantly higher cumulative efficiency ($η_{\rm cum} \approx 19\%$) compared to continuous thrust ($\sim$2--4%) due to path-averaging penalties. These constraints quantify the extreme fine-tuning required for material-based Penrose extraction, consistent with the astrophysical dominance of electromagnetic mechanisms. Simulation code is available at https://github.com/anindex/penrose_process.

</details>


### [60] [Fading Echoes of Interaction: Probing Centuries of Mass-Loss in Four Old Type IIn Supernovae](https://arxiv.org/abs/2601.19891)
*Elizabeth Hillenkamp,Raphael Baer-Way,Poonam Chandra,Arkaprabha Sarangi,Roger Chevalier,Nayana A. J.,Annika Deutsch,Keiichi Maeda,Nathan Smith*

Main category: astro-ph.HE

TL;DR: The paper presents late-time X-ray and radio observations of four SNe IIn, revealing mass-loss rates significantly lower than previous optical estimates, indicating rapidly evolving progenitor mass-loss processes in the centuries before core-collapse.


<details>
  <summary>Details</summary>
Motivation: To better understand the progenitors of SNe IIn and their late-stage mass-loss processes, as direct constraints are scarce. X-ray and radio observations can provide insights into mass-loss at different times pre-supernova when optical data is insufficient.

Method: Analyzing late-time (≥~3000 days post-explosion) X-ray and radio data from the Chandra and VLA telescopes for four SNe IIn (2013L, 2014ab, 2015da, KISS15s). Calculated mass-loss rates using emission models, compared with earlier optical-based estimates.

Result: Mass-loss rates from X-ray/radio are 1-3 orders of magnitude lower than optical estimates. KISS15s shows possible spectral inversion suggesting secondary shocks from binary/pulsar mechanisms. All objects show declining mass-loss rates over centuries, implying evolving progenitor mass-loss.

Conclusion: Late-stage progenitors underwent rapid mass-loss evolution in the centuries before explosion. X-ray/radio observations provide crucial insights into progenitor environments when optical faded, challenging prior mass-loss assumptions and highlighting the need for multi-wavelength studies.

Abstract: Supernovae characterized by enduring narrow optical hydrogen emission lines (SNe IIn) are believed to result primarily from the core-collapse of massive stars undergoing sustained interaction with a dense circumstellar medium (CSM). While the properties of SN IIn progenitors have relatively few direct constraints, the ongoing ejecta-CSM interaction provides unique information about late-stage stellar mass-loss preceding core-collapse. We present late-time X-ray and radio observations of four $\geq$3000 day-old SNe IIn: SN 2013L, SN 2014ab, SN 2015da, and KISS15s. The radio and X-ray emission from KISS15s indicate a mass-loss rate of $\rm{\dot M\sim4\times 10^{-3}~{M_{\odot}\,yr^{-1}}}$ at $\sim$450 years pre-supernova -- 2 orders of magnitude below earlier optical estimates (which probed the mass-loss immediately preceding the supernova). We find hints of a spectral inversion in the radio SED of KISS15s; a possible signature of a secondary shock due to a binary system or the emergence of a pulsar wind. For SN 2013L, we obtain a mass-loss rate of $\rm{\dot M\sim2 \times 10^{-3}~\rm{M_{\odot}\,yr^{-1}}}$ at $\sim$400 years pre-explosion based on the X-ray detection. For SN 2014ab and SN 2015da, we find a upper limits on the mass-loss rates of $\rm{\dot M<2\times10^{-3}~M_{\odot}\,yr^{-1}}$ explosion at $\sim$ 250 and 300 years pre-explosion, respectively. All four objects display mass-loss rates lower than estimates from earlier optical analyses by at least 1-3 orders of magnitude, necessitating a rapidly evolving progenitor process over the last centuries pre-explosion. Our analysis reveals how X-ray and radio observations can elucidate progenitor evolution when these objects have faded at optical wavelengths.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [61] [Beyond FINDCHIRP: Breaking the memory wall and optimal FFTs for Gravitational-Wave Matched-Filter Searches with Ratio-Filter Dechirping](https://arxiv.org/abs/2601.18835)
*Alexander H. Nitz,Keisi Kacanja,Kanchan Soni*

Main category: astro-ph.IM

TL;DR: The paper introduces Ratio-Filter Dechirping to address memory bandwidth limitations in FFT-based gravitational wave searches, replacing memory-intensive FFTs with cache-efficient FIR convolutions for significant speedups.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome memory bandwidth bottlenecks in current FFT-based methods like FINDCHIRP, which stall processors when templates exceed cache capacities, limiting search capabilities.

Method: The method involves using a reference template to eliminate common phase evolution, creating slowly varying frequency ratios implementable as short FIR filters, transforming the process into a compute-bound FIR convolution.

Result: Measured 8× speedup in offline searches and projected >10× for low-latency analysis, with applicability to searches involving eccentricity, precession, and other effects.

Conclusion: This approach reduces computational costs, enabling expanded searches in dense/high-dimensional parameter spaces and facilitating GPU acceleration.

Abstract: A primary bottleneck in modern FFT-based matched-filter searches for gravitational waves from compact binary coalescences is not raw processor throughput, but available memory bandwidth. Standard frequency-domain implementations, such as the FINDCHIRP algorithm, rely on streaming long template waveforms and data from main memory, which leads to significant processor stalling when template durations exceed cache capacities. In this work, we introduce \textit{Ratio-Filter Dechirping} as a solution, an algorithmic restructuring of the matched filter that transforms the operation from a memory-bound Fast Fourier Transform (FFT) into a cache-efficient, compute-bound Finite Impulse Response (FIR) convolution. By utilizing a reference template to remove common orbital phase evolution, we produce slowly changing frequency-domain ratios that can be accurately implemented as short FIR filters. This method delivers a measured speedup of $8\times$ for the core filtering loop used in offline searches and should enable $>10\times$ for low-latency analysis. We find that this approach generalizes to a variety of searches that include physical features such as finite size effects, eccentricity, and precession. By dramatically reducing the computational cost of matched filtering, this approach enables the expansion of searches into dense or high-dimensional parameter spaces, such as those for eccentric or subsolar-mass signals, that are already limited by available computing budgets. Furthermore, this framework provides a natural path for hardware acceleration on GPU architectures.

</details>


### [62] [Towards a Comprehensive Understanding of Planetary Systems through Population-Level, Large-Scale Surveys](https://arxiv.org/abs/2601.18841)
*Francisco J. Pozuelos,Pedro J. Amado,Jesús Aceituno,Marina Centenera-Merino,Stefan Cikota,Javier Flores,Julius Göhring,Sergio León-Saval,Kalaga Madhav,Giuseppe Morello,Abani Nayak,Jose L. Ortiz,David Pérez-Medialdea,María Isabel Ruiz-López,Miguel A. Sánchez-Carrasco,Alejandro Sánchez-López*

Main category: astro-ph.IM

TL;DR: The paper addresses the challenge of characterizing exoplanet masses, structures, and atmospheres at scale, proposing photonics-enabled modular telescopes to enable large-scale, high-precision surveys in the 2040s.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of current exoplanet research where most planets remain physically uncharacterized, hindering population-level analysis and understanding of planet formation/evolution.

Method: Proposes photonics-based modular telescope architectures to achieve scalable surveys with required survey speed, stability, and precision for mass/atmosphere studies across large planetary samples.

Result: Such telescopes would enable statistically significant, physically detailed characterizations, allowing robust population trends and a comprehensive understanding of planetary systems.

Conclusion: A new observational strategy using advanced photonics tech is critical for next-decadal missions to fully leverage upcoming exoplanet detections and advance planetary science.

Abstract: Over the past three decades, exoplanet research has delivered an extensive census of planets spanning a wide range of masses, sizes, and orbital configurations. Despite this progress, the physical interpretation of these populations remains severely limited, as precise constraints on planetary masses, interior structures, and atmospheres are available only for a small, highly selected subset of targets. As a result, most known exoplanets remain physically ambiguous, preventing the construction of robust population-level trends and limiting our understanding of planet formation, evolution, and habitability.
  In the coming decades, missions such as PLATO, Earth 2.0, and the Nancy Grace Roman Space Telescope will dramatically expand the number of exoplanets detected. However, without a corresponding capability to characterise planetary masses and atmospheres at scale, these discoveries will remain largely detection-driven. Current and planned facilities, including JWST and ELT-class instruments, excel at detailed studies of individual systems but are intrinsically unsuited for large, homogeneous surveys.
  This white paper identifies population-level physical characterisation as a fundamental science challenge for the 2040s and motivates the need for a new observational paradigm. We outline how photonics-enabled, modular telescope architectures can deliver the survey speed, stability, and scalability required to jointly probe planetary interiors and atmospheres across statistically meaningful samples, thereby enabling a comprehensive and physically grounded understanding of planetary systems.

</details>


### [63] [Uncertainties in Low-Count STIS Spectra](https://arxiv.org/abs/2601.18910)
*Joshua D. Lothringer,Leonardo dos Santos,Joleen Carlberg,Sean Lockwood,Jacqueline Brown*

Main category: astro-ph.IM

TL;DR: The paper addresses the breakdown of the 'root-N' approximation in low-count UV observations using MAMA detectors and presents updates to CalCOS and STIS pipelines, along with a new utility for Poisson uncertainty calculation and a bug fix in stistools.inttag.


<details>
  <summary>Details</summary>
Motivation: The root-N approximation fails in low-count regimes (0-1 counts per pixel) due to Poisson statistics, leading to inaccurate uncertainties. This is critical for UV-dim sources observed with MAMA detectors which have low dark rate and no read noise.

Method: Assessment of STIS observations affected by low-count uncertainties, development of a Jupyter notebook to explore the issue, creation of stistools.poisson_err for manual Poisson confidence interval calculation, and fixing a bug in stistools.inttag related to Poisson interval usage.

Result: Demonstrated improved uncertainty calculations through Poisson methods, resolved the software bug, and provided tools for manual Poisson error analysis in 1D STIS spectra.

Conclusion: Correctly applying Poisson confidence intervals is essential for accurate error estimation in low-count astronomical data. The provided tools and fixes enhance data analysis reliability, highlighting the importance of statistical rigor in low-signal regimes.

Abstract: We evaluate uncertainty calculations in the calstis pipeline for data in the low-count regime. Due to the low dark rate and read-noise free nature of MAMA detectors, observations of UV-dim sources can result in exposures with 0 or 1 counts in some pixels. In this regime, the "root-N" approximation widely used to calculate uncertainties breaks down, and one must compute Poisson confidence intervals for more accurate uncertainty calculations. The CalCOS pipeline was updated in 2020 to account for these low-count uncertainties. Here, we assess how STIS observations are currently affected by this phenomenon, describe a new Jupyter notebook exploring the issue, and introduce a new utility, stistools.poisson_err, to manually calculate Poisson confidence intervals for 1D STIS spectra. Additionally, we describe a related software bug in the stistools.inttag utility, which splits TIME-TAG data into sub-exposures. This newly fixed bug serves as a useful case-study for the proper use of Poisson confidence intervals.

</details>


### [64] [Development of Electroformed X-ray Optics Bridging Synchrotron Technology and Space Astronomy](https://arxiv.org/abs/2601.19188)
*Ryuto Fujii,Koki Sakuta,Kazuki Ampuku,Yusuke Yoshida,Makoto Yoshihara,Ayumu Takigawa,Keitoku Yoshihira,Tetsuo Kano,Naoki Ishida,Noriyuki Narukage,Keisuke Tamura,Kikuko Miyata,Gota Yamaguchi,Hidekazu Takano,Yoshiki Kohmura,Shutaro Mohri,Takehiro Kume,Yusuke Matsuzawa,Yoichi Imamura,Takahiro Saito,Kentaro Hiraguri,Hirokazu Hashizume,Hidekazu Mimura,Ikuyuki Mitsuishi*

Main category: astro-ph.IM

TL;DR: The paper details the development and testing of a 60mm diameter X-ray telescope mirror using an electroforming technique, achieving an angular resolution of 0.7 arcsec FWHM. Tests used a special system (HBX-KLAEES) at SPring-8, showing correlation between figure errors and image quality. The mirror was selected for the FOXSI-4 rocket mission and suggests potential for CubeSat applications.


<details>
  <summary>Details</summary>
Motivation: To advance X-ray telescope technology by creating high-resolution, ultra-short focal-length mirrors capable of capturing sharp images from celestial sources, with applications for future small satellite missions.

Method: Fabrication of a monolithic electroformed nickel mirror and Mirror Module Assembly (MMA) using established electroforming replication. Testing involved the HBX-KLAEES system at SPring-8 to evaluate imaging performance via PSF analysis, measuring core sharpness and angular components.

Result: Achieved 0.7 arcsec FWHM and 14 arcsec HPD resolution. Identified axial figure errors in primary/secondary mirrors impacting resolution. Success in FOXSI-4 sounding rocket experiment validates design suitability for space applications.

Conclusion: Demonstrates feasibility of high-resolution X-ray optics for compact platforms like CubeSats. Indicates pathways for further resolution improvements through error reduction, supporting future solar flare observation missions.

Abstract: We have developed X-ray telescope mirrors using an original electroforming replication technique established through the fabrication of millimeter-aperture, ultra-short-focal-length nanofocusing mirrors for synchrotron X-ray microscopy. This paper presents detailed results of X-ray illumination tests of a 60-mm-diameter, full-circumference, double-reflection monolithic electroformed nickel mirror and its Mirror Module Assembly (MMA). The experiments were conducted at the 1-km beamline BL29XUL at SPring-8. To simulate a parallel X-ray beam from celestial sources, we constructed a dedicated evaluation system, the High-Brilliance X-ray Kilometer-long Large-Area Expanded-beam Evaluation System (HBX-KLAEES). Owing to the high photon flux and the quasi-point-like source with a small divergence provided by HBX-KLAEES, the imaging performance was evaluated with high fidelity, resolving both the sharp core and large-angle components of the Point Spread Function (PSF). The results show an extremely sharp core with a Full Width at Half Maximum (FWHM) of 0.7 arcsec and a Half Power Diameter (HPD) of 14 arcsec, even after integration into the MMA. In addition, a positive correlation was found between angular resolution and axial figure error in both the primary and secondary mirror sections, indicating that axial figure errors contribute to image degradation. Based on these results, the MMA was selected as one of the hard X-ray optics for the FOXSI-4 sounding rocket experiment, which performs high-resolution soft and hard X-ray imaging spectroscopy of solar flares and was successfully launched. These results demonstrate the potential for further improvements in angular resolution and the development of high-resolution, ultra-short focal length X-ray optics for small satellites, including CubeSats.

</details>


### [65] [Gaia serial CTI modelling and radiation damage study](https://arxiv.org/abs/2601.19353)
*C. Pagani,N. C. Hambly,M. Davidson,N. Rowell,C. Crowley,R. Collins,F. van Leeuwen,G. M. Seabroke,A. Holland,M. A. Barstow,D. W. Evans*

Main category: astro-ph.IM

TL;DR: The study examines the impact of charge transfer inefficiency (CTI) in Gaia's CCDs due to radiation damage, develops a pixel-based model (CtiPixel), and demonstrates its effectiveness in capturing CTI's temporal evolution, including effects from solar events and a post-mission annealing experiment.


<details>
  <summary>Details</summary>
Motivation: To accurately model and mitigate the distortion caused by CTI in Gaia's data, ensuring precise measurements of stellar positions and fluxes.

Method: Developed the CtiPixel model using serial CTI diagnostic data taken every 3-4 months. Analyzed the model's performance against observed data and tracked CTI evolution, noting linear trends and abrupt changes from solar flares/coronal mass ejections. Included analysis of an engineering annealing experiment's impact.

Result: CtiPixel effectively represents observed CTI signatures, revealing insights into CCD defects caused by space radiation. Serial CTI showed gradual linear growth with sudden jumps post-solar events and a significant increase post-annealing.

Conclusion: The model enables improved data correction for Gaia's mission, highlights the importance of monitoring radiation effects on CCDs, and underscores the need for adaptive calibration strategies in long-term space missions.

Abstract: During the course of its mission, ESA's Gaia spacecraft has generated a map of the stars of the Galaxy of exquisite detail. While in its L2 orbit, the satellite has been exposed to high energy cosmic rays and solar particles, that caused permanent damage to its CCDs. The main effect of radiation damage on Gaia data is the distortion of its images and spectra, caused by the CCDs charge transfer inefficiency (CTI) during the readout process, that, if not taken into account, can result in inaccurate measurements of a star's location and flux. In this work, the impact of CTI in the serial readout direction, larger than in the parallel due to the presence of CCDs manufacturing defects, has been analysed and modelled. A pixel-based, physically motivated CTI model, CtiPixel, has been developed to characterise the damage in Gaia CCDs. The model has been calibrated using dedicated serial CTI diagnostic data, taken every 3-4 months over the course of the mission. The model is shown to be a good representation of the observed signatures of CTI in the calibration datasets, and its parameters reveal significant insights into the nature of the CCD defects generated by space irradiation. The evolution of the damage in the serial direction shows a general small linear increase over time, with sudden step changes after strong solar flares and coronal mass ejections directed towards Earth. The serial CTI showed a further step increase as a consequence of the engineering CCD annealing experiment carried out after the completion of Gaia science observations.

</details>


### [66] [Silicon-based vacuum window for millimeter and submillimeter-wave astrophysics](https://arxiv.org/abs/2601.19627)
*Ryota Takaku,Scott Cray,Kosuke Aizawa,Akira Endo,Shaul Hanany,Kenichi Karatsu,Jürgen Koch,Kuniaki Konishi,Tomotake Matsumura,Haruyuki Sakurai*

Main category: astro-ph.IM

TL;DR: The paper presents a silicon-based vacuum window for millimeter-wave astrophysics, demonstrating high transmittance (99%), low reflectance (1%), and wide bandwidth (67%). It uses sub-wavelength structures for anti-reflection and was successfully integrated into the DESHIMA v2.0 instrument.


<details>
  <summary>Details</summary>
Motivation: To develop a highly efficient millimeter-wave vacuum window with low loss and wide bandwidth for advanced astrophysical observations, addressing the need for reliable optical components in telescopes like the Atacama Submillimeter Telescope Experiment.

Method: The team designed and fabricated a 124 mm diameter silicon window with a 68 mm active area and 4 mm thickness. They used laser ablation to create sub-wavelength structures (SWS) for anti-reflection coating. Performance was characterized via transmittance/reflectance measurements and modeled using the measured SWS geometries.

Result: The window achieved 99% average transmittance, 1% reflectance, 67% fractional bandwidth, with absorptive loss undetectable. Experimental results align closely with theoretical models.

Conclusion: The developed window meets stringent astrophysical requirements, validated both experimentally and through simulation. Its integration into DESHIMA v2.0 confirms its reliability for long-term observational campaigns on submillimeter telescopes.

Abstract: We designed, fabricated, and characterized the properties of a silicon-based vacuum window suitable for millimeter-wave astrophysical applications. The window, which has a diameter of 124 mm, optically active diameter of 68 mm, and thickness of about 4 mm, gives an average transmittance and reflectance of 99% and 1%, respectively, a fractional bandwidth of 67%. Absorptive loss is below the detection limit of our measurement. The anti-reflection coating is made with laser ablated sub-wavelength structures (SWS), and the measured transmittance and reflectance values agree with modeling based on the measured SWS shapes. The window has been integrated into DESHIMA v2.0, an astrophysics instrument that took year-long observations with the Atacama Submillimeter Telescope Experiment.

</details>


### [67] [Accelerating radio astronomy imaging with RICK: a step towards SKA-Mid and SKA-Low](https://arxiv.org/abs/2601.19714)
*Giovanni Lacopo,Emanuele De Rubeis,Claudio Gheller,Giuliano Taffoni,Luca Tornatore*

Main category: astro-ph.IM

TL;DR: RICK 2.0 introduces a GPU-optimized imaging pipeline using HeFFTe library for distributed FFTs, achieving significant performance improvements and scalability for large-scale radio interferometry data.


<details>
  <summary>Details</summary>
Motivation: To address computational challenges from massive data volumes in modern radio interferometers like SKA precursors, needing software that's portable, high-performing, and scalable across diverse HPC architectures.

Method: RICK 2.0 leverages HeFFTe library for distributed FFTs, optimizing for GPUs and reducing communication overheads by avoiding full grid communication. Validated using real data from MeerKAT and LOFAR.

Result: Achieved substantial performance gains on GPUs, effective scaling with large pixel resolutions and frequency planes, reducing communication overhead from 96% to manageable levels.

Conclusion: The new architecture overcomes prior scaling limitations, offering a scalable imaging solution ready for Square Kilometre Array (SKA) era demands.

Abstract: The data volumes generated by modern radio interferometers, such as the SKA precursors, present significant computational challenges for imaging pipelines. Addressing the need for high-performance, portable, and scalable software, we present RICK 2.0 (Radio Imaging Code Kernels). This work introduces a novel implementation that leverages the HeFFTe library for distributed Fast Fourier Transforms, ensuring portability across diverse HPC architectures, including multi-core CPUs and accelerators. We validate RICK's correctness and performance against real observational data from both MeerKAT and LOFAR. Our results demonstrate that the HeFFTe-based implementation offers substantial performance advantages, particularly when running on GPUs, and scales effectively with large pixel resolutions and a high number of frequency planes. This new architecture overcomes the critical scaling limitations identified in previous work (Paper II, Paper III), where communication overheads consumed up to 96% of the runtime due to the necessity of communicating the entire grid. This new RICK version drastically reduces this communication impact, representing a scalable and efficient imaging solution ready for the SKA era.

</details>
