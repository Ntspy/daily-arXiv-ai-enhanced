<div id=toc></div>

# Table of Contents

- [astro-ph.IM](#astro-ph.IM) [Total: 11]
- [hep-ph](#hep-ph) [Total: 20]
- [gr-qc](#gr-qc) [Total: 17]
- [astro-ph.HE](#astro-ph.HE) [Total: 12]


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [1] [A Unified Analytic Framework for Microlensing Caustics: Geode Solutions and Hyper--Catalan Signatures](https://arxiv.org/abs/2511.15756)
*Gleb Berloff,Natalia G. Berloff*

Main category: astro-ph.IM

TL;DR: The paper presents a preparation-invariant analytic framework for describing image formation near microlensing caustics. It introduces a method using Weierstrass preparation, Hyper-Catalan recurrences, and an HC predictor-corrector algorithm, alongside metrics like HC signature and spectrum, to accurately model and predict lensing phenomena with high precision and continuity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide a rigorous mathematical framework that can handle the complexities of image formation near caustics in microlensing scenarios, which are critical for accurate photometric and astrometric modeling. Existing methods may struggle with the analytic description near singularities, stiffness, and ensuring continuity during parameter continuation.

Method: The method involves applying the Weierstrass preparation theorem locally to reduce the lens mapping to a single geode variable, deriving Hyper-Catalan recurrences for coefficients of the series solution, and implementing a predictor-corrector algorithm based on these recurrences. It defines HC signature (from kernel coefficients) and spectrum (analyticity properties) to assess solution behavior.

Result: The results include the creation of a unified template (single-series with metadata) that accurately models microlensing systems, achieving machine precision with HC iterations and few Newton steps. The method was validated on various lens configurations, demonstrating robust handling of stiffness and continuity.

Conclusion: The conclusion is that the proposed framework effectively addresses challenges in microlensing caustic analysis, enabling precise and continuous modeling required for advanced astrophysical observations. It provides tools for determining safe evaluation domains and quantifying system properties through the HC signature and spectrum.

Abstract: We give a preparation-invariant analytic description of image formation near microlensing caustics. After a local Weierstrass preparation at any multiple image (order $d\ge2$), the lens mapping reduces to a single geode variable $m$ satisfying $m=U\,\varphi(m)$, where $U$ is a prepared source coordinate and $\varphi$ is an image-side kernel. The coefficients of $m(U)$ obey closed Hyper-Catalan (HC) recurrences, allowing termwise derivatives and truncation control from the characteristic system. We also use the same form for a short HC predictor-corrector: evaluate the series within its certified radius and apply a brief Newton polish near the boundary. We define an HC signature (first nonzero kernel coefficients) and an HC spectrum (branch points and analyticity radius $ρ_U$), which quantify sparsity, stiffness, and safe evaluation domains. The construction covers folds and cusps of any global degree. On a binary fold and cusp, an artificial decic with a resonant unit, and two triple-lens cusps of a challenging geometry, HC seeds plus a few Newton steps recover the exact images to machine precision within the certified domain and maintain continuity under continuation. The resulting single-series templates (with $(\mathrm{Sig}_R,ρ_U)$ metadata) are ready for photometric and astrometric modeling.

</details>


### [2] [Decoding the Radio Sky: Bayesian Ensemble Learning and SVD-Based Feature Extraction for Automated Radio Galaxy Classification](https://arxiv.org/abs/2511.15788)
*Theophilus Ansah-Narh,Jordan Lontsi Tedongmo,Joseph Bremang Tandoh,Nia Imara,Ezekiel Nii Noye Nortey*

Main category: astro-ph.IM

TL;DR: The paper introduces a probabilistic ML framework using SVD and Bayesian ensembles for accurate radio galaxy classification, achieving 99% accuracy and providing uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Traditional manual classification methods are insufficient for handling large, heterogeneous radio survey datasets, necessitating an automated, scalable, and robust approach.

Method: The framework uses SVD for dimensionality reduction and Local Neighbourhood Encoding to address class imbalance. Bayesian ensembles combine Logistic Regression, SVM, LightGBM, and MLP models optimized via stacking, bagging, and boosting with Bayesian weighting.

Result: Bayesian stacking model achieved 99% accuracy and F1-score on Compact, Bent, FR-I, and FR-II sources. SHAP analysis identified key morphological features, and the framework offers uncertainty quantification.

Conclusion: The method sets a reproducible, interpretable standard for automated radio galaxy classification, critical for future data-intensive surveys.

Abstract: The classification of radio galaxies is central to understanding galaxy evolution, active galactic nuclei dynamics, and the large-scale structure of the universe. However, traditional manual techniques are inadequate for processing the massive, heterogeneous datasets generated by modern radio surveys. In this study, we present a probabilistic machine learning framework that integrates Singular Value Decomposition (SVD) for feature extraction with Bayesian ensemble learning to achieve robust, scalable radio galaxy classification. The SVD approach effectively reduces dimensionality while preserving key morphological structures, enabling efficient representation of galaxy features. To mitigate class imbalance and avoid the introduction of artefacts, we incorporate a Local Neighbourhood Encoding strategy tailored to the astrophysical distribution of galaxy types. The resulting features are used to train and optimize several baseline classifiers: Logistic Regression, Support Vector Machines, LightGBM, and Multi-Layer Perceptrons within bagging, boosting, and stacking ensembles governed by a Bayesian weighting scheme. Our results demonstrate that Bayesian ensembles outperform their traditional counterparts across all metrics, with the Bayesian stacking model achieving a classification accuracy of 99.0% and an F1-score of 0.99 across Compact, Bent, Fanaroff-Riley Type I (FR-I), and Type II (FR-II) sources. Interpretability is enhanced through SHAP analysis, which highlights the principal components most associated with morphological distinctions. Beyond improving classification performance, our framework facilitates uncertainty quantification, paving the way for more reliable integration into next-generation survey pipelines. This work contributes a reproducible and interpretable methodology for automated galaxy classification in the era of data-intensive radio astronomy.

</details>


### [3] [The SPHEREx Image and Spectrophotometry Processing Pipeline](https://arxiv.org/abs/2511.15823)
*Rachel Akeson,Gregory P. Dubois-Felsmann,Brendan P. Crill,Andreas L. Faisst,Tamim Fatahi,Candice M. Fazar,Tatiana Goldina,Daniel C. Masters,Christina Nelson,Roberta Paladini,Harry I. Teplitz,Gabriela Torrini,Phani Velicheti,Matthew L. N. Ashby,Dan Avner,Yoonsoo P. Bach,James J. Bock,Sean Bruton,Sean A. Bryan,Tzu-Ching Chang,Shuang-Shuang Chen,Ari J. Cukierman,O. Dore,C. Darren Dowell,Spencer Everett,Richard M. Feder,Zhaoyu Huai,Howard Hui,Woong-Seob Jeong,Young-Soo Jo,Phil M. Korngut,Yuna G. Kwon,Bomee Lee,Gary J. Melnick,Giulia Murgia,Chi H. Nguyen,Milad Pourrahmani,Zafar Rustamkulov,Volker Tolls,Pao-Yu Wang,Yujin Yang,Michael Zemcov*

Main category: astro-ph.IM

TL;DR: This paper details the SPHEREx mission's data processing pipeline, developed by the SPHEREx Science Data Center to provide calibrated spectral and photometric data products from four all-sky surveys.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create a robust pipeline capable of handling the vast data from SPHEREx's surveys, ensuring accurate flux and wavelength calibration for scientific analysis.

Method: The pipeline uses a modular framework with specific modules for processing images and spectrophotometry data at 6.15 arcsecond resolution across 102 spectral channels (0.75-5 microns).

Result: The pipeline successfully generates calibrated data products, now available via the NASA/IPAC Infrared Science Archive.

Conclusion: The pipeline meets mission requirements, enabling astronomers to access and analyze SPHEREx data for astrophysical studies.

Abstract: In this paper, we describe the SPHEREx image and spectrophotometry data processing pipeline, an infrastructure and software system designed to produce calibrated spectral images and photometric measurements for NASA's SPHEREx mission. SPHEREx is carrying out a series of four all-sky spectrophotometric surveys at 6.15 arcsecond resolution in 102 spectral channels spanning 0.75 to 5 microns. The pipeline which will deliver the flux- and wavelength-calibrated data products deriving from these surveys has been developed and is operated by the SPHEREx Science Data Center at Caltech/IPAC in collaboration with the SPHEREx Science Team. Here we describe the framework and modules used in the pipeline, along with the data products, which are available at the NASA/IPAC Infrared Science Archive.

</details>


### [4] [Optimization design and analysis for the mechanical test platform of scientific probe module of the Cool Planet Imaging Coronagraph](https://arxiv.org/abs/2511.16076)
*Lingyi Kong,Jiangpei Dou,Wei Guo,Mingming Xu,Shu Jiang,Bo Chen*

Main category: astro-ph.IM

TL;DR: The paper presents an optimized mechanical test platform design for the Cool Planet Imaging Coronagraph's probe module, using response surface methodology and vibration analysis to ensure structural integrity and meet frequency requirements.


<details>
  <summary>Details</summary>
Motivation: To design and analyze a mechanically robust test platform that meets stiffness and frequency specifications for space telescope components, while considering economic constraints.

Method: Structural optimization using Central Composite Design for parameter selection, third-order regression models for response surface analysis, modal analysis, and vibration simulations (sine, random, swept frequency).

Result: Achieved a fundamental frequency of 436.2Hz (>300Hz requirement), validated through experiments and simulations showing good agreement between response surface algorithm and test data.

Conclusion: The optimized platform meets design criteria, provides a methodological framework for future space station structural designs, and validates response surface techniques for vibration analysis in aerospace applications.

Abstract: This paper optimizes the design and analysis of the mechanical test platform for the scientific probe module of the Cool Planet Imaging Coronagraph, which is the fifth part of the China Space Station survey Telescope. First, according to the module layout and economic requirements, the preliminary structural design of the module mechanical test platform is carried out, and the stiffness sensitive parameters of the assembly are identified to determine the optimization parameters. The Central Composite Design method is used to design the test platform, and a third-order regression model is constructed for response surface analysis. The third-order response surface model of the fundamental frequency and amplitude of the test platform is obtained by fitting the test data with the least squares method, and the structure of the module mechanical test platform is determined. The modal analysis is carried out to determine the fundamental frequency and vibration modes of the mechanical test platform. The vibration response of the platform is simulated by sine, random and swept frequency vibration simulations. The response surface fitting algorithm is verified by the test platform swept frequency test. The agreement between the response surface fitting algorithm and the experiment is good. The fundamental frequency of the test platform is 436.2Hz (>300Hz), which meets the design index requirements of the test platform and can accurately guide the optimization design work. At the same time, it provides the theoretical basis and design method for the structural design of the Chinese manned space station.

</details>


### [5] [The Fluorescence Camera for the PBR mission](https://arxiv.org/abs/2511.16212)
*Francesco Saverio Cafagna*

Main category: astro-ph.IM

TL;DR: The POEMMA Balloon with Radio (PBR) instrument is a NASA SPB-borne telescope designed to detect cosmic rays via fluorescence and Cherenkov emissions, validating future space missions like POEMMA.


<details>
  <summary>Details</summary>
Motivation: The motivation is to validate the detection strategy for future space-based cosmic ray missions such as POEMMA using a cost-effective balloon platform.

Method: PBR uses a 1.1m Schmidt telescope with Fluorescence and Cherenkov cameras. The Fluorescence Camera employs 4 PDMs with MAPMTs and SPACIROC-3 ASICs for high-speed, high-resolution photon counting, enhanced by optical filters and lenses for optimal wavelength selection.

Result: Expected outcomes include validation of detection methods and demonstration of PBR's capability to detect cosmic rays with sub-orbital accuracy, supporting mission feasibility for future POEMMA.

Conclusion: PBR successfully combines advanced photon detection technology with a balloon platform to bridge ground-based and space-based cosmic ray observations, advancing multi-messenger astrophysics.

Abstract: The Probe Of Extreme Multi-Messenger Astrophysics (POEMMA) Balloon with Radio (PBR) is an instrument designed to be borne by a NASA suborbital Super Pressure Balloon (SPB), in a mission planned to last as long as 50 days. The PBR instrument consists of a 1.1 m aperture Schmidt telescope, similar to the POEMMA design, with two cameras in its hybrid focal surface: a Fluorescence Camera (FC) and a Cherenkov Camera (CC), both mounted on a frame that can be tilted to point from nadir up to 13 degrees above the horizon. The FC camera is designed to detect the fluorescence emission of Extensive Air Showers produced by Ultra-High Energy Cosmic Rays from sub-orbital altitudes. This measurement will validate the detection strategy for future space-based missions, such as POEMMA. The FC will be made of 4 Photo Detection Modules (PDMs), each consisting of a 6x6 matrix of 64-channel Multi Anode PhotoMulTipliers (MAPMT), for a grand total of 2304 pixels for each PDM. Custom-designed SPACIROC-3 ASICs perform single photoelectron counting on each pixel as well as charge integration on groups of 8 pixels to measure extremely bright or fast signals, reaching a double pulse resolution in the order of 10 ns for a 1 microsecond acquisition gate. A field flattener lens and a BG3 filter, to match the wavelength range of interest (300-400 nm), are mounted in front of the PDM. The camera will be able to detect showers in a field of view of 24x24 square degrees, with a pixel size on ground corresponding to 115 m. Details on the camera design and implementation will be given, along with the expected performance and the state of the construction.

</details>


### [6] [Accelerating Reionization Constraints: An ANN-Emulator Framework for the SCRIPT Semi-numerical Model](https://arxiv.org/abs/2511.16256)
*Saptarshi Sarkar,Tirthankar Roy Choudhury*

Main category: astro-ph.IM

TL;DR: The paper introduces an efficient emulator-based framework using a neural network to reduce computational costs for EoR parameter inference, decreasing the number of required simulations by 100x and CPU cost by 70x while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: The high computational cost of conventional MCMC methods hinders the constraining of the Epoch of Reionization (EoR) using physically motivated simulations. The need to handle upcoming large datasets (e.g., JWST) necessitates a more efficient inference method.

Method: The framework combines a coarse-resolution MCMC to identify high-likelihood regions and an adaptive sampling strategy for building a compact training set, which is used to train a neural network emulator of the model likelihood. This reduces the number of high-resolution simulations (≈1,000) needed while achieving high predictive accuracy (R² ≈ 0.97–0.99).

Result: The emulator reproduces posterior distributions accurately compared to full high-resolution runs, reducing simulation costs by ~100× and total CPU usage by ~70× while preserving statistical fidelity.

Conclusion: The method enables computationally feasible inference for higher-dimensional EoR models required for future datasets like JWST and 21 cm data. It provides a scalable strategy for next-generation EoR constraints using emulator-based approaches.

Abstract: Constraining the Epoch of Reionization (EoR) with physically motivated simulations is hampered by the high cost of conventional parameter inference. We present an efficient emulator-based framework that dramatically reduces this bottleneck for the photon-conserving semi-numerical code SCRIPT. Our approach combines (i) a reliable coarse-resolution MCMC to locate the high-likelihood region (exploiting the large-scale convergence of SCRIPT) with (ii) an adaptive, targeted sampling strategy to build a compact high-resolution training set for an artificial neural network based emulator of the model likelihood. With only $\approx 10^3$ high-resolution simulations, the trained emulators achieve excellent predictive accuracy ($R^2 \approx 0.97$--$0.99$) and, when embedded within an MCMC framework, reproduce posterior distributions from full high-resolution runs. Compared to conventional MCMC, our pipeline reduces the number of expensive simulations by a factor of $\sim 100$ and lowers total CPU cost by up to a factor of $\sim 70$, while retaining statistical fidelity. This computational speedup makes inference in much higher-dimensional models tractable (e.g., those needed to incorporate JWST and upcoming 21 cm datasets) and provides a general strategy for building efficient emulators for next generation of EoR constraints.

</details>


### [7] [WFC3/UVIS EPER CTE 2009-2025](https://arxiv.org/abs/2511.16391)
*Anne O'Connor,Harish Khandrika*

Main category: astro-ph.IM

TL;DR: The report analyzes the degradation of Charge Transfer Efficiency (CTE) in the WFC3/UVIS detector over 16 years using EPER data, finding a steeper decline in low-signal CTE, best modeled by quadratic/cubic fits, with residual oscillations showing periodicities of 8-9 years using Lomb-Scargle analysis.


<details>
  <summary>Details</summary>
Motivation: To monitor CTE degradation in WFC3/UVIS over time, improve predictive models, and understand residual patterns to enhance data calibration and mission planning.

Method: Analyzed EPER-derived CTE data (2009–2025) from Hubble's WFC3/UVIS detector. Evaluated linear, quadratic, and cubic fits for CTE decline. Applied Lomb-Scargle periodogram to residuals to identify periodic oscillations (~8-9 year cycles).

Result: CTE decline accelerates at lower signal levels (e.g., 0.0001/year at 160e-). Quadratic/cubic models outperform linear fits. Lomb-Scargle analysis reveals ~8-year periodicity in residuals vs linear fits, ~9-year vs quadratic/cubic fits.

Conclusion: CTE degradation requires non-linear models for accurate predictions. Periodic residual variations suggest external factors influencing charge transfer. Updated models and residual analysis inform ongoing calibration and observational strategies for Hubble data.

Abstract: In this report, we examine the behavior of Charge Transfer Efficiency (CTE) on the WFC3/UVIS detector over time as computed by the Extended Pixel Edge Response (EPER) technique, using internal calibration data acquired from 2009 through 2025. We find that the CTE has continued to decline as expected, with a steeper loss rate for lower signal levels. The lowest signal level (160e-) has continued to decline at a rate of 0.0001 per year, with a total overall decline of 0.0015. Analyses from 2016 and 2020 found that the rate of decline was not well fit by a linear function. This report verifies the rate of decline is instead better fit by a quadratic function (which results in the smallest min. and max. residuals, on average) or a cubic function (which has the best "goodness of fit" $χ^2$ and $R^2$ values). We continue to see periodic oscillations of the residuals around all three fit lines (linear, quadratic, and cubic) on which we perform a Lomb-Scargle periodogram analysis of the residuals. We find a periodicity of about 8 years for the residuals around the linear fit lines and about 9 years for the quadratic and cubic fit lines.

</details>


### [8] [WFC3/IR Geometric Distortion - Time Evolution of Linear Terms w.r.t. Gaia DR3](https://arxiv.org/abs/2511.16392)
*Anne O'Connor,Varun Bajaj*

Main category: astro-ph.IM

TL;DR: The study analyzed the geometric distortion of WFC3/IR relative to Gaia DR3 over the instrument's lifetime, finding increased uncertainty in rotation and skew terms between 2018-2021 linked to telescope jitter, but no linear trends in other parameters. Shift offsets were attributed to pointing inaccuracies.


<details>
  <summary>Details</summary>
Motivation: To assess the temporal stability of WFC3/IR geometric distortion alignment with Gaia DR3, ensuring accurate astrometric calibrations for high-precision observations.

Method: Examined MAST pipeline alignment solutions comparing WFC3/IR geometric distortion parameters (rotation, skew, scale, shift) to Gaia DR3 across 2009-2024. Analyzed temporal variations and correlations with telescope jitter periods.

Result: Found increased uncertainty in rotation and skew offsets from 2018-2021, coinciding with known jitter issues. No significant linear drift in rotation, skew, or scale. Shift offsets showed temporal evolution due to telescope pointing errors.

Conclusion: WFC3/IR distortion parameters remain stable except for shift offsets caused by external pointing inaccuracies. Observers should continue using tweakreg for precise astrometry despite no linear trends.

Abstract: We examine the relative offsets of the linear terms in the geometric distortion between WFC3/IR and the Gaia DR3 catalog using the Mikulski Archive for Space Telescopes (MAST) pipeline WFC3/IR to Gaia DR3 alignment solutions to assess temporal stability over the lifetime of the WFC3 instrument (2009-2024). We find a period of increased uncertainty and offsets in the rotation term between 2018 and 2021, as seen in a previous analysis of WFC3/UVIS linear geometric distortion (O'Connor et al., 2024), corresponding with a period of increased jitter. We find a similar pattern of increased uncertainty between 2018 and 2021 in the skew offsets to Gaia DR3, as well. We find no significant linear temporal evolution in the rotation, skew, or scale offsets between the WFC3/IR IDCTAB distortion solution and Gaia DR3 over the 16-year lifetime of the WFC3 instrument; however, we do see temporal evolution in the shift offsets (the difference -in pixels- between the IDCTAB and Gaia WCS positions), which are dominated by telescope pointing inaccuracy external to the WFC3/IR geometric distortion solution. For observers requiring high-precision astrometry, we continue to recommend that observers verify or improve image alignment using the tweakreg routine.

</details>


### [9] [How Single-Star Guiding affects HST's Pointing Stability](https://arxiv.org/abs/2511.16411)
*Jay Anderson,Sylvia Baggett*

Main category: astro-ph.IM

TL;DR: The Hubble Space Telescope (HST) can use a single guide star (1GS) with a reduced gyro mode (RGM) to maintain pointing, achieving acceptable drift levels for exposures under 500 seconds. Longer exposures show minimal drift (mostly under 0.2 pixels per orbit), with post-processing capable of correcting astrometric and photometric residuals.


<details>
  <summary>Details</summary>
Motivation: To evaluate the pointing accuracy of HST when using 1GS-RGM tracking mode, particularly during extended exposures, and assess its impact on scientific data quality compared to traditional 2GS mode.

Method: Analyzed archival data and test observations from HST's 1GS-RGM mode, quantifying drift over time and comparing PSF quality between 1GS and 2GS configurations. Tested post-processing techniques using perturbed PSF models to correct drift effects.

Result: For exposures <500 seconds, 1GS-RGM produces PSF quality indistinguishable from 2GS. Over full orbits (~2500 sec), drift was <0.2 pixels in four-fifths of cases, with up to 0.4 pixels in one case. Post-processing with perturbed PSF models effectively corrects astrometric and most photometric residuals.

Conclusion: 1GS-RGM is viable for most HST science programs, with drift impacts smaller than other instrumental effects. Observers can rely on this mode for longer exposures and use post-acquisition corrections for precision requirements.

Abstract: HST is designed to use two guide stars (GSs) in the fine-guidance sensors (FGSs) to maintain its pointing and tracking during exposures. The primary GS holds the boresight fixed and the secondary GS keeps the orientation fixed. However, HST is also able to track using only a single GS by fixing the boresite on one star and maintaining the orientation using the available gyro(s). We evaluate the pointing quality achieved in this latter case, when one GS and one gyro (RGM, a.k.a., reduced gyro mode) are used. We find that in 1GS-RGM, there is indeed more drift during the course of an orbit than when two guide stars are used, but the drift is much smaller than was seen in previous times of sub-optimal gyro performance. We quantify the 1GS-RGM drift seen in recent archival GO data and in images from a test calibration program and find that (a) for exposures < 500 sec, the PSF quality in 1GS is indistinguishable from that of 2GS and (b) over the course of full orbits (~2500 sec), the drift in four of five cases was less than 0.2 pixel and ~0.4 pix in the other case. Such a drift is marginally detectable in observations, but it should have a marginal impact on most science programs, since the variation in the PSF caused by drift is smaller than the PSF variations with focus and with location on the detector. For observers who wish to correct drift effects, we show that the use of a perturbed PSF during post-acquisition data analysis removes essentially all astrometric residuals, even for drift levels up to ~0.5 pix, as well as most of the photometric residuals.

</details>


### [10] [A Butterfly's Eye Camera for Intensity Interferometry with Cherenkov Telescopes](https://arxiv.org/abs/2511.16505)
*Juan Cortina,Alejo Cifuentes-Santos,Tarek Hassan,Fernando Frias*

Main category: astro-ph.IM

TL;DR: The paper presents the I3T concept, which improves the sensitivity and imaging capabilities of imaging atmospheric Cherenkov telescopes (IACTs) for optical intensity interferometry. By focusing primary mirror segments onto different camera pixels, the design addresses limitations such as poor optical quality and small focal ratios. This leads to enhanced sensitivity (4–6x), enables narrow-band filters and photon-counting photodetectors, and opens new observational opportunities, like imaging red giant surfaces and studying nova ejecta or fast-rotating stars.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome key IACT design constraints (poor optical quality, small focal ratios) that limit sensitivity improvements. Current upgrades have reached their limits, necessitating innovative solutions to achieve greater scientific impact through next-generation photodetectors and optical filters.

Method: The I3T concept (three implementations) involves segmenting IACT primary mirrors to focus onto different camera pixels ('Butterfly's Eye' configuration). This allows use of larger focal ratios with narrow-band filters and lower photon fluxes, enabling photon-counting detectors. The method was validated using realistic simulations targeting specific science cases.

Result: The I3T design enhances sensitivity by 4–6 times, introduces imaging capabilities at 2–40 milliarcseconds, and demonstrates transformative potential for imaging nova ejecta, measuring oblateness of fast-rotating stars, and studying circumstellar disks. Simulations validate its feasibility for these key science cases.

Conclusion: The I3T concept significantly advances IACT-based intensity interferometry, breaking previous sensitivity barriers and enabling new astronomical observations. It provides a practical path toward achieving unprecedented angular resolution and opens exciting research avenues in stellar astrophysics and transient studies.

Abstract: In recent years, imaging atmospheric Cherenkov telescopes (IACTs) have emerged as promising platforms for optical interferometry through the use of intensity interferometry. IACTs combine large segmented mirrors, photodetectors with nanosecond-scale time response capable of detecting signals from just a few photo-electrons, and array configurations with baselines of hundreds of meters. As a result, all major IACT facilities have now been upgraded to function also as optical intensity interferometers, achieving sensitivities an order of magnitude better than their predecessor, the Narrabri Stellar Intensity Interferometer. However, further improvements in sensitivity are currently limited by key IACT design constraints, namely the combination of poor optical quality and small focal ratios. Here we present three practical implementations of the "I3T concept", in which segments of the IACT primary mirror are focused onto different pixels of its camera. This approach yields several unexpected but significant advantages. Optics with larger focal ratios allow to integrate narrow-band optical filters, while lower photon fluxes enable to deploy next-generation photodetectors operating in photon-counting mode. We demonstrate that this so-called "Butterfly's Eye" configuration enhances the sensitivity of IACT-based intensity interferometers by a factor between 4 and 6. Moreover, as originally envisioned, the I3T design introduces imaging capabilities on angular scales from 2 to 40 milliarcseconds, unlocking new scientific opportunities such as direct surface imaging of nearby red giants. Besides, realistic simulations show that it can have a transformative impact on at least two key science cases: imaging the earliest stages of nova ejecta, and measuring the oblateness and circumstellar disks of fast-rotating stars.

</details>


### [11] [Bayesian polarization calibration and imaging in very long baseline interferometry](https://arxiv.org/abs/2511.16556)
*Jong-Seo Kim,Jakob Roth,Jongho Park,Jack D. Livingston,Philipp Arras,Torsten A. Enßlin,Michael Janssen,J. Anton Zensus,Andrei P. Lobanov*

Main category: astro-ph.IM

TL;DR: This paper introduces a Bayesian polarization calibration and imaging method for VLBI data, offering improved resolution and uncertainty quantification compared to traditional CLEAN-based methods. It was tested on quasars, producing physically consistent images and enabling automated processing for future radio arrays.


<details>
  <summary>Details</summary>
Motivation: Conventional VLBI methods struggle with suboptimal resolution, lack of uncertainty estimation, and require manual input. There's a need for an automated, high-fidelity approach to extract polarimetric data for studying celestial magnetic fields and synchrotron processes.

Method: Uses Bayesian imaging software (湾res湾le) to jointly model antenna gains, polarization leakages, and polarimetric images from pre-calibrated data. Applies constraints like flux/polarization positivity to ensure physically realistic results.

Result: Demonstrated on 3C273 (VLBA) and OJ287 (GMVA+ALMA), the method produces images with better structural complexity reconstruction, quantified uncertainties in calibration and Stokes images, and automation benefits over CLEAN.

Conclusion: The Bayesian approach improves VLBI polarimetry by addressing calibration uncertainties and manual intervention issues. It's ready for next-gen radio arrays via an open-source pipeline.

Abstract: Extracting polarimetric information from very long baseline interferometry (VLBI) data is demanding but vital for understanding the synchrotron radiation process and the magnetic fields of celestial objects, such as active galactic nuclei (AGNs). However, conventional CLEAN-based calibration and imaging methods provide suboptimal resolution without uncertainty estimation of calibration solutions, while requiring manual steering from an experienced user. We present a Bayesian polarization calibration and imaging method using Bayesian imaging software resolve for VLBI data sets, that explores the posterior distribution of antenna-based gains, polarization leakages, and polarimetric images jointly from pre-calibrated data. We demonstrate our calibration and imaging method with observations of the quasar 3C273 with the VLBA at 15 GHz and the blazar OJ287 with the GMVA+ALMA at 86 GHz. Compared to the CLEAN method, our approach provides physically realistic images that satisfy positivity of flux and polarization constraints and can reconstruct complex source structures composed of various spatial scales. Our method systematically accounts for calibration uncertainties in the final images and provides uncertainties of Stokes images and calibration solutions. The automated Bayesian approach for calibration and imaging will be able to obtain high-fidelity polarimetric images using high-quality data from next-generation radio arrays. The pipeline developed for this work is publicly available.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [12] [SURFing to the Fundamental Limit of Jet Tagging](https://arxiv.org/abs/2511.15779)
*Ian Pang,Darius A. Faroughy,David Shih,Ranit Das,Gregor Kasieczka*

Main category: hep-ph

TL;DR: This paper introduces the SURF method to validate generative models and assess the limits of jet tagging algorithms. It shows that EPiC-FM is a valid surrogate for JetClass jets, suggesting current taggers are near optimal, while GPT models overestimate separation between top and QCD jets.


<details>
  <summary>Details</summary>
Motivation: To determine the upper performance limits of jet tagging algorithms and validate the accuracy of generative surrogate models in representing real data distributions.

Method: The SURF framework trains a target generative model on samples from a tractable surrogate model (EPiC-FM) trained on real data, enabling exact Neyman-Pearson hypothesis tests to evaluate model validity.

Result: SURF validation confirms EPiC-FM as a valid surrogate for JetClass jets, indicating modern jet taggers approach true statistical limits. Autoregressive GPT models were found to unrealistically enhance top/QCD separation, providing misleading estimates of fundamental performance limits.

Conclusion: Generative model surrogates like EPiC-FM offer a reliable way to assess algorithmic limits, while models like GPT may produce overoptimistic results. This work provides a critical validation framework for understanding theoretical performance boundaries in particle physics analysis.

Abstract: Beyond the practical goal of improving search and measurement sensitivity through better jet tagging algorithms, there is a deeper question: what are their upper performance limits? Generative surrogate models with learned likelihood functions offer a new approach to this problem, provided the surrogate correctly captures the underlying data distribution. In this work, we introduce the SUrrogate ReFerence (SURF) method, a new approach to validating generative models. This framework enables exact Neyman-Pearson tests by training the target model on samples from another tractable surrogate, which is itself trained on real data. We argue that the EPiC-FM generative model is a valid surrogate reference for JetClass jets and apply SURF to show that modern jet taggers may already be operating close to the true statistical limit. By contrast, we find that autoregressive GPT models unphysically exaggerate top vs. QCD separation power encoded in the surrogate reference, implying that they are giving a misleading picture of the fundamental limit.

</details>


### [13] [The Heavy Dark Photon Handbook: Cosmological and Astrophysical Bounds](https://arxiv.org/abs/2511.15785)
*Andrea Caputo,Jaeyoung Park,Seokhoon Yun*

Main category: hep-ph

TL;DR: The paper explores constraints on dark photons through cosmological and astrophysical observations, focusing on thermal relic abundances and supernova production. It updates CMB, nucleosynthesis, and gamma-ray data constraints under minimal reheating assumptions, and analyzes supernova-related limits like fireball formation and positron injection.


<details>
  <summary>Details</summary>
Motivation: To constrain the properties of dark photons within a broad mass range (10^-1 to 10^3 MeV) using both cosmological data and astrophysical phenomena like supernovae. The study aims to provide conservative yet unavoidable bounds in the minimal dark photon model, particularly considering the minimal reheating temperature scenario.

Method: The authors derive constraints from multiple observables: CMB spectrum, primordial element abundances, and gamma-ray flux for thermal relics. For supernova-sourced dark photons, they re-examine traditional cooling arguments and introduce new bounds using fireball formation, low-energy supernova observations, and galactic positron measurements. They assume a minimal reheating temperature of 6 MeV to ensure conservative results.

Result: The analysis provides updated cosmological and astrophysical limits on dark photon parameters. These constraints are stronger under the minimal reheating temperature scenario and offer comprehensive bounds from both early universe and supernova production channels, including novel limits from fireball and positron data.

Conclusion: The study establishes robust observational constraints on dark photons across their mass range, emphasizing the necessity of considering multiple astrophysical processes. It underscores the significance of the minimal reheating temperature and highlights the role of supernova-related observations in tightening parameter spaces for dark photon models.

Abstract: We investigate cosmological and astrophysical constraints on dark photons with masses $\sim 10^{-1}$-$10^3$ MeV. These dark photons can be copiously produced either in the early universe or during core-collapse supernovae, potentially leaving distinct observational signatures. First, we derive updated constraints from cosmological and astrophysical observables that rely on the thermal relic abundance of dark photons, including the CMB spectrum, primordial light element abundances, and galactic/extragalactic gamma-ray flux. We consider the minimal reheating temperature possible, $T_{\rm RH} = 6 \, \rm MeV$, such that our constraints are conservative, but unavoidable within the minimal dark photon model. Then, for supernova-sourced dark photons, we systematically examine all relevant observational bounds, revisit the standard cooling argument and derive limits from other arguments such as fireball formation, low energy supernovae and galactic positron injection.

</details>


### [14] [How light can ALP dark matter be?](https://arxiv.org/abs/2511.15790)
*Kierthika Chathirathas,Thomas Schwetz*

Main category: hep-ph

TL;DR: The paper investigates axion-like particles (ALPs) as dark matter, deriving mass lower bounds by analyzing post- and pre-inflationary symmetry breaking scenarios. New constraints from isocurvature perturbations, black hole superradiance, and CMB tensor modes are considered, yielding mass limits between ~1e-18-5e-7 eV depending on parameters.


<details>
  <summary>Details</summary>
Motivation: To determine viable ALP dark matter models by establishing mass constraints, addressing uncertainties in production mechanisms (e.g., cosmic strings) and observational data (isocurvature, CMB tensors, superradiance).

Method: Contrast post- and pre-inflationary scenarios; update isocurvature bounds using cosmic string simulations; combine constraints from multiple observational probes (isocurvature, black hole superradiance, free-streaming, CMB tensor modes).

Result: Post-inflationary case allows weaker lower bounds (~6e-19 eV with temp-dependent mass) but CMB tensor non-observations impose stricter limits (>5e-7 eV for low reheating). Most cases require m_a >1e-17 eV. Pre-inflationary scenarios have different parameter dependencies.

Conclusion: ALP dark matter models must satisfy multi-faceted observational constraints. Strongest bounds come from CMB tensors in low reheating efficiency scenarios. Uncertainties in axion emission spectra and reheating remain key uncertainties.

Abstract: We assume axion-like particles (ALPs) to provide the full dark matter abundance and derive various lower bounds on the ALP mass. We contrast the post- and pre-inflationary symmetry breaking cases and present allowed regions in the plane of ALP mass and energy scale of inflation. For the post-inflationary case, we revisit bounds from isocurvature perturbations taking into account that, as suggested by simulations, axion radiation by cosmic strings during the scaling regime provides the dominant production mechanism of dark matter, obtaining significantly weaker limits than previously. Combining isocurvature, with constraints from black hole superradiance and free streaming, we find that the bound $m_a \gtrsim 10^{-17}$ eV applies for most cases considered here. It can be potentially relaxed to $\sim 6\times 10^{-19}$ eV only in the post-inflationary case with a strongly temperature-dependent axion mass, subject to uncertainties on the axion emission spectrum. Significantly stronger bounds are obtained in the post-inflationary scenario from the non-observation of CMB tensor modes, which can be as strong as $m_a > 5\times 10^{-7}$ eV for small reheating efficiencies, $ε\lesssim 5\times 10^{-4}$.

</details>


### [15] [Axiverse Baryogenesis](https://arxiv.org/abs/2511.15794)
*Pouya Asadi,David Cyncynates,Stefania Gori*

Main category: hep-ph

TL;DR: The QCD axion's role in explaining both baryon asymmetry and dark matter is addressed by incorporating the axiverse framework, which introduces multiple axion-like fields to resolve issues of underproduction of baryons and overproduction of dark matter in the minimal scenario.


<details>
  <summary>Details</summary>
Motivation: To resolve the tensions in the minimal QCD axion model where axiogenesis either underproduces baryons or overproduces dark matter and faces issues with kinetic misalignment and axion quality.

Method: Proposing a framework where the QCD axion is a linear combination of multiple axion-like fields (axiverse), enabling new dissipation channels to prevent dark matter overproduction and additional Peccei-Quinn symmetries to ensure quality axion. A toy model with two axions is analyzed.

Result: The approach successfully avoids the overclosure problem and maintains a high-quality QCD axion, predicting observable phenomena like dark matter detection signals, astrophysical signatures, and colliders.

Conclusion: The axiverse model provides a viable pathway to unify baryogenesis and dark matter via axiogenesis while addressing the shortcomings of the minimal QCD axion scenario, opening up testable experimental predictions.

Abstract: The QCD axion may offer a unified origin for the baryon asymmetry and dark matter through axiogenesis. However, in the minimal QCD axion scenario, axiogenesis either underproduces baryons or overproduces dark matter, and the required kinetic misalignment initial conditions are in tension with axion quality. In this \textit{Letter}, we demonstrate that the axiverse naturally resolves these tensions: the QCD axion emerges as a linear combination of multiple axion-like fields, evading the overclosure problem thanks to new dissipation channels , while introducing additional Peccei--Quinn symmetries that ensure a high quality QCD axion. We illustrate these points in a toy model with two axions. This framework predicts a rich phenomenology within experimental reach, including dark matter detection prospects, astrophysical signals, and collider signatures.

</details>


### [16] [Constraints on lepton-flavor mixing with third-generation new physics](https://arxiv.org/abs/2511.15800)
*Sebastiano Covone,Pol Morell,Arianna Tinari*

Main category: hep-ph

TL;DR: The paper examines the effects of an approximate U(2)^5 flavor symmetry at the TeV scale, focusing on the U(2)_ell subgroup breakage, constraining the spurion δ to |δ|<0.051 from LFV/LFU data. Future experiments could enhance constraints.


<details>
  <summary>Details</summary>
Motivation: To explore the implications of flavor symmetries on particle interactions, particularly third-generation fermions, and to derive constraints from current measurements.

Method: Analyzed lepton flavor violating (LFV) and lepton flavor universality (LFU) observables like R_{K^{(*)}} and B_s→μμ decay rates to constrain the spurion δ parameterizing U(2)_ell symmetry breaking.

Result: Derived |δ|<0.051 at 95% CL, updated LFV bounds, highlighted future prospects for tightening constraints via improved LFV experiments.

Conclusion: Current data strongly constrains the U(2)_ell symmetry breaking δ parameter, with future LFV searches promising further precision in understanding lepton flavor mixing.

Abstract: We study the implications of an approximate $U(2)^5$ flavor symmetry at the TeV scale, under the assumption of new physics predominantly coupled to the third-generation fermions, focusing on the breaking of the $U(2)_\ell$ subgroup governing the mixing between second- and third-generation left-handed leptons. We derive constraints on the corresponding spurion parameter $δ$ from current data on lepton flavor violating (LFV) and lepton flavor universality (LFU) observables, finding that $R_{K^{(*)}}$ and $\mathcal{B}(B_s \to μμ)$ give the most stringent bound on $δ$, yielding ${|δ|<0.051}$ at 95% CL. In addition, we provide updated bounds for LFV decay rates and discuss prospects for future sensitivity improvements, finding that future LFV searches could further tighten constraints on the mixing between second- and third-generation leptons.

</details>


### [17] [Structure Formation with Dark Magnetohydrodynamics](https://arxiv.org/abs/2511.15810)
*Pierce Giffin,Andrew Liu,Jeremias Boucsein,Akaxia Cruz,Anirudh Prabhu,Stefano Profumo,M. Grant Roberts*

Main category: hep-ph

TL;DR: This paper explores how long-range interactions in the dark sector, specifically within a secluded dark $U(1)_D$ model, can generate collective plasma effects that alter dark matter halo evolution via dark magnetic fields, leading to anisotropic pressure changes and suppression of small-scale power in the matter power spectrum. Future observations may constrain the dark charge-to-mass ratio within a specific range.


<details>
  <summary>Details</summary>
Motivation: To investigate whether dark magnetic fields from dark sector interactions modify cosmic structure formation, as current cosmological models may miss such plasma effects which could explain discrepancies in observed matter distributions.

Method: The authors use a magnetohydrodynamic approach to simulate gravitational collapse in a secluded dark $U(1)_D$ model, analyzing the impact of dark magnetic fields on the Jeans scale and matter power spectrum.

Result: Dark magnetic fields induce anisotropic pressure altering the Jeans scale and suppressing small-scale power directionally. Current data lacks constraints, but upcoming experiments (CMB-HD, HERA, EDGES) could test predictions for charge-to-mass ratios between $10^{-20}$ and $10^{-14} \,\text{GeV}^{-1}$.

Conclusion: Dark sector plasma effects like anisotropic pressure from magnetic fields offer a testable framework for modifying cosmic structure formation, with future high-resolution observations necessary to constrain model parameters.

Abstract: Long-range interactions in the dark sector can give rise to collective plasma phenomena that are capable of modifying the evolution of dark matter halos. We present the first study of gravitational collapse in a secluded dark $U(1)_D$ model using a magnetohydrodynamic description of the dark matter. We show that dark magnetic fields generate an anisotropic pressure that alters the Jeans scale and suppresses small-scale power in a direction-dependent manner. For a range of primordial magnetic spectral indices, this effect produces distinctive modifications to the linear matter power spectrum. We find that current observations cannot yet constrain viable dark magnetic fields, as CMB tensor modes mostly provide more stringent constraints. Nevertheless, forthcoming high-resolution probes of the matter power spectrum (CMB-HD lensing, HERA, and EDGES) will be able to test these predictions and are sensitive to dark charge-to-mass ratios in the range $10^{-20}\,\text{GeV}^{-1}\lesssim q_χ/m_χ\lesssim 10^{-14}\,\text{GeV}^{-1}$.

</details>


### [18] [Exploring enhanced non-resonant di-Higgs production at the HL-LHC with neural networks](https://arxiv.org/abs/2511.15897)
*Leandro Da Rold,Manuel Epele,Anibal D. Medina,Nicolás I. Mileo,Alejandro Szynkman*

Main category: hep-ph

TL;DR: The paper explores di-Higgs production in the bbarγγ final state at the LHC, focusing on scenarios where gluon fusion is enhanced by new colored scalars (squarks/leptoquarks). Using Monte Carlo simulations and deep neural networks, the study demonstrates improved discrimination by employing two classifiers (against QCD and Higgs backgrounds) and utilizing high-level features. With 3 ab⁻¹ luminosity, significances of 7.3 (BM_L) and 3.1 (BM_H) are achieved, with BM_L reaching discovery level at 1.7 ab⁻¹.


<details>
  <summary>Details</summary>
Motivation: To investigate di-Higgs production mechanisms involving new colored scalars, testing beyond Standard Model physics, and optimizing analysis techniques for better signal-background separation at the LHC.

Method: Monte Carlo simulations for signal (two benchmarks: BM_L at 464 GeV, BM_H at 621 GeV) and backgrounds. Used deep neural networks with varied architectures, comparing two classifiers (QCD vs. Higgs backgrounds) and high-level vs. low-level features (e.g., invariant masses, transverse momenta, angular separations).

Result: Two-classifier approach and high-level features significantly improve discrimination. BM_L achieves 7.3 significance at 3 ab⁻¹ (discovery at 1.7 ab⁻¹), whereas BM_H reaches only 3.1 significance.

Conclusion: The proposed methodology enhances sensitivity to colored scalar mediators in di-Higgs searches. While BM_L is discoverable, BM_H remains challenging, highlighting the importance of high-level features and optimized machine learning architectures in LHC analyses.

Abstract: We investigate di-Higgs production in the $b\bar{b}γγ$ final state at the LHC, focusing on scenarios where the gluon fusion process is enhanced by new colored scalars, which could be identified as squarks or leptoquarks. We consider two benchmarks characterized by the mass of the lightest colored scalar, BM$_{\mathrm{L}}$ and BM$_{\mathrm{H}}$, corresponding to 464 GeV and 621 GeV, respectively. Using Monte Carlo simulations for both the signal and the dominant backgrounds, we perform a discovery analysis with deep neural networks, exploring various architectures and input variables. Our results show that the discrimination power is maximized by employing two dedicated classifiers, one trained against QCD backgrounds and another against backgrounds involving single-Higgs processes. Furthermore, we demonstrate that including high-level features -- such as the invariant masses $m_{γγ}$, $m_{bb}$, and $m_{hh}$, as well as the transverse momenta and angular separations of the photon and $b$-jet pairs -- significantly improves the performance compared to using only low-level features as the invariant mass and momenta of the final particles. For the latter case, we find that architectures processing photon and $b$-jet variables separately can enhance the significance for BM$_{\mathrm{H}}$. Projecting for an integrated luminosity of 3 ab$^{-1}$, we obtain a significance of 7.3 for BM$_{\mathrm{L}}$, while it drops to 3.1 for BM$_{\mathrm{H}}$. In the particular case of BM$_{\mathrm{L}}$, discovery level significance can be reached at 1.7 ab$^{-1}$.

</details>


### [19] [Modified Tri-bimaximal neutrino mixing confront with JUNO $θ_{12}$ measurement](https://arxiv.org/abs/2511.15978)
*Xiao-Gang He*

Main category: hep-ph

TL;DR: The paper analyzes modifications to the tribimaximal mixing pattern in light of JUNO's precise reactor neutrino oscillation data, specifically focusing on scenarios where one column of the mixing matrix remains unchanged. The third column scenario is excluded due to non-zero θ₁₃, the second column fails to align with the new θ₁₂ measurement, while the first column scenario accommodates data within 1σ, predicting an inverted mass hierarchy, sin²θ₁₂ = 1−3sin²θ₁₃, and specific CP phase values.


<details>
  <summary>Details</summary>
Motivation: To investigate how minimal adjustments to the tribimaximal mixing pattern, historically significant for zero θ₁₃, can be reconciled with JUNO's precise θ₁₂ measurement and other neutrino oscillation data.

Method: The study examines three modification schemes where each column of the tribimaximal mixing matrix is kept intact. Calculations compare predicted mixing parameters (θ₁₂, θ₁₃, CP phases) against JUNO's sin²θ₁₂ = 0.3092 and global data.

Result: Option a (fixed third column) is ruled out due to θ₁₃ exclusions. Option b predicts Ve2=1/√3 which clashes with JUNO's smaller θ₁₂. Option c (first column fixed) fits data within 1σ, predicting sin²θ₁₂ = 1−3sin²θ₁₃ and prefers inverted mass hierarchy with CP phase sin δ ~0.84.

Conclusion: The first column-unmodified scenario remains viable, offering testable predictions for future experiments on CP violation and mass hierarchy.

Abstract: JUNO collaboration has realsed its first measurement of reactor neutrino oscillations results with $\sin^2θ_{12} = 0.3092\pm 0.0087$ which improved the precision by a factor of 1.6 relative to the combination of all previous measurements. We confront the minimally modifred tri-bimaximal mixing pattern with the new data. Before the measurement of a non-zero $θ_{13}$ mixing angle, the tri-bimximal mixing pattern predicting $θ_{13}$ to be zero is one of the most popular simple ones. Modifications have been proposed to keep some features of tri-bimaximal mixing and make it to be consistent with data. One minimal scheme is to keep one of the coloums unchanged by modifing the neutrino mass matrix in models based on $A_4$ flavor symmetry. Three mixing patterms ermergy, a) the third, b) the second and c) the first column in the $3\times 3$ mixing matrix separately unchaged. The option a) mantains the feature that $θ_{13}=0$ which is in conflect with data on $θ_{13}$ and therefore is ruled out. The option b) is the interesting mixing pattern with $θ_{13}$ nonzero, but predicts $V_{e2} = 1/\sqrt{3}$ with interesting predictions for CP violation and other mixing angles. The new $θ_{12}$ value measured by JUNO with $|V_{e2}|$ is, however, smaller than $1/\sqrt{3}$ at a more than 3.5$σ$ level making the search for alternative new paterns desirable. The option c) predicts $V_{e2} = cosα/\sqrt{3}$ which can also accommodate known data at a better than 1$σ$ level. This mixing pattern predicts $\sin^2θ_{12} = 1-3\sin^2θ_{13}$. The model favors inverted neutrino mass herarcy with CP violating variable $\sin θ-0.84$ . Near future experiments can test the model.

</details>


### [20] [Electromagnetic form factors: A window into the $DΛ_c$, $D^*Λ_c$, and $DΛ_c^*$ molecular structure](https://arxiv.org/abs/2511.16052)
*U. Özdem*

Main category: hep-ph

TL;DR: The paper calculates electromagnetic properties, including magnetic dipole and higher multipoles, of three molecular pentaquark states using QCD light-cone sum rules, revealing distinct structural features and providing benchmarks for future studies.


<details>
  <summary>Details</summary>
Motivation: To understand the internal structure of doubly-charmed pentaquarks, which are key to solving puzzles in exotic hadron physics.

Method: Used QCD light-conce sum rules to compute electromagnetic moments (magnetic dipole, electric quadrupole, magnetic octupole) for DΛ_c, D*Λ_c, and DΛ_c^* pentaquarks with specific spin parities.

Result: Found a hierarchy of magnetic moments (μ_DΛ_c* > μ_D*Λ_c > μ_DΛ_c), driven by light quark contributions and strategic charm quark contributions. Predicted higher multipoles indicating prolate/oblate charge distributions.

Conclusion: Results establish benchmarks for future studies and allow quantitative comparisons with other models to determine the nature of these exotic hadrons.

Abstract: The internal structure of exotic hadrons remains one of the most compelling puzzles in strong interaction physics. In this work, we provide crucial insights into the nature of doubly-charmed pentaquarks by investigating their electromagnetic properties. Using QCD light-cone sum rules, we present the first comprehensive calculation of the magnetic dipole moments of $DΛ_c$, $D^*Λ_c$, and $DΛ_c^*$ molecular pentaquarks with $J^P = \frac{1}{2}^-$, $\frac{3}{2}^-$, and $\frac{3}{2}^-$, respectively. Our analysis reveals a striking hierarchy of magnetic moments: $μ_{DΛ_c^*} > μ_{D^*Λ_c} > μ_{DΛ_c}$, driven by distinct quark-level mechanisms. While light quarks dominate the overall response, we find that charm quark contributions become strategically important when light quark contributions partially cancel. Beyond dipole moments, we predict higher multipoles -- electric quadrupole and magnetic octupole moments -- that fingerprint the spatial deformation of these states, revealing prolate versus oblate charge distributions. These results provide the first systematic predictions for electromagnetic moments of molecular pentaquark configurations, establishing essential benchmarks for future theoretical and experimental studies. The distinctive patterns we uncover will enable quantitative comparisons with alternative structural models, ultimately helping to resolve the nature of doubly-charmed exotic hadrons.

</details>


### [21] [Probing quark-lepton correlation in GUTs with high-precision neutrino measurements](https://arxiv.org/abs/2511.16196)
*Zi-Qiang Chen,Gao-Xiang Fang,Ye-Ling Zhou*

Main category: hep-ph

TL;DR: The paper explores quark-lepton correlations in SO(10) GUTs using JUNO data, predicting neutrino mass ordering, CP phase constraints, and implications for neutrinoless double beta decay. It also links these correlations to right-handed neutrino masses, Baryon/Lepton number violation scales, and baryogenesis, emphasizing their role in future neutrino experiments.


<details>
  <summary>Details</summary>
Motivation: To investigate how SO(10) Grand Unified Theories (GUTs) can be tested with new JUNO data by analyzing quark-lepton mass and mixing correlations, addressing predictions for neutrino properties and beyond Standard Model physics.

Method: Numerical scans of the flavor space within SO(10) GUTs using JUNO observational data to analyze quark-lepton correlations and their compatibility with experimental results.

Result: Preference for normal neutrino mass ordering, constrained CP-violating phases, classification of GUT models by their predictivity for neutrinoless double beta decay, and insights into right-handed neutrino mass spectra and early universe processes.

Conclusion: Enhanced precision in neutrino measurements will strengthen the role of quark-lepton correlations in validating GUT frameworks, complementing proton decay searches and offering a critical test of SO(10) model viability.

Abstract: GUTs unify quarks and leptons into same representations and predict correlations between their masses and mixing. We take new data of JUNO and perform numerical scans to explore the flavor space compatible with data in SO(10) GUTs. The quark-lepton correlation shows the preference of normal ordering for light neutrino masses, predicts favored region of the CP-violating phase in neutrino oscillations, and classifies GUT models based their testability in neutrinoless double beta decay experiments. The quark-lepton correlation predicts mass spectrum of right-handed neutrinos, pointing to the energy scale of baryon and lepton number violation and providing sources for baryogenesis. We emphasize that, as the high precision measurements of neutrino physics is coming, the quark-lepton correlation will provide increasingly important role in the testability of GUTs, complementary to the proton decay measurement.

</details>


### [22] [Modular TM$_1$ mixing in light of precision measurement in JUNO](https://arxiv.org/abs/2511.16348)
*Wen-Hao Jiang,Ruiwen Ouyang,Ye-Ling Zhou*

Main category: hep-ph

TL;DR: The paper explores modular S_4 symmetry-based models predicting TM₁ leptonic mixing, using JUNO data to constrain parameters. It presents three distinct models with varying CP phase and neutrinoless double beta decay predictions.


<details>
  <summary>Details</summary>
Motivation: To investigate modular S_4 symmetry models generating TM₁ mixing and assess their viability with updated experimental data, distinguishing between different CP and beta decay predictions.

Method: Analyzes symmetry-breaking mechanisms through vacuum alignment of modular and flavon fields. Constructs three models with same symmetry structure but different building strategies to predict varied CP phases and effective masses.

Result: Models produce distinguishable predictions for CP-violating phases and neutrinoless double beta decay masses, constrained by JUNO's θ₁₂ and Δm²₂₁ measurements.

Conclusion: Different model-building approaches under same symmetry yield testable predictions, enabling experimental discrimination between models through CP phase measurements and beta decay experiments.

Abstract: This paper investigates the landscape of models based on modular $S_4$ symmetry that predicts the trimaximal TM$_1$ mixing pattern for leptonic flavor mixing, and explores their parameter spaces with constraints from the latest high-precision measurement on $θ_{12}$ and $Δm^2_{21}$ given by JUNO experiment. We review on how the mixing pattern arises from residual symmetries after the spontaneous breaking of a flavor symmetry, via an appropriate vacuum alignment of modular fields and flavon fields. We show three different models that realize the TM$_1$ in three approaches with the same symmetry structure. Due to different model building strategies used, predictions on the CP-violating phase and the effective mass in neutrinoless double beta decay are different, making them distinguishable.

</details>


### [23] [Composite Asymmetric Dark Matter from Primordial Black Holes](https://arxiv.org/abs/2511.16354)
*Takumi Kuwahara,Yoshiki Uchida*

Main category: hep-ph

TL;DR: The paper explores a cogenesis scenario where dark matter originates from a dark sector with strong dynamics similar to QCD, with primordial black hole evaporation producing heavy scalars that decay asymmetrically to generate baryon and dark matter asymmetries, requiring specific mass ranges for scalars (1e6-1e9 GeV) and PBHs (1e7-1e9 g) to match observed densities.


<details>
  <summary>Details</summary>
Motivation: To explain the baryon asymmetry of the universe and dark matter abundance through a unified cogenesis mechanism involving primordial black hole evaporation and CP-violating decays.

Method: Proposes a composite asymmetric dark matter model where dark baryons from a QCD-like dark sector receive asymmetry via CP-violating decays of Hawking-radiated heavy scalars from PBHs evaporating post-electroweak phase transition but before BBN.

Result: The scenario successfully accounts for observed baryon and dark matter energy densities when heavy scalars are 1e6-1e9 GeV and PBH masses are 1e7-1e9 g, ensuring cosmological consistency.

Conclusion: Successful cogenesis of baryon and dark matter asymmetries via primordial black hole evaporation is viable within specific mass parameters, offering a unified framework for two cosmic puzzles.

Abstract: We investigate a cogenesis scenario for composite asymmetric dark matter framework: a dark sector has a similar strong dynamics to quantum chromodynamics in the standard model, and the dark-sector counterpart of baryons is the dark matter candidate. The Hawking evaporation of primordial black holes plays the role of a source of heavy scalar particles whose $CP$-violating decay into quarks and dark quarks provides particle--anti-particle asymmetries in baryons and dark matter, respectively. Primordial black holes should evaporate after the electroweak phase transition and before the big-bang nucleosynthesis for explaining the baryon asymmetry of the Universe and for consistent cosmology. We find that this scenario explains the observed values for both baryon and dark matter energy densities when the heavy scalar particles have a mass of $10^6 \text{--} 10^9\, \mathrm{GeV}$ and the primordial black holes have masses of $10^7 \text{--} 10^9\,\mathrm{g}$.

</details>


### [24] [High-frequency Gravitational Waves from Superstring Phases in the Early Universe](https://arxiv.org/abs/2511.16404)
*Joseph P. Conlon,Edmund J. Copeland,Edward Hardy,Noelia Sánchez González*

Main category: hep-ph

TL;DR: The paper explores gravitational wave signals from cosmic string loops with time-varying tension during moduli-dominated epochs, finding high-frequency spectra peaking in GHz today, with observability hinging on post-modulus epoch durations.


<details>
  <summary>Details</summary>
Motivation: To understand the gravitational wave signatures produced by cosmic string loops when moduli fields evolve in early universe scenarios, addressing how their time-varying tensions affect loop dynamics and gravitational wave emission.

Method: Analyzing the cosmological evolution of cosmic string loops during moduli rolling phases, calculating gravitational wave spectra considering energy densities and decay mechanisms when moduli reach their potential minima.

Result: The gravitational wave spectrum peaks at GHz frequencies today, with signal amplitudes diluted by subsequent matter-dominated eras, making observability contingent on the length of the moduli-dominated epoch post-modulus oscillation.

Conclusion: Such moduli-induced cosmic string loop trackers offer a unique probe of early universe moduli dynamics, with detectability possible under specific cosmological epoch duration conditions that future high-frequency gravitational wave experiments might explore.

Abstract: When moduli roll in the early universe, all physical scales - including string tensions - simultaneously evolve. The dynamics of cosmic string loops with time-varying tension can produce cosmic string loop trackers in which most of the energy density of the universe lies in the form of string loops. This solution can exist as an attractor until the rolling modulus reaches its minimum, when the loops ultimately decay through gravitational wave emission. We explore the spectrum of gravitational waves produced by such string loop trackers. The resulting spectrum is high-frequency and peaks in the GHz regime today. The amplitude of the signal is diluted by any subsequent matter-dominated epochs, and thus the potential observability of the signal crucially depends on the duration of the moduli-dominated epoch that follows once the moduli settle down and oscillate about their minimum.

</details>


### [25] [WIMP Freeze-out dynamics under Tsallis statistics](https://arxiv.org/abs/2511.16487)
*Matias P. Gonzalez,Roberto A. Lineros*

Main category: hep-ph

TL;DR: This paper generalizes the thermal WIMP freeze-out mechanism using Tsallis nonextensive statistics, modifying key thermodynamic quantities and the Boltzmann equation to include a nonextensive parameter q. The study shows that the freeze-out parameter increases with q and that QCD threshold effects influence relic density. Parameter scans reveal a degeneracy in q values but find best-fit q接近1 for realistic cross sections.


<details>
  <summary>Details</summary>
Motivation: To investigate how nonextensive statistics, modeled by Tsallis q-distributions, affect dark matter relic density calculations and explore new degeneracies in parameters beyond standard WIMP scenarios.

Method: Generalizes freeze-out calculations using Curado-Tsallis q-distributions, modifies thermodynamic quantities and Boltzmann equation for nonextensive statistics, performs q-grid scans varying dark matter mass and cross-section parameters, compares relic densities to Planck observations.

Result:  found that q values >1 increase freeze-out parameter x_f and relic density variability, but realistic cross-sections constrain q towards the extensive limit (q→1). Parameter degeneracies exist when varying mass or cross-section coefficient a.

Conclusion: Nonextensive statistics can modulate WIMP relic density predictions but observational constraints tightly restrict q near unity, suggesting minimal departure from standard Boltzmann-Gibbs statistics in dark matter freeze-out.

Abstract: We generalize thermal WIMP (Weakly Interacting Massive Particle) freeze-out within Tsallis nonextensive statistics. Using Curado-Tsallis $q$-distributions $f_q(E;μ,T)$ we compute $q$-deformed number and energy densities, pressure, entropy density and Hubble rate, $\{n_q,ρ_q,P_q,s_q,H_q\}$. The Boltzmann equation is generalized accordingly to obtain the comoving abundance $Y_{χ,q}(x)$ and relic density $Ω_{χ,q}h^2$ for a dark-matter candidate $χ$ in a model-independent setup. The thermally averaged cross section is expanded as $\langleσv\rangle_q \approx a + b\,\langle v_{\rm rel}^2\rangle_q$ up to $p$-wave. The freeze-out parameter $x_f(q)$ is determined from $Γ_{{\rm ann},q}(T_f)\simeq H_q(T_f)$ using a $q$-logarithmic inversion, with the expansion rate modified through ultra-relativistic rescalings $R_ρ(q)$ of the effective relativistic degrees of freedom $g_*$ and $g_{*s}$. We show that $x_f$ increases with $q$ and that QCD-threshold features propagate into $Y_{χ,q}(x)$ and $Ω_{χ,q}h^2$. We then perform two $q$-grid scans: fixing $\langleσv\rangle_q$ while varying the dark-matter mass $m_χ$, and fixing $m_χ$ while varying the $s$-wave coefficient $a$. For an $s$-wave dominated scenario we construct $χ^2$ profiles in these planes by comparing $Ω_{χ,q}h^2$ with the Planck benchmark $Ω_c h^2 = 0.120\pm 0.001$. In both cases we find a clear degeneracy in the preferred nonextensive parameter $q_{\rm best}$ along valleys in parameter space. However, fixed-mass scans (varying $\langleσv\rangle_q$) are significantly more constraining than fixed-cross-section scans, reflecting that $Ω_{χ,q}h^2$ is mainly controlled by $\langleσv\rangle_q$, so that for realistic cross sections the best-fit $q_{\rm best}$ remains close to the extensive limit $q\to 1$.

</details>


### [26] [Prospects for Neutrino Observation and Mass Measurement from Binary Neutron Star Mergers](https://arxiv.org/abs/2511.16658)
*Vedran Brdar,Dibya S. Chattopadhyay,Samiur R. Mir,Tousif Raza,Marc S. Romanowski*

Main category: hep-ph

TL;DR: The paper evaluates the detectability of neutrinos from binary neutron star mergers using next-generation detectors like Deep-TITAND and proposes methods to improve sensitivity using time windows and luminosity cuts, suggesting potential beyond current neutrino mass constraints.


<details>
  <summary>Details</summary>
Motivation: To address the lack of attention on detecting neutrinos from binary neutron star mergers, which require megaton-scale detectors due to low event rates and background challenges, and to utilize time-of-flight delays from neutrino masses to enhance detection prospects.

Method: Analyzed updated binary neutron star merger rates from LIGO-Virgo-KAGRA data, modeled energy-dependent time windows accounting for neutrino mass-induced delays, applied luminosity distance cuts, and estimated required detector runtime for low-background detection. Also, explored using neutrino-gravitational wave timing to probe neutrino mass.

Result: Megaton detectors with 10 MeV thresholds are necessary; time windows extended by neutrino mass effects can reduce backgrounds. The proposed methods could achieve neutrino mass sensitivity surpassing KATRIN and galactic supernova limits.

Conclusion: Future megaton-scale detectors like Deep-TITAND can effectively detect neutron star merger neutrinos with low background and potentially provide the best neutrino mass constraints if current methods plateau.

Abstract: Over the next decade, $\mathcal{O}(100)$ diffuse supernova neutrino background (DSNB) events are expected in Hyper-Kamiokande. Another neutrino source that has received far less attention is binary neutron star mergers. Including the data from recent simulations, we find that detection in current and near-future neutrino experiments is not feasible, and a megaton-scale detector with $\mathcal{O}(10)$ MeV threshold, such as the proposed Deep-TITAND, MEMPHYS, or MICA, will be required. This is due to the updated binary neutron star merger rate and the time-of-flight delay caused by the nonzero neutrino mass. Regarding the former, recent results from LIGO, Virgo, and KAGRA has significantly lowered the upper limit on the neutron star merger rate. As for the latter, neutrino events from neutron star mergers are expected to be recorded shortly after the gravitational wave signal. Limiting the analysis to such short time windows can significantly reduce background rates. While this approach has been qualitatively discussed in the literature, the effect of the time delay caused by neutrino mass, which can substantially extend the observation windows, has been disregarded. We present a refined analysis employing energy-dependent time windows and luminosity distance cuts for the mergers and provide realistic estimates of the detector runtime required to record neutrinos from binary neutron star mergers with small background contamination. The relative timing between the neutrino and gravitational wave signals can also be employed to probe the scale of neutrino mass. We find that the sensitivity to the lightest neutrino mass exceeds both the most stringent terrestrial bounds from KATRIN and the projections based on galactic supernovae. This level of sensitivity may become particularly relevant in the future if terrestrial and supernova constraints are not significantly improved.

</details>


### [27] [Electroweak precision tests](https://arxiv.org/abs/2511.16534)
*L. Reina,L. Silvestrini*

Main category: hep-ph

TL;DR: The Standard Model (SM) is a precise framework for predicting electroweak processes, and electroweak precision fits are used to detect deviations from SM predictions as signs of new physics by comparing theoretical calculations with high-precision experimental data.


<details>
  <summary>Details</summary>
Motivation: Test the consistency of the Standard Model at the quantum level and identify evidence for physics beyond the Standard Model through deviations in precision measurements.

Method: Performing electroweak precision fits that combine multiple experimental observables to compare with Standard Model predictions.

Result: Such analyses can statistically identify discrepancies between SM expectations and experimental measurements, pointing to potential new physics contributions.

Conclusion: Electroweak precision tests remain a critical tool for probing limitations of the Standard Model and guiding searches for beyond-SM phenomena.

Abstract: The Standard Model of particle physics provides a rigorous framework within which processes mediated by electroweak interactions can be calculated with great accuracy. By comparing with high-precision experimental measurements of the same processes, deviations from Standard Model predictions can be identified as indirect signals of new physics. In particular, electroweak precision fits combine multiple observables and provide a unique test of the Standard Model consistency at the quantum level.

</details>


### [28] [Domain walls in the scaling regime: Equal Time Correlator and Gravitational Waves](https://arxiv.org/abs/2511.16649)
*Simone Blasi,Alberto Mariotti,Aäron Rase,Miguel Vanvlasselaer*

Main category: hep-ph

TL;DR: The paper uses 3D lattice simulations with CosmoLattice to study domain wall dynamics and their gravitational wave (GW) signatures, finding scaling behavior, universal subhorizon ETC/GW features, and dependence of GW IR characteristics on cosmology.


<details>
  <summary>Details</summary>
Motivation: To understand domain wall network behavior post-symmetry breaking, their approach to scaling, and accurate GW signal predictions for detection.

Method: 3D lattice field theory simulations (N=1250, 2048, 4096) with CosmoLattice; analyzed scaling regime dynamics, ETC extraction for energy-momentum tensor, UTC extensions for GW coherence assessment, and cosmology-dependent simulations.

Result: Scaling established in ~few Hubble times; universal ETC shape confirmed at subhorizon scales with GW UV universality; GW IR features reflect expansion history.

Conclusion: Domain walls are viable GW sources with predictable subhorizon signals, while cosmological expansion affectsGW large-scale properties, aiding GW observational strategies.

Abstract: Domain walls are topological defects that may have formed in the early Universe through the spontaneous breakdown of discrete symmetries, and can be a strong source of gravitational waves (GWs). We perform 3D lattice field theory simulations with CosmoLattice, considering grid sizes $N = 1250$, $2048$ and $4096$, to study the dynamics of the domain wall network and its GW signatures. We first analyze how the network approaches the scaling regime with a constant $\mathcal{O}(1)$ number of domain walls per Hubble volume, including setups with a large initial number of domains as expected in realistic scenarios, and find that scaling is always reached in a few Hubble times after the network formation. To better understand the properties of the scaling regime, we then numerically extract the Equal Time Correlator (ETC) of the energy-momentum tensor of the network, thus determining its characteristic shape for the case of domain walls, and verifying explicitly its functional dependence as predicted by scaling arguments. The ETC can be further extended to the Unequal Time Correlator (UTC) controlling the GW emission by making assumptions on the coherence of the source. By comparison with the actual GW spectrum evaluated by CosmoLattice, we are then able to infer the degree of coherence of the domain wall network. Finally, by performing numerical simulations in different background cosmologies, e.g. radiation domination and kination, we find evidence for a universal ETC at subhorizon scales and hence a universal shape of the GW spectrum in the UV, while the expansion history of the Universe may instead be determined by the IR features of the GW spectrum.

</details>


### [29] [Cold Quark Matter: Renormalization group improvement of the perturbative series](https://arxiv.org/abs/2511.16545)
*Loïc Fernandez,Jean-Loïc Kneur*

Main category: hep-ph

TL;DR: The paper explores enhanced methods for calculating QCD pressure in cold and dense conditions by employing all-order resummation of soft modes and renormalization group optimized perturbation theory (RGOPT), both of which reduce renormalization scale dependence compared to existing perturbative approaches.


<details>
  <summary>Details</summary>
Motivation: To address the significant residual renormalization scale dependence in current state-of-the-art calculations of the QCD pressure, especially under cold and dense conditions.

Method: 1. All-order resummation of soft modes
2. Renormalization Group Optimized Perturbation Theory (RGOPT)
Both methods aim to suppress renormalization scale uncertainty by systematically improving perturbative series convergence.

Result: Both approaches demonstrate marked reductions in renormalization scale dependence compared to conventional perturbative methods, yielding more reliable pressure calculations in cold, dense QCD regimes.

Conclusion: RGOPT and soft-mode resummation provide superior improvements in reducing theoretical uncertainties, offering a pathway toward more accurate predictions of QCD thermodynamics in extreme conditions.

Abstract: We discuss recent improvements of the cold and dense QCD pressure owing to an all-order resummation of the soft modes, or to the so-called renormalization group optimized perturbation theory (RGOPT). Both approaches show a significant improvement of the residual renormalization scale dependence with respect to the state-of-the-art results for the perturbative pressure.

</details>


### [30] [Search for Ultralight Dark Matter with Quantum Magnetometry in the Earth's Cavity](https://arxiv.org/abs/2511.16553)
*Ariel Arza,Yuanlin Gong,Jun Guo,Xiaofei Huang,Jing Shu,Hongliang Tian,Wenyu Wang,Kai Wei,Lei Wu,Mingming Xia,Jin Min Yang,Qiang Yuan,Yang Zhang,Yi Zhang,Bin Zhu*

Main category: hep-ph

TL;DR: The researchers used a high-sensitivity atomic magnetometer (GPEX) to search for ultralight dark matter candidates (axions and dark photons) in China's XiaoDushan desert. No signals were detected, setting new constraints on their coupling parameters. This demonstrates the feasibility of ground-based quantum sensors for future dark matter searches.


<details>
  <summary>Details</summary>
Motivation: To detect ultralight dark matter candidates (axions/dark photons) which may couple weakly to photons, producing oscillating electromagnetic signals in magnetic fields. The Earth's resonant cavity could amplify these signals, making them detectable.

Method: Deployed GPEX magnetometer in XiaoDushan desert; collected one-hour data to search for axion/dark photon-induced magnetic field oscillations. Analyzed data for resonance signals corresponding to expected dark matter masses.

Result: No evidence found for axion or dark photon signals. Derived constraints: axion-photon coupling $g_{aγγ} < 7×10^-10 GeV⁻¹$ and dark photon kinetic mixing ε < 2×10^-6 in the mass range 3.5×10^-16 eV to 1.8×10^-14 eV.

Conclusion: Confirmed the viability of ground-based quantum magnetometers for dark matter detection. Future detector networks with longer observation times could achieve sensitivity approaching astrophysical limits.

Abstract: Ultralight dark matter candidates, such as axions and dark photons, are leading dark matter candidates. They may couple feebly to photons, sourcing oscillating electromagnetic signals in the presence of magnetic field. The Earth resonant cavity formed between the ground and the ionosphere provides a natural waveguide that can amplify such signals. We carry out a project aiming to search for new physics using the unshielded high-sensitivity atomic magnetometer, termed the Geomagnetic Probe for nEw physiCS (GPEX). In this work, we report our first search for axion and dark photon dark matter, conducted in the desert of XiaoDushan in Gansu Province, China. Analysis of the collection of one-hour data shows no evidence for axion- or dark photon-induced magnetic signals. Correspondingly, we set the constraints on the axion-photon coupling with $g_{aγγ} < 7\times10^{-10}\, \mathrm{GeV^{-1}}$ and the dark photon kinetic-mixing parameter $ε< 2\times10^{-6}$ in the mass range $3.5 \times 10^{-16}\, \mathrm{eV} \sim 1.8 \times 10^{-14}\, \mathrm{eV}$. Our findings demonstrate the feasibility of using ground-based quantum magnetic sensors for ultralight dark matter searches. Future networks of such detectors operating over extended periods could further enhance sensitivity, approaching the limits set by astrophysical observations.

</details>


### [31] [KrkNLO matching and phenomenology for vector boson processes](https://arxiv.org/abs/2511.16605)
*Pratixan Sarmah,Andrzej Siódmok,James Whitehead*

Main category: hep-ph

TL;DR: The paper presents an extended implementation of the KrkNLO method in Herwig to handle a broader range of processes, addressing NLO matching uncertainties and negative-weight issues through a modified PDF factorisation scheme. Comparisons with MC@NLO variants and ATLAS data are provided.


<details>
  <summary>Details</summary>
Motivation: To address matching uncertainties in LHC physics arising from different NLO matching methods and eliminate the negative-weight problem in NLO event generation.

Method: Extends the KrkNLO method using a modified PDF factorisation scheme, integrated with an external matrix-element library in Herwig, enabling application to four vector-boson production processes.

Result: The KrkNLO method effectively eliminates negative weights across studied processes, shows consistency with LHC ATLAS data, and provides detailed comparisons with MC@NLO variants across phase space.

Conclusion: The extended KrkNLO method offers a robust approach for NLO matching, reducing uncertainties and computational challenges, validated through comparisons with experimental data and alternative methods.

Abstract: The combination of NLO matrix elements with parton showers is indispensable for LHC physics. Differences between matching methods introduce matching uncertainties, corresponding to formally higher-order terms. We recently presented the process-independent generalisation of the KrkNLO method for NLO matching, which employs a modified PDF factorisation scheme to achieve NLO accuracy. With this factorisation scheme, the method can be used for colour-singlet final-states, and was previously implemented in the Herwig Monte Carlo Event Generator and applied to the diphoton-production process.
  Here we present the extension of the implementation of the KrkNLO method within Herwig to support the full class of applicable processes, using an external matrix-element library. We re-validate the implementation, and use it to study the NLO matching uncertainty for four vector-boson production processes at the LHC: $W$, $Zγ$, $WW$ and $ZZ$. We demonstrate that the KrkNLO method effectively eliminates the negative-weight problem in NLO event generation, across the four processes studied. We provide detailed comparisons between KrkNLO and variants of the MC@NLO method with different shower starting-scale choices, across processes and throughout phase-space, including double-differential observables. For each process, we compare the predictions to LHC data from ATLAS.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [32] [Extended Entropic Dark Energy with Four Free Parameters: Theory, Dynamics, and Constraints](https://arxiv.org/abs/2511.15747)
*Davood Momeni*

Main category: gr-qc

TL;DR: The paper proposes a four-parameter entropic dark energy model in a spatially curved FLRW universe based on a generalized entropy-area relation, deriving analytical expressions for cosmological observables and identifying parameter regions that address the Hubble tension.


<details>
  <summary>Details</summary>
Motivation: To investigate an alternative to conventional dynamical dark energy models that can explain the elevated Hubble constant values observed by SH0ES and contribute to resolving the Hubble tension.

Method: Develop a generalized entropy-area relation framework, derive analytical Hubble parameter, dark energy density, and equation of state expressions, and perform a parameter space analysis focusing on spatially curved FLRW models.

Result: Viable model regions found for β>1 and small positive curvature, supporting higher H₀ values consistent with SH0ES data, offering an analytically tractable solution compared to standard models.

Conclusion: The model provides a viable alternative to standard dynamical dark energy approaches, analytically simple yet capable of addressing the Hubble tension through its parameter configurations.

Abstract: We investigate a four-parameter entropic dark energy model in a spatially curved FLRW universe, based on a generalized entropy-area relation at the apparent horizon. While the proposed entropy function captures a broad class of gravitational entropy corrections, including Bekenstein-Hawking, Tsallis, and power-law forms, it does not encompass information-theoretic entropies such as Sharma-Mittal or Renyi. Within this framework, we derive exact analytical expressions for key cosmological observables, including the Hubble parameter $H(z)$, the dark energy density parameter $Ω_D(z)$, and the equation of state $w_D(z)$. A comprehensive parameter-space analysis reveals viable regions, particularly for $β> 1$ and small positive curvature, that accommodate elevated $H_0$ values consistent with recent SH0ES measurements. Our results offer a simple and analytically tractable alternative to conventional dynamical dark energy models, with potential relevance to the ongoing Hubble tension.

</details>


### [33] [Comment to Comment to Black Hole in Dehnen $\left(1,4,\frac{1}{2}\right)$ Dark Matter Halo: Exact Solution, Lensing, Light Ring, and Thermodynamics](https://arxiv.org/abs/2511.15748)
*David Senjaya*

Main category: gr-qc

TL;DR: The paper refutes a critique from another author, stating that the errors mentioned are typographical and do not affect the validity of the original analyses, results, or interpretations.


<details>
  <summary>Details</summary>
Motivation: To correct a misplaced and unfounded critique that inaccurately claims foundational errors in the original work, thereby defending the integrity and validity of the original findings.

Method: The authors analyze the nature of the errors in the foundational components (3) and (5) of their original work, demonstrating that they are merely typographical and do not impact subsequent analyses or results.

Result: The critique by Al-Badawi is shown to be based on a misunderstanding, and the original results and interpretations remain fully valid.

Conclusion: The original analyses, numerical results, and physical interpretations are confirmed to be unaffected by the minor typographical errors, and the critique is dismissed as unsubstantiated.

Abstract: The claim in \cite{Al-Badawi:2025ipr} that *"the errors in the foundational components (3) and (5) of Ref. [1] invalidate all subsequent analyses, numerical results, and physical interpretations that depend on them"* is **entirely unfounded**. This statement reflects a fundamental misunderstanding of the typographical nature of the error and appears to be a misplaced critique originating from a competing author, rather than a substantive assessment of the results, which remain fully valid.

</details>


### [34] [Thermal Vacuum Model for Cosmology without Inflaton](https://arxiv.org/abs/2511.15760)
*Robert Alicki*

Main category: gr-qc

TL;DR: The paper proposes a modified $\Lambda CDM$ model where the inflaton field and dark energy are replaced by vacuum effects in an expanding FLRW universe. The joint vacuum state of all matter ingredients, including Standard Model particles and dark matter, is considered as a thermal equilibrium at a Gibbons-Hawking temperature proportional to the Hubble parameter. This approach explains inflation, its graceful exit, late-time cosmic acceleration, baryogenesis, and sets constraints on dark matter properties.


<details>
  <summary>Details</summary>
Motivation: To address shortcomings of the standard $\Lambda CDM$ model, particularly the need for separate inflaton fields and dark energy components, by unifying these phenomena through vacuum effects in a thermal equilibrium framework.

Method: The model treats the expanding universe's vacuum as a thermal equilibrium state with temperature proportional to the Hubble parameter and chemical potentials equal to particle masses. Anomalous quantum gravity effects are incorporated to explain baryogenesis, while constraints on dark matter particle properties arise naturally.

Result: The model successfully unifies inflationary expansion with late-time acceleration without ad hoc components. It allows for a graceful exit from inflation, provides a baryogenesis mechanism, and predicts specific bounds on dark matter mass and lifetime.

Conclusion: This unified vacuum-based cosmological model offers a coherent alternative to standard $\Lambda CDM$, potentially resolving multiple cosmological puzzles through a consistent thermodynamic treatment of the expanding universe's vacuum state.

Abstract: The previously proposed modification of the standard (flat) inflationary $ΛCDM$ model in which the inflaton field(s) and ``dark energy" are replaced by the vacum in expanding Friedmann-Lemaître-Robertson-Walker Universe is studied. The expanding joint vacuum of the all ingrediences of matter, including Standard Model particles and a dark matter sector, is treated as a thermal equilibrium state at temporal Gibbons-Hawking temperature, proportional to the Hubble parameter, and chemical potentials equal to particle masses. This theory provides not only the new mechanism of inflation and its graceful exit, but also explains acceleration of expansion for the late Universe. The formalism can be combined with the anomalous quantum gravity effects leading to a viable baryogenesis mechanism and certain bounds on dark matter particle masses and lifetimes.

</details>


### [35] [Embedding Einstein-Cartan theory to Cosmology](https://arxiv.org/abs/2511.15773)
*Aarav Shah,M. Yu. Khlopov,M. Krasnov*

Main category: gr-qc

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We investigate torsion-driven cosmological dynamics within the framework of Einstein-Cartan gravity using the De Donder-Weyl Hamiltonian formalism, where the tetrad and Lorentz connection act as independent variables and the Hamiltonian includes quadratic Riemann Cartan corrections. Embedding this theory in an FLRW background, we derive the corresponding torsion-modified Friedmann equations and analyze their solutions across radiation and matter-dominated epochs. The commonly assumed power law form $a(t)=βt^α$ is shown to generate multiple solution branches, many of which can be considered to be 'unphysical', atleast to hold true globally, for all times. A hybrid solution, $a(t)=Ct^αe^{Dt^β}$ emerges in the special case $g_1=0$, where the quadratic Riemann-Cartan term vanishes. For $g_1\not=0$, the equations become nonlinear, precluding closed-form analytic solutions. These findings highlight the limitations of the power-law approximation and identify the restricted conditions under which torsion can coherently drive cosmic expansion.

</details>


### [36] [Teukolsky by Design: A Hybrid Spectral-PINN solver for Kerr Quasinormal Modes](https://arxiv.org/abs/2511.15796)
*Alexandre M. Pombo,Lorenzo Pizzuti*

Main category: gr-qc

TL;DR: The paper introduces SpectralPINN, a hybrid pseudo-spectral/PINN solver for calculating Kerr quasinormal modes from the Teukolsky equation, achieving high precision across separable and non-separable cases. It demonstrates applicability for constraining perturbations within Einstein Telescope precision.


<details>
  <summary>Details</summary>
Motivation: Address the computational challenges in solving the Teukolsky equation for Kerr quasinormal modes, especially in non-separable cases, and validate against established methods. Also, enable exploration of perturbed systems relevant for gravitational wave astronomy.

Method: Combines pseudo-spectral methods with PINNs using Chebyshev polynomials as activation functions. Implements Leaver's normalization through soft (loss penalties) and hard (analytic masks) approaches. Solves both separated and joint 2D Teukolsky formulations, including a non-separable perturbation term.

Result: Achieved 0.001-0.1% relative error vs Leaver's method, with best performance in hard joint cases. Successfully applied to non-separable perturbed systems, showing constraints compatible with Einstein Telescope precision requirements.

Conclusion: SpectralPINN offers a versatile and accurate method for quasinormal mode calculations, particularly in complex non-separable scenarios where high precision is critical for astrophysical applications.

Abstract: We introduce SpectralPINN, a hybrid pseudo-spectral/physics-informed neural network (PINN) solver for Kerr quasinormal modes that targets the Teukolsky equation in both the separated (radial/angular) and joint two-dimensional formulations. The solver replaces standard neural activation functions with Chebyshev polynomials of the first kind and supports both soft -- via loss penalties -- and hard -- enforced by analytic masks -- implementations of Leaver's normalization. Benchmarking against Leaver's continued-fraction method shows cumulative (real+imaginary part) relative frequency errors of $\sim 0.001\%$ for the separated formulation with hard normalization, $\sim 0.1\%$ for both the soft separated and soft joint formulations, and $\sim 0.01\%$ for the hard joint case. Exploiting our ability to solve the joint equation, we add a small quadrupolar perturbation to the Teukolsky operator, effectively rendering the problem non-separable. The resulting perturbed quasinormal modes are compared against the expected precision of the Einstein Telescope, allowing us to constrain the magnitude of the perturbation. These proof-of-concept results demonstrate that hybrid spectral-PINN solvers can provide a flexible pathway to quasinormal spectra in settings where separability, asymptotics, or field content become more intricate and high accuracy is required.

</details>


### [37] [Dimensional Phenomenology in Polymeric Quantization Framework](https://arxiv.org/abs/2511.15826)
*Kourosh Nozari,Hamed Ramezani*

Main category: gr-qc

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this paper, we study the statistical mechanics within the polymer quantization framework in the semiclassical regime. We apply a non-canonical transformation to the phase space variables. Then, we use this non-canonical transformation to calculate the deformed density of states of the $2n$-dimensional phase space, which encompasses all polymer effects. In the next step, some thermodynamic features of a system of $n$-dimensional harmonic oscillators are studied by computing the deformed partition function. The results show that the number of microstates decreases because there is an upper bound on the momentum within the polymer framework. We found that in the high-temperature regime, when the thermal de Broglie wavelength is close to the Planck length, $n$ degrees of freedom of the system are frozen in this setup. In other words, there is an effective reduction in space dimensions from $n$ to $\frac{n}{2}$ in the polymeric framework, which also signals the fractional dimension for odd-dimensional oscillators.

</details>


### [38] [Friedmann equations from GUP-modified equipartition law](https://arxiv.org/abs/2511.15871)
*Özgür Ökcü*

Main category: gr-qc

TL;DR: The paper explores the combination of thermodynamical horizon arguments and quadratic GUP to derive modified Friedmann equations in entropic gravity, leading to a non-singular universe with maximum energy density and Hubble parameter. It also connects GUP effects to braneworld cosmology and gravitational baryogenesis.


<details>
  <summary>Details</summary>
Motivation: To address cosmological singularities and understand the role of the generalized uncertainty principle in modifying gravitational laws, particularly through Verlinde's entropic gravity framework.

Method: Heuristically applying quadratic GUP to thermodynamical horizon arguments to derive a modified equipartition law, then using this to obtain GUP-modified Friedmann equations. Calculations include Kretschmann scalar analysis, deceleration parameter, and baryogenesis mechanism.

Result: Derivation shows a non-singular universe with maximum initial energy density and Hubble parameter, preference for negative GUP parameter based on initial acceleration, parallels to braneworld cosmology, and a mechanism for baryon asymmetry generation.

Conclusion: GUP provides a viable framework to resolve initial singularities and offers insights into cosmological dynamics and particle physics phenomena like baryogenesis, with observational constraints on the GUP parameter.

Abstract: In this paper, combining the thermodynamical arguments of the horizon with the quadratic generalised uncertainty principle (GUP), we heuristically obtain the modified equipartition law of energy. Employing this modified equipartition law of energy, we derive the Friedmann equations in Verlinde's entropic gravity. We find a maximum energy density at the beginning of the Universe. Remarkably, this feature emerges not only for positive GUP parameter but also for negative GUP parameter. From the initial acceleration, we deduce that the negative GUP parameter is more preferable. We also obtain maximum Hubble parameter from the first Friedmann equation, indicating a universe without initial singularity. Moreover, we compute the Kretschmann curvature scalar, again indicating a non-singular universe. Interestingly, we find that GUP-modified Friedmann equations share some similarities with braneworld cosmolgy where the quadratic term in energy density appears. We also compute the deceleration parameter. Finally, we revisit the gravitational baryogenesis and show that the GUP-modified equipartition law of energy provides a mechanism for generating baryon asymmetry. Moreover, we constrain the GUP parameter from observations.

</details>


### [39] [Ten years of extreme gravity tests of general theory of relativity with gravitational-wave observations](https://arxiv.org/abs/2511.15890)
*Anuradha Gupta*

Main category: gr-qc

TL;DR: This review discusses the current status and challenges of testing Einstein's general relativity (GR) with gravitational wave (GW) observations from compact binary mergers, highlighting the potential path forward to detect any violations of GR.


<details>
  <summary>Details</summary>
Motivation: To capitalize on the unique opportunity provided by GW observations, such as GW150914, to test GR in the extreme gravity regime where traditional experimental methods are limited.

Method: Summarizes existing GR tests using GW data from binary black hole/neutron star mergers, evaluates statistical methodologies employed, and analyzes systematic uncertainties in waveform models and data analysis techniques.

Result: CurrentGW observations have confirmed GR predictions within observational uncertainties but have not yet detected any deviations. Challenges include improving detector sensitivity, enhancing waveform models, and developing robust statistical frameworks.

Conclusion: While GR remains consistent with GW data to date, future improvements in observation technology and analysis methods could enable detection of potential GR violations, emphasizing the need for continued investment in multi-messenger astronomy.

Abstract: Ten years ago, the first direct detection of gravitational waves (GWs) from the merger of two black holes, GW150914, provided the very first opportunity to test Einstein's general theory of relativity (GR) in the extreme gravity regime, where the gravitational field is strong, characteristic speeds are highly relativistic, and spacetime is dynamical. Such a regime is currently accessible only through coalescing compact binaries. In this review, we summarize the status of testing GR with GW observations and discuss the lessons learned. We also touch upon the challenges we currently have in testing GR and the potential path forward to detect a credible violation of GR, should one exist in the data.

</details>


### [40] [Finite-Dimensional ZX-Calculus for Loop Quantum Gravity](https://arxiv.org/abs/2511.15966)
*Ben Priestley*

Main category: gr-qc

TL;DR: The thesis presents a new approach to spin network calculations in canonical Loop Quantum Gravity (LQG) by translating them into the finite-dimensional ZX-calculus, enabling accurate numerical results and flexible handling of changing graph structures. It introduces mixed-dimensional ZX-diagrams, derives fundamental LQG objects, and explores a matrix-like normal form using W-nodes.


<details>
  <summary>Details</summary>
Motivation: To overcome the difficulty of numerical calculations in canonical LQG by providing an intuitive graphical language (ZX-calculus) that maintains flexibility in handling evolving graph structures, addressing limitations of existing spin network tools.

Method: Translating spin networks into finite-dimensional ZX-calculus, deriving mixed-dimensional ZX-diagrams for generating objects, and developing rules for Penrose Spin Calculus (PSC) operations like loop removal. Additionally, explores a matrix-like normal form based on W-node analysis.

Result: First-time derivations of LQG objects in ZX-calculus, a high-level graphical language that balances intuition and flexibility. Demonstrates correctness of 'loop removal' and establishes PSC as a leading framework for canonical LQG.

Conclusion: The ZX-calculus-based Penrose Spin Calculus provides a powerful toolset for canonical LQG, enabling complex calculations while preserving visual intuition. It opens pathways for future research into numerical methods and structural analysis of spin networks.

Abstract: Loop quantum gravity (LQG) attempts to unify general relativity with quantum physics to offer a complete description of the universe by quantising spacetime geometry, but the numerical calculations we encounter are extraordinarily difficult. Progress has been made in the covariant formulation of LQG, but the tools do not carry over to the canonical formulation. These tools are graphical by nature, describing space with spin networks to make calculations in LQG more intuitive to the human hand.
  Recently, a new notation for working with spin networks has been used by arXiv:2412.20272 to offer the first accurate numerical results in canonical LQG by allowing the underlying graphs to change throughout the calculation, though they are forced to concede visual intuitiveness. In this thesis, we offer a more radical rephrasing of spin network calculations by translating them into the finite-dimensional ZX-calculus, extending previous attempts to translate into the standard (qubit) ZX-calculus (arXiv:2111.03114). Specifically, we derive the mixed-dimensional ZX-diagrams representing the generating objects of spin networks and the rules for the Penrose Spin Calculus (arXiv:2511.06012), and use these to present the ZX-form and correctness of "loop removal". We also derive the forms for several fundamental LQG objects in the finite-dimensional ZX-calculus for the first time. This gives us a high-level, intuitive graphical language that retains a flexibility to handle changing graph structures, and thus we argue positions the PSC as the new definitive language for canonical LQG.
  Furthermore, we investigate the possibility for a matrix-like normal form for spin networks deriving from a novel perspective of the PSC in terms of W-nodes.

</details>


### [41] [Grey-body Factors and Absorption Cross Sections of Non-Commutative Black Holes under Einstein-Coupled Scalar Fields](https://arxiv.org/abs/2511.16012)
*SiHao Fan,Chen Wu,WenJun Guo*

Main category: gr-qc

TL;DR: The paper examines scalar field perturbations interacting with the Einstein tensor in non-commutative black hole spacetimes, computing gray-body factors and absorption cross-sections. It finds that increasing non-commutativity (θ) and coupling (η) reduces absorption, and confirms the correspondence between gray-body factors and quasinormal modes for large l.


<details>
  <summary>Details</summary>
Motivation: The motivation is to study how non-commutative geometry effects (via parameter θ) and scalar-Einstein tensor couplings (η) influence black hole absorption processes and validate the connection between gray-body factors and quasinormal modes.

Method: Used the partial wave method to compute gray-body factors and absorption cross-sections for scalar fields coupled to the Einstein tensor in non-commutative black hole backgrounds.

Result: Increased θ and η reduce absorption cross-sections. The correspondence between gray-body factors and quasinormal modes holds accurately for large angular momentum l.

Conclusion: Non-commutative parameters significantly affect black hole perturbation dynamics, and the validated correspondence supports using quasinormal mode data to infer absorption properties in high-l regimes.

Abstract: This paper investigates scalar field perturbations coupled to the Einstein tensor of non-commutative black holes . We compute the grey-body factors and absorption cross-sections for different choices of the parameters using the partial wave method , and verify the latest correspondence between grey-body factors and quasinormal modes. The results show that larger values of the non-commutativity parameter $θ$ and the coupling constant $η$ introduced in this model lead to smaller absorption cross-sections. Furthermore, we find that this correspondence is accurate for non-commutative black holes in the limit of large angular momentum quantum number $l$.

</details>


### [42] [Neutrino oscillations induced by a new bumblebee black hole](https://arxiv.org/abs/2511.16181)
*Yuxuan Shi,A. A. Araújo Filho*

Main category: gr-qc

TL;DR: This paper examines how neutrinos behave around a special type of black hole that breaks Lorentz symmetry in bumblebee gravity. It looks at three aspects: energy from neutrino-antineutrino annihilation, phase shifts in neutrino oscillations, and gravitational lensing effects on flavor transitions. The results show significant changes in annihilation efficiency, phase shifts, and angular dependence due to Lorentz violation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand how spontaneous Lorentz-symmetry breaking in bumblebee gravity affects neutrino propagation near black holes, exploring new physical effects in annihilation, oscillation phases, and lensing compared to previous models.

Method: The study uses a two-flavor neutrino system to analyze three components: annihilation energy deposition, geometric phase contributions, and lensing-induced flavor transitions. Both mass orderings are considered, incorporating trajectory interference in calculations.

Result: Numerical results indicate the Lorentz-violating black hole significantly boosts annihilation efficiency, introduces unique phase shifts in oscillations, and alters the angular dependence of flavor transitions due to lensing.

Conclusion: The findings highlight that Lorentz-violating spacetime geometries in bumblebee gravity produce measurable deviations in neutrino propagation, offering new observational signatures for testing theories beyond standard physics.

Abstract: This work investigates neutrino propagation in the spacetime of a newly introduced black hole arising from spontaneous Lorentz-symmetry breaking in bumblebee gravity. The analysis focuses on three independent components: the rate at which neutrino-antineutrino annihilation deposits energy in the surrounding region, the geometric contribution to the phase accumulated by neutrino mass eigenstates, and the modifications to flavor conversion produced by weak gravitational lensing. Working with a two-flavor system, both mass orderings are examined, and the calculation incorporates the interference between distinct trajectories reaching the detector. The numerical results show that the Lorentz-violating deformation substantially increases the efficiency of the annihilation channel, produces characteristic shifts in the oscillation phase not present in earlier bumblebee configurations, and reshapes the angular dependence of the lensing-induced flavor transition pattern.

</details>


### [43] [Temperature definitions and phase transitions within non-minimal large and small inflationary potentials](https://arxiv.org/abs/2511.16250)
*Jesus Anaya-Galeana,Orlando Luongo,Hernando Quevedo*

Main category: gr-qc

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We explore and compare two distinct temperature definitions for scalar field inflation in the context of small- and large-field potentials. The first is based on a real gas, fluid-like temperature, $T_{RG}$, while the second corresponds to a relativistic species-like temperature, $T_{RS}$. We derive the fundamental thermodynamic relations for both and analyze their implications for the most viable inflationary potentials, consistent with Planck constraints. We also investigate non-minimally coupled scenarios, finding that $T_{RS}$ is the most self-consistent choice, as it decreases during inflation, satisfies standard thermodynamic laws, and exhibits frame-independent behavior in both the Jordan and Einstein frames. Remarkably, the $T_{RS}$ approach shows that the inflaton's dynamics is well-described by Van der Waals-like isotherms, linking inflationary evolution to thermodynamic phase transitions. We find that the onset of inflation is associated with a phase transition acting as the ``trigger'' of the inflationary epoch. Our analysis highlights inconsistencies in the hilltop potential and, more generally, in small-field potentials unless a non-minimal coupling is introduced. Conversely, the Starobinsky and $α$-attractor models emerge as the most suitable paradigms. We further show that \emph{frame independence} is achieved only for coupling values $ζ\leq 1/6$, supporting very small values. Finally, our study of natural inflation with non-minimal coupling reveals a strong dependence on the coupling parameter, where bounds associated with thermodynamic phase transitions coincide with observationally viable ranges, suggesting that thermodynamic considerations may provide an additional criterion to discriminate among inflationary scenarios.

</details>


### [44] [Herglotz-type $f(R,T)$ gravity](https://arxiv.org/abs/2511.16304)
*Marek Wazny,Lehel Csillag,Miguel A. S. Pinto,Tiberiu Harko*

Main category: gr-qc

TL;DR: This paper proposes a new formulation of f(R,T) gravity using the Herglotz variational principle to account for dissipation, leading to modified gravitational field equations. These modifications align with observations like Mercury's perihelion precession and Cassini data, and allow previously excluded models to fit observational data.


<details>
  <summary>Details</summary>
Motivation: The non-conservation of the energy-momentum tensor in f(R,T) gravity suggests dissipation, motivating the application of the Herglotz principle to extend the theory to handle dissipative systems.

Method: The authors apply the Herglotz variational principle, which allows the Lagrangian to depend on the action, extending f(R,T) gravity. They derive new field equations, analyze the Newtonian limit, and test against Mercury's precession and light deflection. Two cosmological models are examined, focusing on the Herglotz vector's role.

Result: Herglotz contributions modify the gravitational potential, aligning with Cassini observations. The Herglotz vector can act like a cosmological constant, enabling cosmic acceleration. The linear f(R,T)=R+αT model, previously invalid, becomes observationally viable under this formulation.

Conclusion: The Herglotz approach offers a valid framework for incorporating dissipation in f(R,T) gravity, resolving prior inconsistencies and providing a mechanism for cosmic acceleration without dark energy.

Abstract: The non-conservation of the energy-momentum tensor in $f(R,T)$ gravity can be interpreted as an effective manifestation of dissipation. Motivated by this, we propose a new formulation of $f(R,T)$ gravity based on the Herglotz variational principle, which extends the usual {Hamilton} variational principle to dissipative systems by allowing the Lagrangian to depend explicitly on the action. The resulting gravitational field equations extend those of $f(R,T)$ gravity by including Herglotz contributions. In the Newtonian limit, these contributions modify the gravitational potential, allowing us to constrain the Herglotz vector through Mercury's perihelion precession and the relativistic light deflection. Remarkably, the Herglotz corrections lead to a scaling law consistent with observations from the Cassini spacecraft. Examining two representative cosmological models, the Herglotz vector effectively reduces to a single function that, under suitable conditions, can play the role of a cosmological constant, providing an alternative mechanism for the Universe's accelerated expansion. Within the Herglotz variational approach, the linear $f(R,T)=R+αT$ model, previously ruled out in the standard formulation due to its fixed deceleration parameter, becomes consistent with observations.

</details>


### [45] [Primordial non-Gaussianity in noncanonical warm inflation with nonminimal derivative coupling](https://arxiv.org/abs/2511.16312)
*Xiao-Min Zhang,Run-Qing Zhao,Yun-Cai Feng,Peng-Cheng Chu,Zhi-Peng Peng,Xi-Bin Li*

Main category: gr-qc

TL;DR: The paper explores non-Gaussian perturbations in warm k-inflation models driven by kinetic energy, focusing on three-point and four-point correlations (f_{NL}^{int} and f_{NL}^{δN}), comparing their contributions and constrain model parameters using observational data.


<details>
  <summary>Details</summary>
Motivation: To investigate how non-Gaussian features arise in warm k-inflation scenarios and to determine parameter constraints through comparison with observational data, enhancing our understanding of inflationary dynamics.

Method: The study separately computes the intrinsic non-Gaussian component from three-point correlations (f_{NL}^{int}) and the δN contribution from four-point correlations (f_{NL}^{δN}). These components are analyzed and compared, with theoretical results juxtaposed against observational constraints.

Result: The analysis derives expressions for both non-Gaussian parameters within the warm k-inflation framework and identifies parameter ranges consistent with observational bounds, offering insights into kinetic-driven inflation's viability.

Conclusion: The findings highlight the significance of considering both three- and four-point correlations in assessing non-Gaussianity in warm inflation models, providing a path for refining inflationary parameter spaces through observational consistency.

Abstract: This paper presents and investigates non-Gaussian perturbations for the warm k-inflation model that is driven by pure kinetic energy. The two complementary components of the overall non-Gaussianity are the three-point and four-point correlations. The intrinsic non-Gaussian component, denoted as the nonlinear parameter f_{NL}^{int}, is rooted in the three-point correlation for the inflaton field. Meanwhile, the δN part non-Gaussianity, denoted as f_{NL}^{δN}, is the contribution attributed to the four-point correlation function of the inflaton field. In this paper, the above two components in warm k-inflation are individually computed and analyzed. Then, comparisons and discussions between them are conducted, and the non-Gaussian theoretical results are compared with experimental observations to determine the range of model parameters within the allowable range of observation.

</details>


### [46] [Quantum corrections in general relativity explored through a GUP-inspired maximal acceleration analysis](https://arxiv.org/abs/2511.16502)
*Christian Corda,Carlo Cafaro,Newshaw Bahreyni*

Main category: gr-qc

TL;DR: The paper revises Pati's 1992 maximum acceleration analysis using the Generalized Uncertainty Principle (GUP) instead of the Heisenberg Uncertainty Principle, deriving a maximum acceleration a_max ≈ 4c²/l_P. This quantum correction suggests that general relativity breaks down not at the Planck scale but at the Schwarzschild scale, preventing infinite gravitational acceleration in black holes and eliminating singularities.


<details>
  <summary>Details</summary>
Motivation: To improve the traditional maximum acceleration analysis by incorporating the GUP's minimum length prediction, addressing limitations of the HUP and exploring implications for black hole physics and quantum gravity.

Method: Replacing HUP with GUP in Pati's framework to calculate maximum acceleration. Applied to Schwarzschild black holes to test quantum corrections to general relativity at the event horizon scale.

Result: Calculated a_max ≈ 4c²/l_P. Demonstrated finite maximum gravitational acceleration at the Schwarzschild radius, indicating quantum gravity effects become significant at the black hole scale rather than the Planck scale.

Conclusion: GUP-based quantum corrections prevent black hole singularities, showing that quantum effects influence spacetime curvature well below the Planck length, challenging the conventional wisdom that quantum gravity dominates only at Planck scales.

Abstract: A maximun acceleration analysis by Pati dating back to 1992 is here improved by replacing the traditional Heisenberg Uncertainty Principle (HUP) with the Generalized Uncertainty Principle (GUP), which predicts the existence of a minimum length in Nature. This new approach allows one to find a numerical value for the maximum acceleration existing in Nature for a physical particle that turns out to be a_{max}\simeq4\frac{c^{2}}{l_{P}}, that is, a function of two fundamental physical quantities such as the speed of light c and the Planck length l_{p}. An application of this result to black hole (BH) physics allows one to estimate a new quantum limit to general relativity. It is indeed shown that, for every real Schwarzschild BH, the maximum gravitational acceleration occurs, without becoming infinite, when the Schwarzschild radial coordinate reaches the gravitational radius. This means that quantum corrections to general relativity become necessary not at the Planck scale, as the majority of researchers in the field think, but at the Schwarzschild scale, in agreement with recent interesting results in the literature. In other words, the quantum nature of physics, which in this case manifests itself through the GUP, appears to prohibit the existence of real singularities, in this current case forbiddiing the gravitational acceleration of a Schwarzschild BH from becoming infinite.

</details>


### [47] [k-inflation: Non-separable case meets ACT measurements](https://arxiv.org/abs/2511.16621)
*Tahere Fallahi Serish,Seyed Ali Hosseini Mansoori,Fereshteh Felegary,Özgür Akarsu,Mohamad Sami*

Main category: gr-qc

TL;DR: The paper explores a non-separable $k$-essence model where the kinetic and potential sectors interact via an $X^ρV(φ)$ coupling. This interaction allows for a constant sound speed during slow roll while modifying inflationary observables. Analytic solutions for spectral index $n_s$ and tensor-to-scalar ratio $r$ are derived, validated against numerical results, and shown to align with cosmological constraints when $\mathcal{K}<0$.


<details>
  <summary>Details</summary>
Motivation: To investigate how non-separable $k$-essence couplings can modify inflationary predictions, enabling otherwise disfavored monomial potentials to fit within observational bounds set by ACT+Planck+BAO data.

Method: Derivation of analytic expressions for $n_s(N_\ast)$ and $r(N_\ast)$ to $\mathcal{O}(ε_{\text{mix}}^2)$ using monomial potentials $V=Aφ^n$ ($n=2$ and $n=2/3$). Validated via comparison with numerical background integrations.

Result: Analytic predictions match numerical results to sub-per-mille (for $n_s$) and percent (for $r$) accuracy. Negative $\mathcal{K}$ values reduce both $n_s$ and $r$, bringing monomial models into alignment with cosmological constraints. Health conditions remain satisfied.

Conclusion: Non-separable $k$-essence provides a viable framework to adjust inflationary observables, revitalizing monomial potentials previously excluded by observations. The model’s predictions remain consistent with current non-Gaussianity bounds.

Abstract: We investigate a non-separable subset of $k$-essence in which the kinetic and potential sectors interact through an $X^ρV(φ)$ coupling, implemented via a potential-dependent prefactor $f(φ)=1+2\mathcal{K}V$. In slow roll, this structure preserves a constant sound speed $c_s^2=1/(2ρ-1)$ while modifying the Hubble flow in a controlled way, thereby shifting the inflationary observables relative to the separable template. For monomial potentials $V=Aφ^n$ (with $n=2$ and $n=2/3$ as representative cases) we derive closed analytic expressions for $n_s(N_\ast)$ and $r(N_\ast)$ to $\mathcal{O}(ε_{\rm mix}^2)$, where $ε_{\rm mix}\propto\mathcal{K}$ encodes the non-separable $X^ρV$ mixing, and we validate them against exact background integrations. The analytic and numerical predictions agree at the sub-per-mille level for $n_s$ and at the percent level for $r$, confirming the accuracy of the small-mixing expansion. For $\mathcal{K}<0$ the mixing systematically lowers both $n_s$ and $r$ at fixed $N_\ast$, allowing otherwise marginal monomials to fall within the region favored by recent ACT+{\it Planck}+BAO constraints (P--ACT--LB). All solutions shown satisfy the health conditions $f(φ)>0$, $ρ>\tfrac12$, and the positivity bound $V<1/(2|\mathcal K|)$ (from $f>0$). We also discuss parameter dependence and the expected equilateral-type non-Gaussianity, which remains comfortably within current bounds for the benchmarks considered.

</details>


### [48] [Ergodic Hysteresis of the Kerr black hole spectrum](https://arxiv.org/abs/2511.16640)
*João Paulo Cavalcante,Maurício Richartz,Bruno Carneiro da Cunha*

Main category: gr-qc

TL;DR: This paper uncovers a cascade of exceptional points (EPs) in the quasinormal mode spectrum of massive scalar perturbations around Kerr black holes, revealing an intricate non-Hermitian structure. These EPs originate from damped modes entering the extremal spectrum under large field masses, with infinite sequences found in $(\ell,m)=(1,1)$ and $(2,2)$ sectors near extremal limits. The EPs induce adiabatic mode mixing through geometric phases, termed adiabatic ergodicity.


<details>
  <summary>Details</summary>
Motivation: To understand the non-Hermitian structure in Kerr black hole linear response and explore how EP cascades influence mode transitions between damped and zero-damping states near extremal limits.

Method: Analysis of quasinormal mode spectra for massive scalar perturbations of Kerr black holes, numerical identification of EP sequences in specific angular momentum sectors, and examination of geometric phase effects during adiabatic transitions.

Result: Discovery of an infinite EP cascade in $(\ell,m)=(1,1)$ and $(2,2)$ modes near extremality, mediating mode transitions with adiabatic ergodicity via geometric phases.

Conclusion: EP cascades and adiabatic ergodicity provide new insights into non-Hermitian physics in black hole perturbations, suggesting profound implications for gravitational wave signatures and black hole dynamics.

Abstract: We uncover a cascade of exceptional points (EPs) in the quasinormal mode spectrum of massive scalar perturbations of Kerr black holes, revealing an intricate non-Hermitian structure underlying their linear response. The cascade originates from a single damped mode that enters the extremal spectrum for sufficiently large field masses. We obtain evidence for an infinite sequence of EPs in the $(\ell,m)=(1,1)$ and $(2,2)$ sectors near the extremal limit, mediating the transition between damped and zero-damping modes. Each EP carries a geometric phase that enables adiabatic mode mixing across the entire overtone spectrum, a phenomenon we refer to as adiabatic ergodicity.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [49] [Iron Nuclei in Ultra-High Energy Cosmic Rays Near the Earth](https://arxiv.org/abs/2511.15776)
*A. Uryson*

Main category: astro-ph.HE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The propagation of cosmic iron nuclei with energies above 10^19 eV from their sources to the Galaxy is discussed by assuming that cosmic rays at ultra-high energies are of extragalactic origin. In extragalactic space, cosmic nuclei interact with background emissions and inevitably fragment. An analysis is performed of the fraction of iron nuclei reaching the Earth and how its energy depends on the distance from cosmic ray sources. It is found that energies of iron nuclei can be used to determine restriction on the distances from their sources

</details>


### [50] [How Bright in Gravitational Waves are Millisecond Pulsars for the Galactic Center GeV Gamma-Ray Excess? A Systematic Study](https://arxiv.org/abs/2511.15793)
*Ming-Yu Lei,Bei Zhou,Xiaoyuan Huang*

Main category: astro-ph.HE

TL;DR: The study investigates the gravitational wave signals from millisecond pulsars (MSPs) in the Galactic Center that could explain the GeV gamma-ray excess, finding that next-gen detectors like the Einstein Telescope and Cosmic Explorer could detect these signals, which would test the MSP explanation versus dark matter hypotheses.


<details>
  <summary>Details</summary>
Motivation: To determine whether unresolved MSPs in the Galactic bulge cause the GeV excess and assess if gravitational waves can validate or constrain this astrophysical explanation, challenging DM annihilation models.

Method: The authors model MSP populations with three ellipticity scenarios, simulate GW emissions for isolated/binary systems, incorporate Doppler effects in detections, and evaluate detectability using current/future GW detectors.

Result: Current interferometers cannot detect the signals, but next-gen detectors (ET/CE) could find a fraction of these MSPs. Future directed searches will either discover the MSP population or set strict limits on their properties, impacting GCE interpretations.

Conclusion: GW observations provide a critical test of MSP vs dark-matter origins for the GCE, with next-gen detectors enabling this verification or exclusion, advancing understanding of both astrophysical and particle DM aspects of the anomaly.

Abstract: The existence of dark matter (DM) is supported by various macroscopic observations, but its microscopic nature remains elusive. The Galactic Center GeV gamma-ray excess (GCE) has been a leading candidate signal for particle dark matter annihilation. However, an unresolved population of millisecond pulsars (MSPs) in the bulge provides the alternative explanation for the excess. Identifying these MSPs in electromagnetic bands is difficult due to source confusion, pulse broadening, and extinction. Gravitational waves (GWs) provide a complementary probe: a steadily rotating, non-axisymmetric MSP emits a nearly monochromatic GW signal in the sensitive band of ground-based detectors, with amplitude set by its ellipticity. In this work, we systematically investigate the GW emission from the MSP population proposed to explain the GCE and its detectability with current and future detectors. We consider three major scenarios for the origin of ellipticity and model the population properties of these MSPs. We also consider both isolated MSPs and MSPs in binary systems, as well as Doppler effects in the detection. We find that while the signal is below the reach of current interferometers, next-generation detectors such as the Einstein Telescope (ET) and Cosmic Explorer (CE) can detect a fraction of those MSPs, offering a novel test of the MSP interpretation of the GCE. Future directed searches toward the Galactic Center with continued improvements in sensitivities will either uncover this long-sought MSP population or place stringent limits on their ellipticities and abundance, with important implications for both the astrophysical and dark-matter interpretations of the GCE.

</details>


### [51] [SN 2019vxm: A Shocking Coincidence between Fermi and TESS](https://arxiv.org/abs/2511.15975)
*Zachary G. Lane,Ryan Ridden-Harper,Sofia Rest,Armin Rest,Conor L. Ransome,Qinan Wang,Clarinda Montilla,Micaela Steed,Igor Andreoni,Patrick Armstrong,Peter J. Brown,Jeffrey Cooke,David A. Coulter,Ori Fox,James Freeburn,Marco Galoppo,Avishay Gal-Yam,Jared A. Goldberg,Christopher Harvey-Hawes,Rebekah Hounsell,Brayden Leicester,Itai Linial,Thomas Moore,Pierre Mourier,Anya E. Nugent,David O'Neill,Hugh Roxburgh,Koji Shukawa,Stephen J. Smartt,Nathan Smith,Ken W. Smith,Sebastian Vergara Carrasco,V. Ashley Villar,Tal Wasserman,Zenati Yossef,Erez Zimmerman*

Main category: astro-ph.HE

TL;DR: SN 2019vxm is a superluminous Type IIn supernova with unique early-phase observations, showing a shallow luminosity rise and coincident X-ray emission linked to shock breakout in a dense environment.


<details>
  <summary>Details</summary>
Motivation: To study the early stages of core-collapse supernovae through shock breakout and high-energy emissions, focusing on Type IIn systems with interaction-dominated environments.

Method: Comprehensive photometric analysis using TESS data and X-ray observations, modeling the light curve with a broken power law, and associating the X-ray transient GRB191117A with the supernova.

Result: Identified a 3.3σ association between SN 2019vxm and GRB191117A, constrained first light timing to 7.2 hours, found a shallower-than-expected luminosity rise (n=1.41), suggesting a dense asymmetric circumstellar medium.

Conclusion: The observations confirm shock breakout in a clumpy, asymmetric environment, pointing to a massive progenitor like a luminous blue variable transitioning to Wolf-Rayet phase.

Abstract: Shock breakout and, in some cases, jet-driven high-energy emission are increasingly recognized as key signatures of the earliest phases of core-collapse supernovae, especially in Type IIn systems due to their dense, interaction-dominated circumstellar environments. We present a comprehensive photometric analysis of SN 2019vxm, a long-duration, luminous Type IIn supernova, $M_V^{}=-21.41\pm0.05\;{\rm mag}$, observed from X-ray to near-infrared. SN 2019vxm is the first superluminous supernovae Type IIn to be caught with well-sampled TESS photometric data on the rise and has a convincing coincident X-ray source at the time of first light. The high-cadence TESS light curve captures the early-time rise, which is well described by a broken power law with an index of $n=1.41\pm0.04$, significantly shallower than the canonical $n=2$ behavior. From this, we constrain the time of first light to within 7.2 hours. We identify a spatial and temporal coincidence between SN 2019vxm and the X-ray transient GRB191117A, corresponding to a $3.3σ$ association confidence. Both the short-duration X-ray event and the lightcurve modeling are consistent with shock breakout into a dense, asymmetric circumstellar medium, indicative of a massive, compact progenitor such as a luminous blue variable transitioning to Wolf-Rayet phase embedded in a clumpy, asymmetric environment.

</details>


### [52] [A pilot VLBI study of the SQUAB quasar sample featuring multiple Gaia detections](https://arxiv.org/abs/2511.16206)
*Yingkang Zhang,Tao An,Xiang Ji,Zhenya Zheng,Yuanqi Liu,Qiqi Wu,Ruqiu Lin,Shilong Liao*

Main category: astro-ph.HE

TL;DR: The study uses VLBI observations to analyze radio-bright quasars with multiple Gaia detections, confirming most are quasar-star pairs rather than dual AGNs, demonstrating the effectiveness of combining Gaia and VLBI for accurate classification.


<details>
  <summary>Details</summary>
Motivation: To refine the classification of quasars with multiple Gaia counterparts, identify dual AGNs, and evaluate the Gaia-VLBI approach's efficacy in resolving ambiguous systems.

Method: Cross-matched SQUAB quasars with radio catalogs (FIRST/NVSS), selected 18 radio sources, observed 3 brightest with VLBA at dual frequencies, produced spectral index maps and brightness temperature estimates.

Result: Observed compact radio structures at primary Gaia positions but no emission at secondaries, supporting quasar-star pair classification instead of dual AGNs.

Conclusion: VLBI follow-up is valuable for accurately characterizing ambiguous quasar systems, helping purge contaminants from dual AGN candidate lists and validating the Gaia-VLBI methodology.

Abstract: Our previous work identified a class of SDSS quasars exhibiting multiple Gaia detections, classifying them as candidates for various astrophysical systems such as quasar-star pairs, dual quasars, and gravitationally lensed quasars. In this paper, we present a pilot VLBI study targeting a radio-bright subsample and report the first high-resolution imaging results. By leveraging the milliarcsecond-scale resolution of VLBI and its precise astrometric coordination incorporating with Gaia, we aim to refine the classification of these multiple matched sources, search for potential dual AGNs, and assess the efficacy of the combined Gaia-VLBI approach in resolving ambiguous quasar systems. We cross-matched the SQUAB quasar sample with the FIRST and NVSS catalogs, identifying 18 radio-emitting sources. The three brightest were selected for dual-frequency (1.6 and 4.9 GHz) VLBA observations. We performed VLBI imaging at both Gaia positions, constructed spectral index maps, and estimated brightness temperatures to characterize the radio morphology and physical properties. For the three target sources, our VLBI observations reveal compact radio structures consistent with single AGN at the primary Gaia positions. No significant emission is detected at the secondary Gaia locations. These results support the interpretation of the sources as quasar$-$star pairs, in line with earlier studies. This pilot study demonstrates the value of radio-VLBI high-resolution follow-ups on Gaia-selected quasar systems with multiple counterparts, showing how they can unambiguously reveal the true nature of these systems and help remove contaminants from dual AGN candidate samples.

</details>


### [53] [Constraining the Corona Geometry of Cyg X-1 with Broad Band Spectrum and Polarimetric Analysis Based on Observations in May 2022](https://arxiv.org/abs/2511.16246)
*Sixuan Zhang,Tsunefumi Mizuno,Tomohisa Kawashima,Chris Done,Yasushi Fukazawa,Hiromitsu Takahashi,Ryusei Komine,Koudai Takebayashi,Ken Ohsuga*

Main category: astro-ph.HE

TL;DR: During 2022 observations of Cygnus X-1 in its low hard state, new analyses of IXPE, NuSTAR, and NICER data reveal a two-component Comptonization model better fits the data. A lower-than-usual disk temperature (0.15 keV) and a slab corona geometry align polarization angle with the jet but still underpredict observed polarization degrees. Simulations suggest additional coronal structures (wedge-shaped with truncated disk) might explain the discrepancy.


<details>
  <summary>Details</summary>
Motivation: The high polarization degree (PD) observed by IXPE in 2022 (~4x higher than expected) challenged existing models. Subsequent studies proposed non-standard geometries, but a rigorous joint spectral analysis was required to validate these ideas.

Method: Conducted a detailed joint spectral analysis of IXPE, NuSTAR, and NICER data. Tested one- vs. two-component Comptonization models. Simulated polarization effects of varying disk temperature, corona geometry (slab vs. wedge), and optical depths. Compared simulated polarization results with observed data.

Result: Two-component Comptonization model fits data better. Lower disk temperature (0.15 keV) linked to increased PD through simulations. Slab corona geometry matches PA constancy but PDs still too low. Wedge-shaped corona with a truncated disk and slab-like structure explains observed energy-dependent PD and stable PA.

Conclusion: Cyg X-1 2022 shows a two-Comptonization corona with a hard component in a sandwiching slab geometry. Observed polarization isn't fully explained by current models, suggesting complexities like coronal structure or other parameters (e.g., inclination) require further study. Provides insights into accretion geometry and black hole corona modeling.

Abstract: Cygnus X-1 (Cyg X-1) exhibited a low hard state in 2022, observed by several missions. The IXPE reported that the polarization angle is aligned with the radio jet and gave a polarization degree approximately 4 times higher than the general expectations of $1\%$ through the analysis of the time-integrated data with a simple spectral model, indicating that the disk inclination is higher than a canonical value of about $30^{\circ}$. Many subsequent theoretical studies employed a non-standard model to explain this high PD. Here, we revisit the disk/corona spectrum through a detailed joint analysis using IXPE, NuSTAR, and NICER data. By investigating the time variability of the spectrum, we find that the two-Comptonization components model can better reproduce the data than the one-Comptonization component model originally adopted. We observed a lower disk photon temperature of about 0.15 keV. Detailed simulation suggests that lowering the disk temperature by a factor of 2 increases the PD by roughly 2 percentage points in the IXPE 2--8 keV band for a slab-like corona geometry, helping to reconcile the observed high PD with theoretical predictions. However, The simulated PDs are still significantly lower than the observed ones - even for a rather high $60^{\circ}$ inclination. We also investigated the polarization properties of a simple wedge-shaped corona with a truncated disk and a sandwiching slab corona. We find that the slab corona predicts an apparent energy dependence in PD while PA remains constant in the IXPE band, in agreement with the observed polarization. Therefore, we suggest that Cyg X-1 in 2022 May exhibits a two-Comptonization coronal emission with different optical depths, and the hard one is in a sandwiching slab geometry. We also discuss how the polarization is affected by other parameters of the black hole and the corona.

</details>


### [54] [Thermal equilibrium curves of accretion disks driven by magnetorotational instability](https://arxiv.org/abs/2511.16314)
*Shigenobu Hirose*

Main category: astro-ph.HE

TL;DR: The paper examines thermal equilibrium curves of accretion disks using radiation magnetohydrodynamics (MHD) simulations, focusing on dwarf novae. It demonstrates that S-shaped loci (indicative of bistability and limit-cycle oscillations) emerge naturally without relying on the parameterized α-viscosity model. The findings support the disk instability model for dwarf nova outbursts and provide insights into MRI-driven angular momentum transport.


<details>
  <summary>Details</summary>
Motivation: To investigate the thermodynamics of accretion disks in dwarf novae by replacing the traditional α-viscosity parameterization with first-principles radiation MHD simulations, aiming to understand the origin of bistability and outburst cycles without adjustable parameters.

Method: The authors solve the governing equations of radiation MHD to compute thermal equilibrium curves, tracking the balance between viscous heating (from MRI-driven turbulence) and radiative cooling. Simulations explore how disk annuli behavior changes with surface density and temperature, focusing on MRI's role in angular momentum transport.

Result: The simulations reproduce S-shaped thermal equilibrium curves (critical for disk instability models) without assuming α-viscosity. Key results include MRI's self-consistent angular momentum transport, bistable regions, and unstable bands correlating with observed dwarf nova outburst intervals.

Conclusion: Radiation MHD simulations confirm disk instability mechanisms explained by intrinsic MRI dynamics, eliminating the need for α parameterization. This strengthens the theoretical foundation for interpreting accretion disk variability in cataclysmic variables like dwarf novae.

Abstract: Analogous to the HR diagram for stars, the thermal equilibrium curve encodes the thermodynamics of accretion disks by expressing the local balance between heating -- primarily via viscous dissipation -- and cooling -- typically through radiative transfer. These curves are commonly plotted as surface density versus effective temperature. When an S-shaped locus appears, local annuli become bistable, and limit-cycle oscillations arise when the external mass-transfer rate falls within an unstable band. This behavior underpins the disk instability model for recurring outbursts in cataclysmic variables. This paper reviews first-principles thermal equilibrium curves for accretion disks driven by magnetorotational instability (MRI), with emphasis on dwarf novae. Unlike the parameterized $α$-viscosity approach, the curves are obtained by solving the governing equations with radiation magnetohydrodynamics simulations, thereby reproducing S-shaped loci without prescribing $α$. The disk instability in dwarf-nova systems and the physical origin of angular-momentum transport (shear stresses) are also briefly reviewed. Notes on the stability of radiation-dominated accretion flows are included in the Appendix.

</details>


### [55] [Aql X-1 from dawn 'til dusk: the early rise, fast state transition and decay of its 2024 outburst](https://arxiv.org/abs/2511.16437)
*A. Marino,F. Coti Zelati,K. Alabarta,D. M. Russell,Y. Cavecchi,N. Rea,S. K. Rout,T. Di Salvo,J. Homan,Á. Jurado-López,L. Ji,R. Soria,T. D. Russell,Y. L. Wang,A. Anitra,M. C. Baglio,H. Feng,S. Fijma,S. Guillot,Y. F. Huang,G. Illiano,M. Imbrogno,C. Jin,F. Lewis,Y. F. Liang,M. J. Liu,R. Ma,G. Mastroserio,S. E. Motta,J. U. Ness,E. Parent,A. Patruno,P. Saikia,L. Tao,M. Veresvarska,X. P. Xu,W. Yuan,G. B. Zhang,Z. J. Zhang*

Main category: astro-ph.HE

TL;DR: The Einstein Probe detected the early stages of an outburst from the neutron star LMXB Aql X-1 at unprecedentedly low luminosities, enabling a multi-wavelength study of its evolution through X-ray spectral states. The study revealed a 12-hour hard-to-soft state transition and insights into accretion disk dynamics.


<details>
  <summary>Details</summary>
Motivation: To explore the early phases of transient LMXB outbursts, which are critical for understanding accretion processes but were previously observationally challenging due to detection limits. The Einstein Probe's sensitivity allows for studying fainter, earlier stages than before.

Method: A multi-wavelength campaign combining data from EP, NICER, NuSTAR, Swift, and Las Cumbres Observatory across X-ray and optical bands. Time-resolved spectroscopy was used to track accretion flow changes during the outburst's rise and state transitions.

Result: The X-ray emission lagged the optical rise by ≤3 days. A rapid 12-hour hard-to-soft state transition occurred two weeks post-optical onset, accompanied by disk inner region and boundary layer formation. The study observed evolutionary changes in disk temperature/sizes and NS surface properties.

Conclusion: EP's capabilities revolutionize early outburst studies, providing key constraints on accretion flow dynamics and state transition mechanisms in neutron star systems. The findings shed light on boundary layer physics and evolutionary timescales relevant to both neutron stars and black holes.

Abstract: Transient Low-Mass X-ray Binaries (LMXBs) are usually first detected by all-sky X-ray monitors when they enter new outbursts, typically at X-ray luminosities above $\sim$10$^{36}$ erg/s. Observations of these sources during the early rise of the outbursts have so far been very limited. However, the launch of the Einstein Probe (EP) has greatly improved our ability to detect fainter X-ray activity, unlocking access to the outburst early rise. In September 2024, EP detected the early onset of a new outburst from the neutron star LMXB Aql X-1, catching the source at a luminosity below 10$^{35}$ erg/s. In this paper we present results from a comprehensive, multi-wavelength campaign of this event, combining data from EP, NICER, NuSTAR, Swift and Las Cumbres Observatory covering the full outburst from its early rise through its decay. By comparing X-ray and optical light curves obtained with Las Cumbres Observatory during the initial rise, we show that the start of the X-ray emission lagged the optical rise by, at most, 3 days. Time-resolved X-ray spectroscopy revealed how the geometry and the physical properties of the accretion flow evolve during this early stage of the outburst, as well as at higher luminosities as the source transitioned through the canonical X-ray spectral states - hard, intermediate and soft. These data show that the source underwent a very rapid, about 12-h long, transition from the hard to the soft state about two weeks after the optical onset of the outburst. The evolution of the temperature and physical sizes of both the inner region of the disk and a black body near the NS surface suggest that at the state transition, a boundary and spreading layer likely formed. We discuss these results in the context of time-scales for outburst evolution and state transitions in accreting neutron stars and black holes.

</details>


### [56] [Tracing Early Cosmic Chemical Enrichment: A Uniform XMM-Newton Survey of Metallicity in Galaxy Groups and Clusters](https://arxiv.org/abs/2511.16448)
*Anne E Blackwell,Joel N Bregman,Sophia Chan Davis*

Main category: astro-ph.HE

TL;DR: The study analyzes metal abundances in galaxy clusters using XMM-Newton data, finding that present-day stellar populations alone cannot explain observed intracluster medium (ICM) metallicity. A correlation between ICM metallicity and stellar fraction suggests an early enrichment population (EEP) is necessary, especially in systems with low stellar-to-gas mass ratios.


<details>
  <summary>Details</summary>
Motivation: To resolve the discrepancy between observed ICM metallicities and predictions from current stellar populations, requiring identification of additional metal sources.

Method: Uniform XMM-Newton survey of 26 clusters/groups; radial metallicity profiling via multi-annulus spectral fitting. Applied closed-box models with updated SN yields and stellar/metal loss corrections.

Result: Found linear relation between Z_ICM and log(M_*/M_gas) with significant scatter. Closed-box models underpredict Z_ICM by ~0.15 solar units, necessitating EEP contribution. Three systems show anomalous properties suggesting late formation.

Conclusion: An early enrichment population distinct from visible stars is essential to explain observed ICM metallicities. The study provides empirical constraints for chemical evolution models and highlights exceptions necessitating further study.

Abstract: Observed metal abundances in the intracluster medium (ICM) of galaxy groups and clusters, $Z_{ICM}$, exceed what is expected from present-day stellar populations alone. Galaxy clusters are presumed to be near closed-box systems, allowing constraints to be placed on the origins of metals and stellar populations responsible for $Z_{ICM}$. We present a uniform XMM-Newton survey of 26 galaxy groups and clusters, measuring radial metallicity profiles and relating $Z_{ICM}$ with the stellar fraction $M_*/M_{gas}$. We determine $Z_{ICM}$ via spectral fitting across multiple annuli finding a best fit of $Z_{ICM} = -0.08^{+0.07}_{-0.07}\, log\left(\frac{M_*}{M_{gas}}\right) + 0.30^{+0.06}_{-0.06}$ with intrinsic scatter $σ_p = 0.09^{+0.02}_{-0.01}$. We use closed-box chemical evolution models to estimate the metallicity yield from observable stellar populations, incorporating updated supernova yields and corrections for metals locked in remnants, $Z_* = (1.14 \pm 0.52) \, log\left(1 + \frac{M_*}{M_{gas}}\right)$. Our results demonstrate that present-day stellar populations systematically underpredict $Z_{ICM}$, with an inferred excess component increasing in systems with low $M_*/M_{gas}$. This trend supports the need for an early enrichment population (EEP) distinct from visible stars, $Z_{EEP}$. We find this necessity holds when reconsidering the closed-box assumption by removing all galaxy groups, potential leaky systems, deriving $Z_{EEP}$ within $1σ$ when including and excluding groups. Three systems (NGC1132, NGC5098, and NGC4325) deviate from the survey trend, exhibiting steep negative radial metallicity gradients and unusually low $Z_{ICM}$. We posit these systems to be late-forming whose ICM enrichment reflects only recent stellar populations. Our analysis quantifies the necessity of an EEP and provides trends for testing cluster chemical evolution models.

</details>


### [57] [Two-beam Multiparticle Many-body simulations of Inhomogeneous FFI](https://arxiv.org/abs/2511.16506)
*Zoha Laraib,Sherwood Richers*

Main category: astro-ph.HE

TL;DR: The paper introduces a tensor-network framework to study neutrino flavor evolution in astrophysical environments, addressing limitations in previous studies by considering inhomogeneous, anisotropic systems and comparing different initial conditions. The framework shows that many-body systems equilibrate faster than mean-field models, with differences in final states based on initial beam configurations.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in existing many-body studies such as small system sizes, closed boundaries, and idealized symmetry assumptions, which hinder accurate modeling of flavor evolution in core-collapse supernovae and neutron-star mergers.

Method: A unified tensor-network framework enabling simulations of inhomogeneous/anisotropic neutrino distributions. The method examines effects of inhomogeneity, boundary conditions, and resolution convergence across various neutrino setups under a single consistent formulation.

Result: Many-body systems equilibrate earlier than mean-field counterparts but reach similar final states. Open boundaries can reproduce closed-system behavior only when beams start superimposed and interact continuously. Separated configurations develop entanglement more slowly, leading to different equilibrium flavor contents.

Conclusion: Tensor-network methods provide a robust tool for studying realistic neutrino interactions in dense astrophysical environments, highlighting the importance of initial conditions and system geometry in flavor evolution dynamics.

Abstract: Neutrino flavor evolution in dense astrophysical environments is inherently nonlinear and sensitive to many-body (MB) quantum effects beyond the mean-field (MF) approximation. Existing MB studies are constrained by small system sizes, closed boundaries, and highly idealized symmetry assumptions. We present a unified tensor-network framework that enables simulations of inhomogeneous and anisotropic flavor evolution under conditions relevant to core-collapse supernovae and neutron-star mergers. Within this framework, we examine the effects of inhomogeneity, boundary conditions, and convergence with resolution for multiple neutrino distributions, allowing direct comparison of these setups under one consistent formulation. In our simulations, many-body systems equilibrate earlier than their mean-field counterparts while approaching similar final flavor states. Enlarging the interaction region allows open boundaries to reproduce closed-system behavior, but only when the beams begin superimposed and interact continuously. By contrast, initially separated configurations develop entanglement more slowly, interact over longer times, and equilibrate to a flavor content that differs from that obtained from initially superimposed calculations.

</details>


### [58] [Neutron star heating vs. HST observations](https://arxiv.org/abs/2511.16507)
*Luis E. Rodríguez,Andreas Reisenegger,Denis González-Caniulef,Cristóbal Petrovich,George Pavlov,Sébastien Guillot,Oleg Kargaltsev,Blagoy Rangelov*

Main category: astro-ph.HE

TL;DR: This paper explores heating mechanisms in neutron stars (NSs) to explain observed high surface temperatures in old pulsars. Through modeling rotochemical heating (RH), vortex creep (VC), and crustal heating, the authors find a combined model of RH with a large pairing gap and VC best explains the data for specific pulsars.


<details>
  <summary>Details</summary>
Motivation: Observations show old neutron stars have unexpectedly high surface temperatures (~10^5 K), contradicting passive cooling models which predict much lower values. The paper aims to identify heating mechanisms (e.g., RH, VC, crustal heating) that can reconcile these discrepancies.

Method: The team computed thermal evolution models for 5 pulsars (including PSR J0437-4715, PSR B0950+08) incorporating RH in the core (with/without Cooper pairing), VC in the inner crust, and crustal heating. They compared model temperatures to observational data and upper limits.

Result: No single mechanism explains all pulsars. J0437-4715 requires RH with a large pairing gap (∆i~1.5 MeV) but needs unrealistic initial spin periods. Alternatively, VC or normal matter RH explain B0950+08 but underpredict J0437-4715. A combined RH-VC model aligns with observations for both pulsars and stays within observational limits for others.

Conclusion: A hybrid model (RH with large pairing gap + VC) best fits the data, predicting temperatures near observed upper limits. Deeper or broader-wavelength observations could validate this mechanism, highlighting the need for multiple heating sources in neutron stars.

Abstract: Passively cooling neutron stars (NSs) should reach undetectably low surface temperatures $T_s<10^4$ K in less than $10^7$ yr. However, HST observations have revealed likely thermal UV emission from the Gyr-old millisecond pulsars PSR~J0437$-$4715 and PSR~J2124$-$3358, and from the $\sim10^{7-8}$ yr-old classical pulsars PSR~B0950$+$08 and PSR~J0108$-$1431, implying $T_s\sim10^5$ K and the need for heating mechanisms. We compute the thermal evolution of these NSs including rotochemical heating (RH) in the core with normal or Cooper-paired matter, vortex creep (VC) in the inner crust, and crustal heating through nuclear reactions, and compare the results with observations and with the upper limit for PSR~2144$-$3933. No single mechanism explains all sources. The high temperature of PSR~J0437$-$4715 can be reproduced by RH with a large Cooper pairing gap $Δ_i\sim1.5$ MeV for either neutrons or protons, but this requires an unrealistically short initial period $P_0\lesssim1.8$ ms to activate the same mechanism in PSR~B0950$+$08. Conversely, the latter can be explained by RH with modified Urca reactions in normal matter or by VC with an excess angular momentum $J\sim3\times10^{43}$ erg,s, but these models underpredict PSR~J0437$-$4715. A model combining RH with a large pairing gap and VC matches both pulsars and is consistent with the upper limits for the remaining three. It further predicts that their temperatures should lie close to these limits, suggesting that deeper or broader-wavelength observations would provide a strong test of this scenario.

</details>


### [59] [A Core-Collapse Supernova Neutrino Parameterization with Enhanced Physical Interpretability](https://arxiv.org/abs/2511.16631)
*Haihao Shi,Zhenyang Huang,Junda Zhou,Guoliang Lü,Xuefei Chen*

Main category: astro-ph.HE

TL;DR: The paper introduces a new parameterization of supernova neutrino energy spectra using τ(t), which effectively models the thermal-diffusion area during explosions. This method improves fitting of historical SN1987A data, distinguishes successful from failed supernovae in simulations, links neutrino spectra to gravitational-wave signals, and estimates the progenitor mass of SN1987A as ~19 solar masses. It provides a tool for understanding core dynamics in future supernova events.


<details>
  <summary>Details</summary>
Motivation: To develop a physically motivated parameterization of neutrino energy spectra that can robustly model low-energy unobserved spectral regions and decode core-collapse supernova dynamics using multi-messenger data.

Method: A novel parameterization with τ(t) as the central parameter, applied to SN1987A data and 3D simulations. Techniques like Smoothed Isotonic Regression were used to constrain progenitor mass and analyze temporal evolution correlations with gravitational-wave strain.

Result: Statistically significant fits for SN1987A, clear separation of successful vs failed explosions via τ(t) evolution, synergistic relationship between τ(t) and gravitational-wave strain amplitude, and progenitor mass estimate of ~19 solar masses sensitive to observational uncertainties.

Conclusion: The framework effectively links neutrino spectra to explosion dynamics and gravitational-wave signals, offering a tool for analyzing multi-messenger data from future galactic supernovae to study core-collapse processes.

Abstract: We introduce a novel parameterization of supernova neutrino energy spectra with a clear physical motivation. Its central parameter, $τ(t)$, quantifies the characteristic thermal-diffusion area during the explosion. When applied to the historic SN1987A data, this parameterization yields statistically significant fits and provides robust constraints on the unobserved low-energy portion of the spectrum. Beyond this specific application, we demonstrate the model's power on a suite of 3D core-collapse supernova simulations, finding that the temporal evolution of $τ(t)$ distinctly separates successful from failed explosions. Furthermore, we constrain the progenitor mass of SN 1987A to approximately 19 solar masses by applying Smoothed Isotonic Regression, while noting the sensitivity of this estimate to observational uncertainties. Moreover, in these simulations, $τ(t)$ and the gravitational-wave strain amplitude display a strong, synergistic co-evolution, directly linking the engine's energetic evolution to its geometric asymmetry. This implies that the thermodynamic state of the explosion is imprinted not only on the escaping neutrino flux, but also recorded in the shape of the energy spectrum. Our framework therefore offers a valuable tool for decoding the detailed core dynamics and multi-messenger processes of future galactic supernovae.

</details>


### [60] [Lucky Strikes: On the Origins of GW190814 Through Isolated Binary Evolution](https://arxiv.org/abs/2511.16648)
*Ignacio Magaña Hernandez,Katelyn Breivik*

Main category: astro-ph.HE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The asymmetric nature of GW190814, particularly its mass ratio ($q \approx 1/10$), has made its astrophysical origin elusive. We explore isolated binary evolution as a potential explanation for GW190814's formation. Using the binary population synthesis code COSMIC, and the backpop sampling technique to map the observed parameters of GW190814 to the initial conditions of Zero Age Main Sequence binary stars while simultaneously inferring the astrophysical prescriptions for common envelope evolution, stable mass transfer and natal kick kinematics that are needed for its formation and eventual merger. We find that the initial conditions for the binary stellar population that forms GW190814 do not stand out significantly from massive star populations observed in the Local Group. Our backpop simulations recover a dominant formation pathway where the first Roche overflow phase includes a common envelope evolution and the second Roche overflow phase remains stable. Our findings suggest that natal kicks imparted during compact object formation play the strongest role in forming GW190814-like systems. Specifically, our models require a low magnitude first natal kick (independent of direction) that prevents the binary from unbinding and a large second natal kick with its direction in the plane of the orbit and toward the binary's center of mass. The second natal kick strength and direction crucially increases the orbital eccentricity, leading to shorter delay times, and thus enabling mergers within a Hubble time. We estimate the chance probability for GW190814-like events that experience such a lucky kick and find that it occurs in $\sim20\%$ of systems if natal kicks are randomly oriented. We discuss the astrophysical implications for the formation of asymmetric GW190814-like systems under the context of binary stellar evolution.

</details>
