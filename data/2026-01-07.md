<div id=toc></div>

# Table of Contents

- [astro-ph.HE](#astro-ph.HE) [Total: 11]
- [gr-qc](#gr-qc) [Total: 18]
- [hep-ph](#hep-ph) [Total: 23]
- [astro-ph.IM](#astro-ph.IM) [Total: 11]


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [1] [Magnetic reconnection with a 0.1 rate: Effective resistivity in general relativistic magnetohydrodynamics](https://arxiv.org/abs/2601.02460)
*B. Ripperda,M. P. Grehan,A. Moran,S. Selvi,L. Sironi,A. Philippov,A. Bransgrove,O. Porth*

Main category: astro-ph.HE

TL;DR: The paper demonstrates that incorporating an effective resistivity derived from kinetic models into relativistic resistive magnetohydrodynamics (RRMHD) improves the accuracy of reconnection rate predictions, matching kinetic simulations in both local and global astrophysical contexts.


<details>
  <summary>Details</summary>
Motivation: To address the discrepancy between RRMHD's underestimated reconnection rates and kinetic models, which are more accurate but computationally intensive.

Method: Modified RRMHD simulations using a resistivity model linked to charge-starved current density from kinetic theory, applied to both localized current sheets and black hole magnetospheres.

Result: The adjusted RRMHD model successfully replicates the higher reconnection rates observed in kinetic simulations across different astrophysical scenarios.

Conclusion: Kinetic-inspired resistivity models enhance RRMHD's predictive capability, offering a computationally efficient alternative for studying relativistic reconnection in neutron stars and black holes.

Abstract: Relativistic magnetic reconnection is thought to power various multi-wavelength emission signatures from neutron stars and black holes. Relativistic resistive magnetohydrodynamics (RRMHD) offers the simplest model of reconnection. However, a small uniform resistivity underestimates the reconnection rate compared to first-principles kinetic models. By employing an effective resistivity based on kinetic models - which connects the reconnection electric field to the charge-starved current density - we show that RRMHD can reproduce the increased reconnection rate of kinetic models, both in local current sheets and in global black hole magnetospheres.

</details>


### [2] [Neutrino flavor instabilities in neutron star mergers with moment transport: slow, fast, and collisional modes](https://arxiv.org/abs/2601.02461)
*Julien Froustey,Francois Foucart,Christian Hall,James P. Kneller,Debraj Kundu,Zidu Lin,Gail C. McLaughlin,Sherwood Richers*

Main category: astro-ph.HE

TL;DR: The paper explores flavor instabilities in neutron star mergers using an angular moment framework, comparing monoenergetic and multi-energy treatments, and assessing approximations like collisional, fast, and slow modes.


<details>
  <summary>Details</summary>
Motivation: To accurately model neutrino flavor oscillations in neutron star merger simulations, addressing the challenge of incorporating oscillations in hot, dense astrophysical environments.

Method: Using an angular moment-based linear stability analysis, the study examines collisional flavor instability, evaluates monoenergetic vs. multi-energy growth rates, and incorporates factors like nuclear corrections and scattering opacities.

Result: A combined energy-averaged estimate closely matches multi-energy growth rates, and anisotropies reveal distinct regions dominated by collisional, fast, or slow instabilities, with slow instabilities underappreciated without considering anisotropies.

Conclusion: Accurate simulations require multi-energy treatments and accounting for anisotropies to capture flavor instability dynamics correctly, especially in diverse regions of neutron merger environments.

Abstract: Determining where, when, and how neutrino flavor oscillations must be included in large-scale simulations of hot and dense astrophysical environments is an enduring challenge that must be tackled to obtain accurate predictions. Using an angular moment-based linear stability analysis framework, we examine the different kinds of flavor instabilities that can take place in the context of the post-processing of a neutron star merger simulation, with a particular focus on the collisional flavor instability and a careful assessment of several commonly used approximations. First, neglecting anisotropies of the neutrino field, we investigate the extent to which commonly used monoenergetic growth rates reproduce the results obtained from a full multi-energy treatment. Contrary to the large discrepancies found in core-collapse supernova environments, we propose a simple combination of energy-averaged estimates that reproduces the multi-energy growth rates in our representative simulation snapshot. We then quantify the impact of additional physical effects, including nuclear many-body corrections, scattering opacities, and the inclusion of the vacuum term in the neutrino Hamiltonian. Finally, we include the neutrino distribution anisotropies, which allows us to explore, for the first time in a multi-energy setting, the interplay between collisional, fast, and slow modes in a moment-based neutron star merger simulation. We find that despite a dominance of the fast instability in most of the simulation volume, certain regions only exhibit a collisional instability, while others, especially at large distances, exhibit a slow instability that is largely underestimated if anisotropic effects are neglected.

</details>


### [3] [On the exceptionality of exceptional gravitational-wave events](https://arxiv.org/abs/2601.02467)
*Rodrigo Tenorio,Davide Gerosa*

Main category: astro-ph.HE

TL;DR: The paper emphasizes the importance of critically evaluating claims of exceptional gravitational-wave sources, like GW231123 and GW241110, by considering measurement uncertainties and population distributions. It shows that while GW231123's high mass is robust, GW241110's misaligned spin may not be as extreme as initially thought.


<details>
  <summary>Details</summary>
Motivation: To address the concern that apparent exceptional characteristics of gravitational-wave events might arise from measurement errors rather than true extremes, particularly when population widths are comparable to measurement uncertainties.

Method: Quantitative analysis of measurement uncertainties and population distributions using agnostic priors to assess the likelihood of true exceptionality in GW231123 and GW241110.

Result: GW231123's total mass is likely accurately measured, but GW241110's anti-aligned spin has a 70% chance of being misinterpreted, possibly reflecting nonspinning or aligned configurations instead.

Conclusion: Claims of exceptionality under agnostic priors require scrutiny when measurement uncertainties are large relative to population variations; spin measurements are more susceptible to this effect than masses.

Abstract: In gravitational-wave astronomy, as in other scientific disciplines, ``exceptional'' sources attract considerable interest because they challenge our current understanding of the underlying (astro)physical processes. Crucially, ``exceptionality'' is defined only relative to the rest of the detected population. For instance, among all gravitational-wave events detected so far, GW231123 is the binary black hole with the largest total mass, while GW241110 is the binary black hole with the most strongly misaligned spin relative to the orbital angular momentum. Mandel [Astrophys.J.Lett. 996 (2026) 1, L4] argued that apparent ``exceptionality'' may reflect measurement error rather than an extreme true value, and suggested that the total mass of GW231123 may be significantly overestimated. Here we present a quantitative analysis that supports this conceptual point. We find that claims of ``exceptionality'' obtained under agnostic priors should be critically questioned whenever measurement uncertainties are comparable to the width of the underlying population. Specifically, we find that the total mass of GW231123 is unlikely to be meaningfully affected by this effect while the spin of GW241110 is far less likely to be anti-aligned than initially claimed: about 70% of realizations that appear to yield an ``exceptionally anti-aligned'' spin are in fact consistent with either nonspinning or aligned configurations.

</details>


### [4] [The Maximum Gravity Model for partial Tidal Disruption Events: Mass Loss, Peak Fallback Rate and Dependence on Stellar Properties](https://arxiv.org/abs/2601.02476)
*Ananya Bandopadhyay,Eric R. Coughlin,C. J. Nixon*

Main category: astro-ph.HE

TL;DR: The paper develops an analytical model for partial tidal disruption events (TDEs) by supermassive black holes, predicting peak fallback rate, peak time, and mass loss. It validates the model against hydrodynamical simulations for main-sequence stars disrupted by a 10^6 solar mass SMBH, showing good agreement in timing but less so in peak rates due to self-gravity effects.


<details>
  <summary>Details</summary>
Motivation: To provide an analytical framework that accurately predicts the dynamics and observables of partial TDEs, which are critical for understanding transient astrophysical phenomena and interpreting observations of black hole interactions with stars.

Method: An analytical model was formulated to calculate tpeak, Mpeak, and ΔM for partial TDEs. The model was tested against 1276 hydrodynamical simulations of disruptions involving main-sequence stars by a 1e6 solar mass SMBH, analyzing varying stellar masses, ages, and orbital parameters.

Result: The model accurately predicts tpeak within ~tens of percent across all cases. Mpeak agreement is within a factor of 2-3 for most cases except low-mass stars in grazing orbits where discrepancies were larger. Demonstrated that TDE lightcurve peak timescales vary from ~20-100 days for typical cases but can extend to ~1000+ days for high-mass stars/black holes in grazing encounters.

Conclusion: The analytical model offers a significant advancement for predicting TDE lightcurves and luminosity functions without needing computationally intensive simulations. It highlights the importance of self-gravity in mass loss calculations and provides a tool for comparing theoretical predictions with observed transients.

Abstract: A star entering the tidal sphere of a supermassive black hole (SMBH) can be partially stripped of mass, resulting in a partial tidal disruption event (TDE). Here we develop an analytical model for properties of these events, including the peak fallback rate, $\dot{M}_{\rm peak}$, the time at which the peak occurs, $t_{\rm peak}$, and the amount of mass removed from the star, $ΔM$, for any star and any pericenter distance associated with the stellar orbit about the black hole. We compare the model predictions to 1276 hydrodynamical simulations of partial TDEs of main-sequence stars by a $10^6 M_\odot$ SMBH. The model yields $t_{\rm peak}$ predictions that are in good agreement (to within tens of percent) with the numerical simulations for any stellar mass and age. The agreement for $\dot{M}_{\rm peak}$ is weaker due to the influence of self-gravity on the debris stream dynamics, which remains dynamically important for partial TDEs; the agreement for $\dot{M}_{\rm peak}$ is, however, to within a factor of $\sim 2-3$ in the majority of cases considered, with larger differences for low-mass stars ($M_\star \lesssim 0.5 M_\odot$) on grazing orbits with small mass loss. We show that partial TDE lightcurves for disruptions caused by $\sim 10^6M_\odot$ SMBHs can span $\sim 20-100$ day peak timescales, whereas grazing encounters of high-mass stars with high-mass SMBHs can yield longer peak timescales ($t\gtrsim 1000$ days), associated with some observed transients. Our model provides a significant step toward an analytical prescription for TDE lightcurves and luminosity functions.

</details>


### [5] [Resolving the Fe K$α$ Doublet of the Galactic Center Molecular Cloud G0.11-0.11 with XRISM](https://arxiv.org/abs/2601.02482)
*Stephen DiKerby,Shuo Zhang,Kumiko Nobukawa,Masayoshi Nobukawa,Yuma Aoki,Jack Uteg*

Main category: astro-ph.HE

TL;DR: The study resolves Fe Kα line emissions from the Galactic center molecular cloud G0.11-0.11 using XRISM-Resolve, distinguishing between X-ray fluorescence and cosmic ray ionization models. The absence of secondary Fe Kα lines and narrow line widths support the X-ray reflection model, suggesting illumination by Sgr A* around 200 years ago.


<details>
  <summary>Details</summary>
Motivation: To determine whether Fe Kα line emission in G0.11-0.11 originates from X-ray fluorescence (via Sgr A*) or cosmic ray ionization, requiring high-resolution spectroscopy to differentiate between the two models.

Method: High-resolution X-ray spectroscopy with XRISM-Resolve was used to resolve the Fe Kα line complex into Fe Kα₁ and Kα₂ with measured energies, widths, velocities, and search for secondary lines indicative of cosmic rays. Data analysis compared observed line characteristics to model predictions.

Result: Fe Kα lines show minimal broadening (~3 eV FWHM), matching quantum width expectations, and no secondary lines were detected. The cloud’s velocity (50±16 km/s) matches radio observations. X-ray reflection from Sgr A* ~200 years ago (L≈1e38 erg/s) best explains the findings.

Conclusion: The data favor the X-ray reflection model over cosmic ray ionization for Fe Kα emission in G0.11-0.11, implying past X-ray activity from Sgr A*. High-resolution X-ray spectroscopy provides a robust method to discriminate ionization mechanisms in molecular clouds.

Abstract: Fe K$α$ line emission from Galactic center molecular clouds can be produced either via fluorescence after illumination by an X-ray source or by cosmic ray ionization. Unparalleled high-resolution X-ray spectroscopy obtained by XRISM-Resolve for the galactic center molecular cloud G0.11-0.11 resolves its Fe K$α$ line complex for the first time, and points to a new method for discrimination between the X-ray reflection and cosmic ray ionization models. The Fe K$α$ line complex is resolved into Fe K$α_1$ at $E_{1} = 6.4040 \: \rm{keV}$ and Fe K$α_2$ at $E_{2}= 6.3910 \:\rm{keV}$. Both lines have non-instrumental FWHM of $\approx 3 \:\rm{eV}$, close to the predicted quantum mechanical width of the lines, suggesting scant other sources of line broadening other than instrumental and quantum effects. We measure a radial velocity of $v_{\rm{LSR}} = 50 \pm 12_{fit} \pm 14_{scale} \:\rm{km/s}$ for G0.11-0.11, achieving the same precision reached by radio observations of such clouds. The high-resolution spectrum tests for the presence of secondary Fe K$α$ lines, expected as a signature of cosmic ray proton/ion ionization. The absence of the secondary lines argues against the cosmic ray ionization model for G0.11-0.11. In the preferred X-ray reflection model, if the illuminating source is Sgr A$^{\star}$, the required luminosity for an X-ray outburst about 200 years ago is $L_8 \approx 10^{38} \:\rm{erg/s}$ in an $8\:\rm{keV}$-wide band at $8\:\rm{keV}$.

</details>


### [6] [Thick Disks, Thin Hopes: Suppressed Capture and Merger Rates in AGN](https://arxiv.org/abs/2601.02487)
*Yashvardhan Tomar,Philip F. Hopkins,Kyle Kremer*

Main category: astro-ph.HE

TL;DR: The study examines how disk thickness (H/R) profoundly affects interaction rates between embedded objects in AGN disks, with rates dropping by up to 10^20 times when considering magnetic pressure in thicker disks.


<details>
  <summary>Details</summary>
Motivation: To reassess the validity of previous AGN disk interaction rate calculations that relied on thin-disk assumptions, as pressure sources and disk geometry may drastically alter outcomes.

Method: Derives scaling laws showing inverse dependence of interaction rates on H/R, evaluates cross-dependency with pressure sources (thermal vs magnetic), and compares thin vs thick disk models.

Result: Interaction rates scale steeply with (H/R)^-8, leading to suppression factors up to ~1e10-1e20 when magnetic pressure dominates in thicker outer disks, challenging prior thin-disk based estimates.

Conclusion: AGN disk parameters like aspect ratio and pressure mechanisms must be carefully modeled to accurately predict interaction rates, as thick/magnetic-pressure dominated disks can negate prior assumptions.

Abstract: Multiple models have been suggested over the years to explain the structure and support of accretion disks around supermassive black holes, from the standard thin thermal-pressure-dominated $α$-disk model to more recent models that describe geometrically thicker radiation or magnetic or turbulence-dominated disks. In any case, objects embedded in the disk (e.g. compact objects, stars, gas, dust) can undergo gravitational and hydrodynamic interactions with each other leading to interesting processes such as binary interaction/capture, gravitational wave merger events, dynamical friction, accretion, gap opening, etc. It has long been argued that disks of active galactic nuclei (AGN) can enhance the rates for many of these events; however, almost all of that analysis has assumed specific thin-disk models (with aspect ratios $H/R \lesssim 0.01$). We show here that the rates for processes such as these that are mediated by gravitational cross-sections has a very strong inverse dependence on the thickness $H/R$ (scaling as steeply as $(H/R)^{-8}$), and $H/R$ can vary in the outer disk (where these processes are often invoked) by factors $\gtrsim 1000$ depending on the assumed source of pressure support in the disk. This predicts rates that can be lower by tens of orders-of-magnitude in some models, demonstrating that it is critical to account for disk parameters such as aspect ratio and different sources of disk pressure when computing any meaningful predictions for these rates. For instance, if magnetic pressure is important in the outer disk, as suggested in recent work, capture rates would be suppressed by factors $\sim 10^{10}-10^{20}$ compared to previous studies where magnetic pressure was ignored.

</details>


### [7] [Exploring Composition Mixing in Kilonova Ejecta with Ray-by-ray Simulations](https://arxiv.org/abs/2601.02600)
*Ruocheng Zhai,David Radice,Fabio Magistrelli,Sebastiano Bernuzzi,Albino Perego*

Main category: astro-ph.HE

TL;DR: The study investigates the impact of composition mixing on r-process nucleosynthesis in binary neutron star merger ejecta using radiation-hydrodynamic simulations. Mixing occurs in regions with rapid electron fraction changes but has little effect on heavy-element yields, as the primary r-process site (equatorial ejecta) is initially neutron-homogeneous. Abundances of major elements remain stable, with minor variations in less abundant elements and negligible effects on kilonova light curves.


<details>
  <summary>Details</summary>
Motivation: To understand how composition mixing affects r-process nucleosynthesis outcomes in BNSMs, which are key sources of heavy elements. Previous models may not account for mixing effects adequately, so this study incorporates mixing into simulations to assess its significance.

Method: Radiation-hydrodynamic simulations with a gradient-based mixing approximation and online nuclear network. Mixing was modeled in ray-by-ray calculations, focusing on electron fraction gradients and their influence on composition evolution.

Result: Mixing smooths composition gradients in transition regions but does not significantly alter overall heavy-element yields. Equatorial ejecta (main r-process site) lacks strong neutron gradients, so mixing has minimal impact. Major element abundances are stable, while less abundant elements show more variation. Kilonova light curves exhibit minor, undetectable reddening.

Conclusion: Composition mixing does not profoundly affect r-process outcomes or observable signatures (light curves) in BNSM ejecta during the studied timeframe. The primary results from non-mixed models remain largely valid, though minor abundance variations exist that could refine future compositional and observational analyses.

Abstract: Binary neutron star merger (BNSM) ejecta are considered a primary repository of $r$-process nucleosynthesis and a source of the observed heavy-element abundances. We implement composition mixing into ray-by-ray radiation-hydrodynamic simulations of BNSM ejecta, coupled with an online nuclear network (NN). We model mixing via a gradient-based mixing approximation that evolves simultaneously with the hydrodynamics. We find that mixing occurs in regions where the electron fraction changes rapidly. While mixing smooths composition gradients in transition regions, it has a negligible impact on the heavy-element yields. This is because the primary $r$-process site (the equatorial ejecta) is initially homogeneous in free neutrons, leaving no strong gradients for mixing to act upon. In each angular ray, the abundances of the most produced elements are robust under mixing, while the less abundant ones are more affected. The total global abundances change only slightly from mixing, since each angular ray contributes its most abundant elements. Furthermore, the predicted kilonova light curves show only minor reddening, with differences below the detectability of state-of-the-art telescopes. In general, we do not observe significant effects from mixing in the time span of the $r$-process. Consequently, mixing only leads to minor variations in abundances and light curves in ray-by-ray simulations.

</details>


### [8] [SN 2024abfl: A Flat-Plateau, Low-Luminosity Type IIP Supernova with Early CSM Interaction](https://arxiv.org/abs/2601.02638)
*Madison Gerard,Jennifer E. Andrews,Geoffrey C. Clayton,David J. Sand,K. Azalee Bostroem,Jeniveve Pearson,Raya Dastidar,Aravind P. Ravi,Conor L. Ransome,Bhagya Subrayan,Griffin Hosseinzadeh,Brian Hsu,Yize Dong,Manisha Shrestha,Stefano Valenti,Nathan Smith,Daryl Janzen,M. J. Lundquist,Nicolas Meza,Saurabh W. Jha,Kate D. Alexander,Collin Christy,Noah Franz,Lindsey A. Kwok,Moira Andrews,Joseph Farah,Daichi Hiramatsu,D. Andrew Howell,Curtis McCully,Kathryn Wynn,Reka Konyves-Toth,Xiaofeng Wang*

Main category: astro-ph.HE

TL;DR: SN 2024abfl is a low-luminosity Type IIP supernova with a 125-day plateau, low nickel-56 mass, and evidence of circumstellar material interaction, providing insights into progenitor properties and mass loss.


<details>
  <summary>Details</summary>
Motivation: To study the characteristics and origins of low-luminosity Type IIP supernovae through detailed photometric and spectroscopic analysis of SN 2024abfl.

Method: Photometric and spectroscopic observations were conducted, including analysis of the bolometric light curve, shock-cooling models, and spectroscopic feature identification (e.g., CSM interaction signatures, Hα profiles).

Result: Estimated 56Ni mass of ~0.01 solar masses, failure of shock-cooling models without CSM interaction, detection of broad ledge feature and multi-peaked Hα profiles indicative of ejecta-CSM interaction.

Conclusion: SN 2024abfl's weak explosion and CSM interaction suggest progenitors with pre-explosion mass loss, contributing to understanding LLSNe diversity and progenitor environments.

Abstract: We present photometric and spectroscopic observations of SN 2024abfl, a low-luminosity Type IIP supernova (LLSN) discovered shortly after explosion. The transient reached a peak absolute magnitude of $M_V = -14.9$ and exhibited an extended, flat plateau lasting $\sim$125 days. From the late-time bolometric light curve, we estimate a $^{56}$Ni mass of $\sim0.01~M_\odot$, consistent with other LLSNe. Analytical shock-cooling models fail to reproduce the rapid early rise, indicating that circumstellar material (CSM) interaction contributed to the initial emission. The spectroscopic evolution is typical of LLSNe, with relatively narrow metal lines and low expansion velocities ($\lesssim 3000$ km s$^{-1}$) that decline slowly over time. We detect a broad ``ledge'' feature around 4600 Åwithin three days of explosion, which we interpret as a blend of high-ionization shock-accelerated CSM lines. Multi-peaked H$α$ profiles develop during the plateau phase, consistent with complex ejecta-CSM interactions. As one of the best-observed examples of LLSNe, SN 2024abfl exhibits a weak explosion and signatures of nearby CSM, offering new insights into progenitor properties, pre-explosion mass loss, and the diversity of LLSNe.

</details>


### [9] [Super-Orbital Variations in Magnetar Rotation Measure Arising from the Precession of Companion Star: Implications for FRB 20220529](https://arxiv.org/abs/2601.02734)
*Ze-Xin Du,Yun-Wei Yu,Aming Chen,Chen-Hui Niu,Jia-Heng Zhang*

Main category: astro-ph.HE

TL;DR: The paper proposes a binary system model where the companion star's precessing spin and magnetic axes, along with orbital motion, explain the observed RM variations in FRB 20220529. The model predicts RM's super-orbital evolution and constrains parameters like a 182-day precession period and 19° inclination, while dismissing a dense disc due to stable DM.


<details>
  <summary>Details</summary>
Motivation: To explain the observed RM variations and partial reversal in FRB 20220529, suggesting a dynamically changing magnetized environment potentially linked to a binary system's orbital dynamics. The existing binary model needs refinement to account for long-term RM changes beyond orbital periods.

Method: Develop a binary model incorporating precession of the companion's spin/magnetic axes around the orbital axis. Analyze how precession period, magnetic inclination, and disc wind affect RM and DM evolution. Apply the model to FRB 20220529 data to derive parameter constraints.

Result: The model successfully reproduces RM variations with a 182-day precession period and 19° inclination angle. Stable DM rules out a dense equatorial disc, aligning with observations. Predicts complex RM patterns over super-orbital timescales requiring long-term monitoring.

Conclusion: FRB 20220529's RM evolution stems from precessing binary dynamics rather than static scenarios. Future observations should focus on tracking RM/DM changes to confirm precession periods and disk properties, strengthening the magnetar-binary model connection.

Abstract: Recent observations of FRB 20220529 reveal significant variation and a partial reversal in its rotation measure (RM), suggesting the presence of a dynamically evolving magnetized environment, which could be caused by the orbital motion of the magnetar within the binary system. Here we develop the binary model by suggesting that the spin and magnetic axis of the companion star could undergo precession around the orbital axis. It is then investigated how the precession period and the inclination of the magnetic axis, as well as a possible disc wind, can influence the evolution behaviors of the RM and dispersion measure (DM) of FRB emission. As the foremost consequence, the RM variation can be significantly altered on timescales longer than the orbital period, producing super-orbital evolution and complex patterns. Applying this model to FRB 20220529, we find that its RM evolution could be reproduced with a precession period of 182 days and an inclination angle of approximately $19^{\circ}$, while the other binary parameters are fixed at their typical values. Meanwhile, the absence of significant variation of the DM argues against the presence of a dense equatorial disc around the companion star, which would be constrained by future long-term observations.

</details>


### [10] [Transient Large-Scale Anisotropy in TeV Cosmic Rays due to an Interplanetary Coronal Mass Ejection](https://arxiv.org/abs/2601.02801)
*Zhen Cao,F. Aharonian,Y. X. Bai,Y. W. Bao,D. Bastieri,X. J. Bi,Y. J. Bi,W. Bian,A. V. Bukevich,C. M. Cai,W. Y. Cao,Zhe Cao,J. Chang,J. F. Chang,A. M. Chen,E. S. Chen,G. H. Chen,H. X. Chen,Liang Chen,Long Chen,M. J. Chen,M. L. Chen,Q. H. Chen,S. Chen,S. H. Chen,S. Z. Chen,T. L. Chen,X. B. Chen,X. J. Chen,Y. Chen,N. Cheng,Y. D. Cheng,M. C. Chu,M. Y. Cui,S. W. Cui,X. H. Cui,Y. D. Cui,B. Z. Dai,H. L. Dai,Z. G. Dai,Danzengluobu,Y. X. Diao,X. Q. Dong,K. K. Duan,J. H. Fan,Y. Z. Fan,J. Fang,J. H. Fang,K. Fang,C. F. Feng,H. Feng,L. Feng,S. H. Feng,X. T. Feng,Y. Feng,Y. L. Feng,S. Gabici,B. Gao,C. D. Gao,Q. Gao,W. Gao,W. K. Gao,M. M. Ge,T. T. Ge,L. S. Geng,G. Giacinti,G. H. Gong,Q. B. Gou,M. H. Gu,F. L. Guo,J. Guo,X. L. Guo,Y. Q. Guo,Y. Y. Guo,Y. A. Han,O. A. Hannuksela,M. Hasan,H. H. He,H. N. He,J. Y. He,X. Y. He,Y. He,S. Hernández-Cadena,B. W. Hou,C. Hou,X. Hou,H. B. Hu,S. C. Hu,C. Huang,D. H. Huang,J. J. Huang,T. Q. Huang,W. J. Huang,X. T. Huang,X. Y. Huang,Y. Huang,Y. Y. Huang,X. L. Ji,H. Y. Jia,K. Jia,H. B. Jiang,K. Jiang,X. W. Jiang,Z. J. Jiang,M. Jin,S. Kaci,M. M. Kang,I. Karpikov,D. Khangulyan,D. Kuleshov,K. Kurinov,B. B. Li,Cheng Li,Cong Li,D. Li,F. Li,H. B. Li,H. C. Li,Jian Li,Jie Li,K. Li,L. Li,R. L. Li,S. D. Li,T. Y. Li,W. L. Li,X. R. Li,Xin Li,Y. Li,Y. Z. Li,Zhe Li,Zhuo Li,E. W. Liang,Y. F. Liang,S. J. Lin,B. Liu,C. Liu,D. Liu,D. B. Liu,H. Liu,H. D. Liu,J. Liu,J. L. Liu,J. R. Liu,M. Y. Liu,R. Y. Liu,S. M. Liu,W. Liu,X. Liu,Y. Liu,Y. Liu,Y. N. Liu,Y. Q. Lou,Q. Luo,Y. Luo,H. K. Lv,B. Q. Ma,L. L. Ma,X. H. Ma,J. R. Mao,Z. Min,W. Mitthumsiri,G. B. Mou,H. J. Mu,A. Neronov,K. C. Y. Ng,M. Y. Ni,L. Nie,L. J. Ou,P. Pattarakijwanich,Z. Y. Pei,J. C. Qi,M. Y. Qi,J. J. Qin,A. Raza,C. Y. Ren,D. Ruffolo,A. Sáiz,D. Semikoz,L. Shao,O. Shchegolev,Y. Z. Shen,X. D. Sheng,Z. D. Shi,F. W. Shu,H. C. Song,Yu. V. Stenkin,V. Stepanov,Y. Su,D. X. Sun,H. Sun,Q. N. Sun,X. N. Sun,Z. B. Sun,N. H. Tabasam,J. Takata,P. H. T. Tam,H. B. Tan,Q. W. Tang,R. Tang,Z. B. Tang,W. W. Tian,C. N. Tong,L. H. Wan,C. Wang,G. W. Wang,H. G. Wang,J. C. Wang,K. Wang,Kai Wang,Kai Wang,L. P. Wang,L. Y. Wang,L. Y. Wang,R. Wang,W. Wang,X. G. Wang,X. J. Wang,X. Y. Wang,Y. Wang,Y. D. Wang,Z. H. Wang,Z. X. Wang,Zheng Wang,D. M. Wei,J. J. Wei,Y. J. Wei,T. Wen,S. S. Weng,C. Y. Wu,H. R. Wu,Q. W. Wu,S. Wu,X. F. Wu,Y. S. Wu,S. Q. Xi,J. Xia,J. J. Xia,G. M. Xiang,D. X. Xiao,G. Xiao,Y. L. Xin,Y. Xing,D. R. Xiong,Z. Xiong,D. L. Xu,R. F. Xu,R. X. Xu,W. L. Xu,L. Xue,D. H. Yan,T. Yan,C. W. Yang,C. Y. Yang,F. F. Yang,L. L. Yang,M. J. Yang,R. Z. Yang,W. X. Yang,Z. H. Yang,Z. G. Yao,X. A. Ye,L. Q. Yin,N. Yin,X. H. You,Z. Y. You,Q. Yuan,H. Yue,H. D. Zeng,T. X. Zeng,W. Zeng,X. T. Zeng,M. Zha,B. B. Zhang,B. T. Zhang,C. Zhang,F. Zhang,H. Zhang,H. M. Zhang,H. Y. Zhang,J. L. Zhang,Li Zhang,P. F. Zhang,P. P. Zhang,R. Zhang,S. R. Zhang,S. S. Zhang,W. Y. Zhang,X. Zhang,X. P. Zhang,Yi Zhang,Yong Zhang,Z. P. Zhang,J. Zhao,L. Zhao,L. Z. Zhao,S. P. Zhao,X. H. Zhao,Z. H. Zhao,F. Zheng,W. J. Zhong,B. Zhou,H. Zhou,J. N. Zhou,M. Zhou,P. Zhou,R. Zhou,X. X. Zhou,X. X. Zhou,B. Y. Zhu,C. G. Zhu,F. R. Zhu,H. Zhu,K. J. Zhu,Y. C. Zou,X. Zuo*

Main category: astro-ph.HE

TL;DR: Observation of transient large-scale cosmic ray anisotropy at TeV energies during a solar storm using LHAASO, revealing energy-dependent gradients and offering new insights into interplanetary magnetic structures.


<details>
  <summary>Details</summary>
Motivation: To investigate whether large/medium-scale cosmic ray anisotropy at TeV energies varies over time and to better understand the influence of solar events like coronal mass ejections on cosmic ray distributions.

Method: Hourly skymaps of cosmic ray intensity variations were analyzed using LHAASO data from November 4, 2021. The gradient of intensity excess/deficit was measured across energy ranges (0.7-3.1 TeV) to detect transient anisotropies.

Result: Significant (5σ) anisotropy enhancements were observed in multiple energy bands, with gradients following E^γ (γ≈-0.5). At energies ≤1 TeV, this corresponds to a 1% dipole anisotropy or higher-order effects.

Conclusion: TeV-scale anisotropy can vary transiently due to solar activity, providing a new method to remotely study interplanetary magnetic fields, complementing traditional plasma measurements.

Abstract: Large- or medium-scale cosmic ray anisotropy at TeV energies has not previously been confirmed to vary with time. Transient anisotropy changes have been observed below 150 GeV, especially near the passage of an interplanetary shock and coronal mass ejection containing a magnetic flux rope ejected by a solar storm, which can trigger a geomagnetic storm with practical consequences. In such events, cosmic rays provide remote sensing of the magnetic field properties. Here we report the observation of transient large-scale anisotropy in TeV cosmic ray ions using data from the Large High Altitude Air Shower Observatory (LHAASO). We analyze hourly skymaps of the transient cosmic ray intensity excess or deficit, the gradient of which indicates the direction and magnitude of transient large-scale anisotropy across the field of view. We observe enhanced anisotropy above typical hourly fluctuations with $>$5$σ$ significance during some hours of November 4, 2021, in separate data sets for four primary cosmic ray energy ranges of median energy from $E$=0.7 to 3.1 TeV. The gradient varies with energy as $E^γ$, where $γ\approx-0.5$. At a median energy $\leq$1.0 TeV, this gradient corresponds to a dipole anisotropy of at least 1\%, or possibly a weaker anisotropy of higher order. This new type of observation opens the opportunity to study interplanetary magnetic structures using air shower arrays around the world, complementing existing in situ and remote measurements of plasma properties.

</details>


### [11] [Large-scale radio bubbles around the black hole transient V4641 Sgr](https://arxiv.org/abs/2601.03140)
*Noa Grollimund,Stéphane Corbel,Rob Fender,James H. Matthews,Ian Heywood,Fraser J. Cowie,Andrew K. Hughes,Francesco Carotenuto,Sara E. Motta,Patrick Woudt*

Main category: astro-ph.HE

TL;DR: The paper discusses the discovery of a large-scale bow-tie-shaped radio structure around the black hole transient V4641 Sgr using MeerKAT observations, suggesting that jets or disk winds from the black hole accelerate electrons to very high energies.


<details>
  <summary>Details</summary>
Motivation: To search for a radio counterpart of the extended very-high-energy gamma-ray emission observed around V4641 Sgr and understand the role of microquasar jets in cosmic ray production.

Method: MeerKAT radio telescope observations at L and UHF bands; high dynamic range imaging to produce deep maps; comparison with XRISM X-ray data.

Result: Discovered a 35 pc-scale radio structure shaped like a bow-tie, not coinciding with VHE emission but matching X-ray extent. The structure's origin is linked to long-term jet or wind activity from the black hole.

Conclusion: The radio and X-ray structures indicate synchrotron emission from electrons accelerated to over 100 TeV by jets/disk winds at distances of tens of parsecs, highlighting microquasars' potential in cosmic ray production.

Abstract: Black holes (BHs) in microquasars can launch powerful relativistic jets that have the capacity to travel up to several parsecs from the compact object and interact with the interstellar medium. Recently, the detection of large-scale very-high-energy (VHE) gamma-ray emission around the black hole transient V4641 Sgr and other BH-jet systems suggested that jets from microquasars may play an important role in the production of galactic cosmic rays. V4641 Sgr is known for its superluminal radio jet discovered in 1999, but no radio counterpart of a large-scale jet has been observed. The goal of this work is to search for a radio counterpart of the extended VHE source. We observed V4641 Sgr with the MeerKAT radio telescope at the L and UHF bands and produced deep maps of the field using high dynamic range techniques. We report the discovery of a large-scale (35 pc), bow-tie-shaped, diffuse, radio structure around V4641 Sgr, with similar angular size to the extended X-ray emission discovered by XRISM. However, it is not spatially coincident with the extended VHE emission. After discussing the association of the structure with V4641 Sgr, we investigate the nature of the emission mechanism. We suggest that the bow-tie structure arose from the long-term action of large-scale jets or disk winds from V4641 Sgr. If the emission mechanism is of synchrotron origin, the radio/X-ray extended structure implies acceleration of electrons up to more than 100 TeV as far as tens of parsecs from the black hole.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [12] [Primordial Black Hole Formation in $f(R)=R+αR^2$ Gravity: Perturbative and Non-Perturbative Analysis](https://arxiv.org/abs/2601.02416)
*G. G. L. Nashed*

Main category: gr-qc

TL;DR: The paper provides an analytic and semi-analytic study of gravitational collapse and primordial black hole (PBH) formation in the quadratic $f(R)=R+αR^2$ model, deriving perturbative corrections to GR and exploring PBH formation thresholds under different cosmic eras.


<details>
  <summary>Details</summary>
Motivation: To understand how modifications to General Relativity via quadratic $f(R)$ gravity affect PBH formation, particularly in high-curvature early Universe scenarios where perturbative approaches may fail.

Method: Perturbative expansion around GR to first order in $α$, analytic calculations of scale factor and horizon formation corrections, and Einstein frame reformulation with numerical integration of ODEs governing scalaron dynamics and closed FLRW patches.

Result: Perturbative effects are minor in radiation-dominated PBH formation but significant in high-curvature early epochs; exact ODE system enables accurate computation of critical overdensity $δ_c(k)$ near inflation's end.

Conclusion: Quadratic $f(R)$ modifications influence PBH formation in high-curvature regimes, necessitating non-perturbative Einstein frame methods for precise threshold calculations during early Universe epochs.

Abstract: We present a complete analytic and semi-analytic study of gravitational collapse and primordial black hole (PBH) formation in the quadratic $f(R)$ model $f(R)=R+αR^2$. We first derive the perturbative expansion around General Relativity (GR), working to first order in the small parameter $α$. For a collapsing flat FLRW dust interior we compute the explicit first-order corrections to the scale factor, the stellar radius, and the horizon formation time. We then use these results to obtain the shift in the PBH formation threshold $δ_c$. The perturbative effect is small for PBHs forming in the deep radiation era, but becomes important when the background curvature is high. To access this early regime we reformulate the theory in the Einstein frame, where the model becomes GR plus the scalaron field $φ$ with the Starobinsky potential. We provide the complete ODE system governing both the cosmological background and the evolution of an overdense closed FLRW patch. This system can be numerically integrated to obtain the critical overdensity $δ_c(k)$ for PBH formation near the end of inflation.

</details>


### [13] [Resonances in b-EMRIs: playing the black hole piano](https://arxiv.org/abs/2601.02468)
*João S. Santos,Vitor Cardoso,Alexandru Lupsasca,José Natário,Maarten van de Meent*

Main category: gr-qc

TL;DR: The paper investigates how stellar-mass binaries near supermassive black holes (SMBHs) excite SMBH quasinormal modes through resonant interactions, finding frequency-dependent energy flux peaks offset from mode frequencies and orientation-dependent mode excitation, especially in Kerr black holes.


<details>
  <summary>Details</summary>
Motivation: Understand the dynamics of stellar binaries near SMBHs to explain potential gravitational-wave signals and astrophysical phenomena related to SMBH perturbations.

Method: Numerical simulations and analytical calculations to model resonant energy flux from binaries at various distances and orientations relative to SMBHs, comparing Schwarzschild and Kerr spacetimes.

Result: Energy flux peaks at frequencies near but not exactly the quasinormal mode frequencies; the offset increases with binary distance. For Kerr SMBHs, mode excitation is more complex but less damped, leading to stronger resonant responses.

Conclusion: Resonant interactions between binaries and SMBHs could be detectable via gravitational waves, with implications for observing SMBH mode excitations and understanding binary dynamics in strong gravitational fields.

Abstract: Stellar-mass binaries evolving in the vicinity of supermassive black holes (SMBHs) may be common in the universe, either in active galactic nuclei or in other astrophysical environments. Here, we study in detail the resonant excitation of SMBH modes driven by a nearby stellar-mass binary. The resulting resonant energy fluxes vary with the orbital location and frequency of the binary, exhibiting a rich and complex structure. In particular, we find that the total energy flux radiated to infinity is maximized at a gravitational-wave frequency that is close to, but not exactly equal to, the real part of the corresponding quasinormal-mode frequency. Moreover, as the binary is moved farther away from the SMBH, this offset from the mode frequency becomes increasingly pronounced. In addition, for suitable orientations, the binary can effectively ``feed'' the light ring of the SMBH, selectively exciting particular oscillation modes. For rotating (Kerr) black holes, the mode spectrum is significantly more intricate; however, individual modes are also less strongly damped, leading to an enhanced - but more difficult to interpret - resonant response.

</details>


### [14] [Saving the Unruh Signal: Coherent Cancellation of Spontaneous Emission with Entangled Detectors](https://arxiv.org/abs/2601.02490)
*Arash Azizi*

Main category: gr-qc

TL;DR: The paper presents a method to overcome the Wigner-Weisskopf spontaneous emission noise in detecting the Unruh effect using entangled detectors in a W-state, enabling noise cancellation through quantum interference.


<details>
  <summary>Details</summary>
Motivation: The Unruh effect is hard to detect due to being overwhelmed by spontaneous emission noise. The goal is to find a way to suppress this noise to make the Unruh signal observable.

Method: The authors use three entangled Unruh-DeWitt detectors in a W-state. By engineering their entanglement, they cause destructive interference of spontaneous emission channels, thus canceling the dominant noise. The key is deriving conditions based on geometric constraints involving the detectors' acceleration parameters.

Result: They derived conditions for canceling all emission modes via specific detector entanglement states. The required state has real coefficients determined by a ratio of sine functions of logarithms of detector accelerations.

Conclusion: This approach establishes multi-detector entanglement as a precision tool for noise reduction in relativistic quantum systems, providing a potential pathway for experimental observation of the Unruh effect.

Abstract: The Unruh effect is notoriously difficult to detect, as it is exponentially overwhelmed by Wigner--Weisskopf spontaneous emission. We show that this fundamental obstacle can be overcome by harnessing multi-detector quantum interference. By preparing a system of three entangled Unruh--DeWitt detectors in a specific W-state, the spontaneous emission channels can be forced to destructively interfere and vanish, thereby "saving" the Unruh signal by coherently silencing this dominant noise. Our central result is the derivation of the condition for complete and simultaneous cancellation of all right- and left-traveling emission modes. We find this requires preparing the detectors in a unique entangled state whose real-valued coefficients are fixed by an elegant geometric constraint, given by a ratio of sines of the logarithms of the detector accelerations. This work establishes multi-detector entanglement as a precision tool for noise cancellation in relativistic quantum settings, offering a new pathway toward the definitive observation of the Unruh signal.

</details>


### [15] [Hints Beyond $Λ$CDM from Barrow and Tsallis Holographic Dark Energy with GO cutoff](https://arxiv.org/abs/2601.02567)
*G. G. Luciano,A. Paliathanasis,G. Leon,A. Sheykhi,M. Motaghi*

Main category: gr-qc

TL;DR: The paper investigates Barrow and Tsallis Holographic Dark Energy models with the Granda-Oliveros IR cutoff, showing they are compatible with observations and can slightly outperform ΛCDM in some cases, highlighting the importance of generalized entropy approaches in cosmology.


<details>
  <summary>Details</summary>
Motivation: To explore the cosmological implications of entropy-corrected HDE frameworks (Barrow and Tsallis) using the GO IR cutoff and assess their competitiveness against ΛCDM.

Method: Derive modified Friedmann equations from the thermodynamic-gravity conjecture, simulate background evolution in non-interacting/interacting dark sector scenarios, and constrain parameters using PantheonPlus, Union3 supernovae, Cosmic Chronometers, and DESI DR2 BAO data via Bayesian MCMC.

Result: BHDE models are observationally viable, showing mild preference over ΛCDM in certain data combinations, demonstrating the potential of generalized entropy frameworks in explaining late-time universe dynamics.

Conclusion: Barrow-Tsallis HDE with GO cutoff emerges as a viable alternative to ΛCDM, emphasizing the role of entropy corrections in cosmological modeling and motivating further observational tests.

Abstract: Barrow and Tsallis Holographic Dark Energy (HDE) are two recent extensions of the standard HDE framework, obtained by introducing generalized entropy corrections through the Barrow and Tsallis formalisms. In this work, we examine the cosmological consequences of Barrow and Tsallis HDE implemented with the Granda-Oliveros (GO) infrared (IR) cutoff. After deriving the modified Friedmann equations within the thermodynamic-gravity conjecture, we study the background evolution in both non-interacting and interacting dark sector scenarios, emphasizing the role of the entropic parameter in shaping late-time dynamics. We then confront the model with state-of-the-art observations, including PantheonPlus and Union3 Type Ia supernovae, Cosmic Chronometers and DESI DR2 BAO measurements. Using Bayesian MCMC methods, we constrain the model parameters and compare the performance of BHDE with that of $Λ$CDM. Our results show that BHDE is compatible with current data and can exhibit a mild statistical preference over the concordance model for certain dataset combinations. Overall, the analysis underscores the relevance of generalized entropy frameworks in late-time cosmology and identifies Barrow-Tsallis holography with the GO cutoff as a competitive alternative to $Λ$CDM.

</details>


### [16] [Exploring Lorentz Violation in Spacetime through Universal Finsler Geometry](https://arxiv.org/abs/2601.02711)
*Jie Zhu,Hao Li,Bo-Qiang Ma*

Main category: gr-qc

TL;DR: The paper proposes that Finsler structure is an intrinsic property of the universe, leading to new dispersion relations where Lorentz violation scales with particle mass and aligns with observations in photons, neutrinos, and electrons.


<details>
  <summary>Details</summary>
Motivation: To extend Riemannian geometry through Finsler geometry for studying Lorentz violation, moving beyond previous approaches that treated Finsler structures as particle-specific.

Method: Deriving modified dispersion relations under the assumption that Finsler structure is a universal intrinsic property, then analyzing their implications.

Result: Derived new dispersion relations showing Lorentz violation scales proportionally with particle mass; results match phenomenological data for photons, neutrinos, and electrons.

Conclusion: Finsler structure as a universal property provides a consistent framework for understanding Lorentz violation across different particles.

Abstract: Finsler geometry serves as a fundamental and natural extension of Riemannian geometry, providing a valuable framework for investigating Lorentz violation in spacetime. Previous studies have treated the Finsler structures associated with different particles as distinct entities. In this paper, we propose a novel hypothesis suggesting that the Finsler structure may represent an intrinsic property of the universe itself. Under this assumption, we derive a series of modified dispersion relations that have not been previously explored, and we analyze their implications. Our findings indicate that the scales of Lorentz violation for massive particles are proportional to their masses. Furthermore, we demonstrate that this hypothesis aligns well with existing phenomenological results regarding Lorentz violation observed in photons, neutrinos, and electrons.

</details>


### [17] [Utilizing anticoincidence veto in a search for gravitational-wave transients](https://arxiv.org/abs/2601.02713)
*Souradeep Pal*

Main category: gr-qc

TL;DR: The paper introduces a veto technique using temporal anticoincidence to suppress noise transients in gravitational-wave detectors, improving search sensitivity for binary black hole signals, particularly for short-duration transients.


<details>
  <summary>Details</summary>
Motivation: To reduce spurious terrestrial-origin noise affecting gravitational-wave searches and improve detection efficiency for short transient signals.

Method: A veto technique is applied based on the non-coincidence of noise transients in geographically separated detectors. Abnormally loud non-coincident triggers are rejected in matched-filter searches for binary black holes.

Result: The method reduces search backgrounds close to Gaussian limits and improves sensitivity when tested with simulated signals.

Conclusion: The technique enhances detection efficiency for short-duration gravitational-wave signals by effectively suppressing non-coincident noise disturbances.

Abstract: We devise a technique to suppress the effect of noise transients occurring at gravitational-wave detectors based on temporal anticoincidence. Searches for gravitational-wave signals in the detector data are prone to spurious disturbances of terrestrial origin. The technique presented here benefits from the fact that such effects are generally non-coincident in time at geographically separated detectors. Therefore, abnormally loud triggers that are not time-coincident can be vetoed. We implement the veto technique in a matched-filter search for transient signals from binary black holes and observe search backgrounds to be generally close to the Gaussian limit. An improvement in the sensitivity of the search is demonstrated using simulated signals. The technique is expected to especially improve the detection efficiency of the search toward short duration transient signals.

</details>


### [18] [ModMax Black Holes in 4-Dimensional Einstein-Gauss-Bonnet Gravity](https://arxiv.org/abs/2601.02717)
*Bilel Hamil*

Main category: gr-qc

TL;DR: The paper investigates charged black holes in a 4-dimensional Einstein-Gauss-Bonnet gravity combined with ModMax electrodynamics, uncovering modified spacetime properties, stable remnants, and observational signatures.


<details>
  <summary>Details</summary>
Motivation: To explore how higher curvature terms and nonlinear electromagnetism alter black hole structure and thermodynamics, offering insights into dark-sector physics and evaporation scenarios beyond standard gravity.

Method: Derived exact black hole solutions with spherically symmetric, charged ModMax fields in regularized 4D Einstein-Gauss-Bonnet gravity. Analyzed particle motion, quasinormal modes via WKB/Padé/Pöschl-Teller methods.

Result: Found minimum mass black holes with stable remnants, modified orbit properties, and quasinormal modes sensitive to Gauss-Bonnet and ModMax parameters. Confirmed linear stability.

Conclusion: These modifications provide testable signatures for dark-sector theories, suggesting gravitational/electromagnetic nonlinearity could explain compact object behavior without dark matter.

Abstract: In this paper, we study charged black hole solutions in 4-dimensional Einstein-Gauss-Bonnet gravity combined with ModMax nonlinear electrodynamics. This is a conformally invariant extension of Maxwell theory that effectively describes nonlinear electromagnetic phenomena. Within the regularized 4-dimensional Gauss-Bonnet framework, we derive an exact static and spherically symmetric black hole solution that is sourced by a purely electric ModMax field. We explore how higher curvature corrections and nonlinear electromagnetic effects change the spacetime geometry, horizon structure, and energy content of the black hole. We examine the thermodynamic properties in detail, revealing a minimum mass and stable black hole remnants. These findings might be significant in scenarios involving dark-sector compact objects or evaporation endpoints beyond standard general relativity. We also investigate the motion of massive particles, discussing the characteristics of circular orbits and the innermost stable circular orbit, highlighting differences from the Maxwell case. Additionally, we analyze the quasinormal modes of a massive scalar field using WKB methods with Padé approximants and the Pöschl-Teller approximation. We explore how the quasinormal spectrum depends on the Gauss-Bonnet coupling, the ModMax parameter, and the scalar field mass. Our results confirm the linear stability of the black hole and offer potential observational signatures of dark-sector inspired modifications of gravity and electrodynamics.

</details>


### [19] [An Interior model of Charged Fluid Spheres](https://arxiv.org/abs/2601.02782)
*Naren Babu O.,Hemalatha. R,Narayanankutty Karuppath,Sabu M. C*

Main category: gr-qc

TL;DR: The paper presents an exact solution to Einstein's field equations for a static, spherically symmetric superdense star using the Vaidya-Tikekar metric, expressed via a hypergeometric series. It models a spheroidal configuration under constant density parameter, satisfying energy conditions and hydrostatic equilibrium.


<details>
  <summary>Details</summary>
Motivation: To explore exact solutions describing superdense stars (like neutron stars) with spheroidal geometry, addressing limitations of previously known spherical models and providing physically valid metrics.

Method: Analytical derivation using the Vaidya-Tikekar metric with a hypergeometric series solution. Parameters are chosen based on physical constraints (stellar density ρ_a = 2×10¹⁴ g/cm³), varying mass and radius through density variation parameters while maintaining total mass and boundary radius consistent with neutron stars.

Result: Derivation of a new class of exact solutions modeling spheroidal stars, confirmed to satisfy hydrostatic equilibrium and all energy conditions (null, weak, strong). Solutions maintain physically realistic properties across parameter variations.

Conclusion: The solution offers a viable model for relativistic superdense stars with spheroidal configurations, extending neutron star models beyond spherical symmetry while preserving essential physical requirements.

Abstract: At constant time $t$, we examine the Vaidya-Tikekar metric characterising a three-dimensional, extremely dense spheroidal star configuration. The static, spherically symmetric solution of Einstein's field equations can be expressed in analytic closed form utilising a hypergeometric series. A relativistic, superdense state of matter at a constant $t$ is represented by the resultant model, which describes the geometry of a three-spheroid.
  Assuming a stellar density of $ρ_{a}= 2*10^{14} gm*cm^{-3}$, we investigate configurations whose total mass and radius vary over a range of well-defined values of the density variation parameter. Similar to an uncharged neutron star, all models possess the same total mass and boundary radius. The hypergeometric solution leads to a new class of exact, physically acceptable solutions. We show that the model satisfies the conditions of hydrostatic equilibrium and fulfils all standard energy conditions, which are verified throughout the analysis.

</details>


### [20] [A Precise Ultra - Dense Star Model on Spheroidal Space-Time](https://arxiv.org/abs/2601.02815)
*Hemalatha R,Naren Babu O.,Narayanankutty Karuppath,Sabu M. C*

Main category: gr-qc

TL;DR: This paper presents a model of a relativistic superdense star using a three-spheroid configuration with constant t₁, solving Einstein's equations analytically. The star's mass and radius are comparable to neutron stars, with stability requiring a density ratio λ=0.4.


<details>
  <summary>Details</summary>
Motivation: To model the interior geometry of relativistic superdense stars and analyze their stability under radial perturbations using a novel three-spheroid structure.

Method: An analytical closed-form solution to Einstein's field equations is derived for a static, spherically symmetric three-spheroid model. The solutions satisfy energy conditions and hydrostatic equilibrium. The parameter λ (ratio of characteristic density to central density) is crucial for the mass-radius relationship.

Result: The model produces stellar configurations with mass and radius similar to neutron stars. Stability analysis shows λ must be 0.4 for dynamic stability. New exact solutions of the differential equations are validated.

Conclusion: The three-spheroid model is viable for describing relativistic superdense stars, with the density ratio λ determining stability and structural parameters. This contributes new insights into compact star models in general relativity.

Abstract: This study presents a static, spherically symmetric configuration in which the interior geometry of a relativistic superdense star is modeled as a three-spheroid with constant $t_1$. The model is constructed using an analytical closed-form solution to Einstein's field equations. Assuming a characteristic density of $ρ_{a}= 2\times10^{14} \mathrm{g\,cm^{-3}}$, we compute the total mass and radius of the star based on a prescribed set of structural parameters that influence the density profile. The resulting stellar configurations exhibit boundary radii and total masses comparable to those of neutron stars with vanishing charge density.
  New exact solutions are obtained by solving the relevant second-order ordinary differential equations. We demonstrate that these solutions satisfy the standard energy conditions and maintain hydrostatic equilibrium throughout the stellar interior. All physical requirements remain valid at every point within the configuration. In this framework, the parameter $λ=\frac{ρ_{a}}{ρ_{0}}$ serves as a key determinant of the mass--radius relationship. We further assess the suitability of the model for representing a relativistic superdense star and analyze its stability under radial perturbations. The investigation indicates that, for the configuration to remain dynamically stable, the density ratio between the outer and inner regions must take the value $λ= 0.4$.

</details>


### [21] [Probing the nature of Einstein nonlinear Maxwell Yukawa black hole through gravitational wave forms from periodic orbits and quasiperiodic oscillations](https://arxiv.org/abs/2601.02904)
*Oreeda Shabbir,Abubakir Shermatov,Bushra Majeed,Tehreem Zahra,Mubasher Jamil,Javlon Rayimbaev*

Main category: gr-qc

TL;DR: The paper investigates gravitational wave emission from test particle orbits around a specific black hole model (Einstein nonlinear Maxwell Yukawa) to constrain its parameters using observational data from microquasars and the galactic center.


<details>
  <summary>Details</summary>
Motivation: To understand gravitational wave signals from periodic orbits, analyze quasi-periodic oscillations, and constrain parameters of a modified black hole model incorporating Yukawa screening and electric charge.

Method: The Hamiltonian approach derives equations of motion; effective potential analysis determines stable and bound orbits. Orbits classified by triplets showing zoom-whirl behavior. Gravitational wave signals are computed in both polarizations. MCMC simulations constrain black hole parameters using data from four microquasars and the galactic center.

Result: Parameters of the ENLMY BH are constrained based on observed quasi-periodic oscillations in X-ray binaries. Yukawa screening and charge effects on orbital stability and energy are quantified. MCMC results identify allowable parameter ranges consistent with observational data.

Conclusion: The study demonstrates how observational data from relativistic systems can test modified black hole models through gravitational wave analysis and orbital dynamics, offering constraints on fundamental physics parameters like Yukawa screening length and electric charge.

Abstract: In this work, we study gravitational wave emission from periodic orbits of test particles, analyze quasi periodic oscillations, and constrain the parameters of the static, spherically symmetric Einstein nonlinear Maxwell Yukawa black hole. Using the Hamiltonian approach, we calculate the equations of motion of the particles. We analyze the effective potential to determine the innermost stable circular orbit and innermost bound circular orbit, illustrating how the Yukawa screening parameter and electric charge Q affect orbital stability and energy requirements. Periodic orbits are classified by integer triplets and exhibit characteristic zoom whirl behavior. Based on these orbits we compute the corresponding GW signals in both the polarizations. Finally, we perform Monte Carlo Markov Chain MCMC simulations to constrain the parameters of the ENLMY BH for four microquasars and the galactic center within the relativistic precession model.

</details>


### [22] [Does relativistic motion really freeze initially maximal entanglement?](https://arxiv.org/abs/2601.02976)
*Si-Han Li,Hui-Chen Yang,Rui-Yang Xu,Shu-Min Wu*

Main category: gr-qc

TL;DR: This paper demonstrates that the 1-3 bipartite entanglement in a four-qubit cluster state remains maximally entangled under any acceleration, including infinite acceleration, challenging the assumption that relativistic motion universally degrades entanglement.


<details>
  <summary>Details</summary>
Motivation: To investigate whether relativistic motion (Unruh effect) necessarily degrades maximal quantum entanglement, and to explore its implications for quantum information in non-inertial or curved spacetime.

Method: Used a four-qubit cluster (CL4) state and a fully operational Unruh-DeWitt detector framework to analyze relativistic dynamics of quantum entanglement.

Result: The 1-3 bipartite entanglement remained strictly maximal for all accelerations, revealing a 'complete freezing' phenomenon where maximal entanglement is preserved despite acceleration.

Conclusion: The findings overturn the conventional view, showing that some entanglement can be preserved under extreme relativistic conditions, making the CL4 state a valuable resource for quantum information processing in non-inertial settings.

Abstract: We investigate the relativistic dynamics of quantum entanglement in a four-qubit cluster ($CL_4$) state using a fully operational Unruh-DeWitt detector framework. Contrary to the widely held expectation that the Unruh effect inevitably degrades initially maximal entanglement, we demonstrate that the 1-3 bipartite entanglement of the $CL_4$ state remains strictly maximal for all accelerations, including the infinite-acceleration limit. This result uncovers a previously unexplored phenomenon, namely the ``complete freezing of initially maximal entanglement" under relativistic motion. To the best of our knowledge, this is the first identification and systematic characterization of such a phenomenon within a relativistic framework. These findings overturn the conventional view that acceleration universally diminishes maximal entanglement and establish the $CL_4$ state as a promising resource for quantum information processing in non-inertial or curved-spacetime settings.

</details>


### [23] [Revisiting Spherically Symmetric Spacetime I: Geometro-Hydrodynamics](https://arxiv.org/abs/2601.03077)
*Puttarak Jai-akson,Yuki Yokokura*

Main category: gr-qc

TL;DR: The paper explores the thermodynamics of spherically symmetric spacetimes by analyzing their geometry and dynamics through a foliation approach, deriving geometro-hydrodynamic analogs of fluid equations and linking them to Misner-Sharp energy and Lovelock gravity.


<details>
  <summary>Details</summary>
Motivation: To re-express gravitational dynamics in terms of hydrodynamical variables to uncover thermodynamic properties inherent in spacetime geometry and dynamics.

Method: Using the rigging technique to construct foliation-adapted frames, deriving frame transformations that map Einstein equations into hydrodynamic form, analyzing apparent horizon dynamics, and extending results to Lovelock gravity.

Result: Demonstrated equivalence between gravitational dynamics and geometro-hydrodynamic equations (Euler/Young-Laplace analogs), identification of spacetime as 'gravitational bubbles' with Misner-Sharp energy and geometric pressure, and validation in Lovelock gravity.

Conclusion: Establishes a new framework connecting spacetime dynamics to thermodynamics through geometro-hydrodynamics, setting a foundation for future thermodynamic interpretations of gravitational systems.

Abstract: This series of works revisits the geometry, dynamics, and covariant phase space of spherically symmetric spacetimes with the aim of exploring the thermodynamics of spacetime from their dynamical properties. In this first paper, we examine the geometry from the perspective of a foliation by spherical hypersurfaces. Using the rigging technique, we first define a local frame adapted to these slices and reconstruct the geometry and dynamics fully. We clarify the connection of the frame adapted to constant-radius slices, to the Kodama vector and Misner-Sharp energy. Through frame transformations, we then show that the gravitational dynamics in a general foliation-adapted frame can be interpreted as hydrodynamics, i.e., geometro-hydrodynamics: the Einstein equations exhibit the gravitational analogs of the Euler and Young-Laplace equations, and the spacetime can be viewed as the worldvolume of a concentric stack of "gravitational bubbles" -- spherical collective modes with the Misner-Sharp energy density and a geometric pressure. We apply this framework to apparent horizons and study the dynamics. Finally, we demonstrate that a similar geometro-hydrodynamic picture holds in Lovelock gravity. These results provide a fresh perspective on this class of spacetimes and lay the foundation for understanding their thermodynamic properties.

</details>


### [24] [Disturbing moving fluids](https://arxiv.org/abs/2601.03081)
*Lorenzo Gavassino*

Main category: gr-qc

TL;DR: This paper demonstrates that under an Onsager-like symmetry principle, Lorentz invariant bounds can be placed on various properties of a fluid's excitation spectrum across inertial frames using only rest frame data, challenging classical time dilation intuition except in non-relativistic scenarios.


<details>
  <summary>Details</summary>
Motivation: To address how relativistic effects influence fluid excitation spectra measurements across different inertial frames and establish Lorentz invariant spectral constraints.

Method: Applying Onsager-like symmetry principles from kinetic theory and hydrodynamics to derive rigorous bounds on phase velocities, convergence radii, spectral gaps, and equilibration rates across inertial frames.

Result: Shows that rest frame zero wavenumber data suffice for bounding spectral properties in all frames, disproving common time dilation-based predictions except under non-relativistic conditions.

Conclusion: Relativistic fluid spectral properties exhibit invariances and bounds beyond naive time dilation effects, requiring symmetry-based analysis rather than classical intuition.

Abstract: In Newtonian physics, the excitation spectrum of a fluid is the same in all reference frames, up to a trivial shift. In special relativity, this is no longer the case. Relativity of simultaneity causes different inertial observers to measure markedly different excitation spectra, with stability being the only property known to be Lorentz invariant in all causal theories. Here, we show that, under a certain Onsager-like symmetry principle (which applies to kinetic theory and transient hydrodynamics), it is possible to place rigorous bounds on phase velocities, eigenmode convergence radii, spectral gaps, and equilibration rates in any inertial frame, using only information about the rest frame spectrum at zero wavenumber. The conventional intuition coming from time dilation is also shown to lead to generically wrong predictions, but becomes accurate if the fluid is non-relativistic in the rest frame.

</details>


### [25] [Epicyclic motion and accretion disk around a charged black hole in Einstein-ModMax theory with a quintessence field](https://arxiv.org/abs/2601.03088)
*Hamza Rehman,Sanjar Shaymatov,Tao Zhu*

Main category: gr-qc

TL;DR: The paper examines the epicyclic motion and quasi-periodic oscillations (QPOs) of charged particles around a weakly magnetized black hole with quintessence using Einstein-ModMax theory. Analyzing parameters like ModMax coupling and dyonic charge, it finds tight constraints on black hole properties from QPO data, supporting general relativity with minor deviations.


<details>
  <summary>Details</summary>
Motivation: To understand how nonlinear electrodynamics and quintessence affect QPOs in X-ray binaries and constrain black hole parameters, testing Einstein-ModMax theory against observations.

Method: Derived epicyclic frequencies for charged particles, applied forced resonance model for QPOs, used MCMC analysis to constrain parameters, and studied accretion disk radiative properties.

Result: QPO observations tightly constrain black hole mass, orbital radius, and upper limits for ModMax coupling, magnetic interaction, and dyonic charge. Results align with general relativity allowing minor Einstein-Maxwell deviations in quintessence.

Conclusion: Einstein-ModMax theory with weak magnetization and quintessence is consistent with observations, showing that QPOs can constrain alternative gravity models and the role of exotic matter fields.

Abstract: We investigate the epicyclic motion of charged test particles and the associated quasi-periodic oscillations (QPOs) around a weakly magnetized black hole surrounded by quintessence within the framework of Einstein-ModMax theory. We analyze the dynamics of charged particles on circular orbits and derive the corresponding radial and vertical epicyclic frequencies. The influence of the nonlinear electrodynamics parameter, magnetic coupling, dyonic charge, and quintessence state parameter on the innermost stable circular orbit and epicyclic frequencies is examined in detail. Using the forced resonance model, we compare the theoretical predictions of high-frequency QPOs with observational data from several X-ray binary systems. A Markov Chain Monte Carlo analysis is employed to constrain the black hole parameters and assess the role of weak magnetization and nonlinear electrodynamics effects. This analysis indicates that QPO observations tightly constrain the black hole mass and orbital radius while placing stringent upper bounds on the ModMax coupling, magnetic interaction, and dyonic charge. In addition, we study the radiative properties of the accretion disk and analyze the effects of the model parameters on the disk flux and temperature profiles. These findings suggest that the observed QPOs are consistent with general relativity in the strong-field regime, allowing only small deviations associated with Einstein-Maxwell theory in the presence of a quintessence field.

</details>


### [26] [A Bayesian Statistical Study of Bianchi Type-I Universe in $f(R,T^ψ)$ Modified Gravity](https://arxiv.org/abs/2601.03116)
*Mohit Thakre,Praveen Kumar Dhankar,Safiqul Islam,Parbati Sahoo,Farook Rahaman,Behnam Pourhassan*

Main category: gr-qc

TL;DR: The paper investigates the cosmological dynamics of a Bianchi type-I universe in f(R,Tψ) gravity using Bayesian methods and observational data, showing strong alignment with observations.


<details>
  <summary>Details</summary>
Motivation: To explore the viability of anisotropic cosmologies under extended matter-geometry couplings and understand late-time cosmic acceleration through a LRS Bianchi type-I model.

Method: Estimated Hubble parameter, effective equation of state, and scalar field potential using H = W(ψ); applied Bayesian statistics and MCMC with OHD, BAO, and Pantheon datasets to constrain parameters.

Result: The model aligns well with observational data, showing statistical stability, consistency in describing cosmic acceleration, and compatibility with ΛCDM in expansion history.

Conclusion: The f(R,Tψ) gravity framework, with LRS Bianchi type-I cosmology, provides a viable and accurate description of the universe's expansion, supported by observational constraints.

Abstract: We have examined the cosmological actions of LRS (Locally Rationally Symmetric) Bianchi type-I universe model in $f(R,T^ψ)$ gravity. For this, we have estimated the Hubble parameter, the effective equation of state parameter ($ω^{eff}$), and the potential of the scalar field as a function of time using equation $H = W(ψ)$. The graphical representation of the potential function $V(ψ)$ with respect to cosmic time t is described. This study explores the dynamical properties of a Bianchi Type-I universe by utilizing Bayesian statistical techniques to constrain the model parameters and evaluate the viability of anisotropic cosmology under extended matter-geometry couplings. Also, we have applied the Markov Chain Monte Carlo (MCMC) mechanism on the derived $H(z)$ model by using observational Hubble data (OHD), the Baryon Acoustic Oscillation (BAO) dataset, and the Pantheon dataset. From the confidence-level contours and best-fit parameter values obtained, along with the corresponding reduced $χ^{2}$, it is evident that the model aligns strongly with observational data, demonstrating statistical stability and consistency in describing late-time cosmic acceleration. Likewise, the error analyses presented in this research, including a comparison between the $Λ$CDM cosmology and the reconstructed $H(z)$ model, confirm the model's compatibility with current observations by yielding a reliable and accurate account of the universe's expansion history.

</details>


### [27] [Wave optical imaging by point-source scattering for a TNdS black hole](https://arxiv.org/abs/2601.03118)
*Felix Willenborg*

Main category: gr-qc

TL;DR: The paper calculates the exact solution of Teukolsky equation in a Taub-NUT-de Sitter spacetime, analyzing wave scattering from monochromatic sources and imaging with single/multiple sources.


<details>
  <summary>Details</summary>
Motivation: To study the accurate dynamics of gravitational waves in Taub-NUT-de Sitter spacetime beyond approximate methods.

Method: Solving the exact Teukolsky equation without approximation for the specified spacetime; analyzing scattering from monochromatic sources and imaging techniques with single/multiple source configurations.

Result: Demonstrates how spacetime parameters affect wave scattering patterns and image formation through wave optics, showcasing the importance of exact calculations over approximations.

Conclusion: Exact solutions provide critical insights into black hole physics and spacetime structures, indicating that traditional approximations may miss subtle but significant phenomena in gravitational wave observations.

Abstract: In our work, we present a calculation based on an exact, non-approximated wave equation, as described by the Teukolsky equation, for a Taub-NUT-de Sitter spacetime. We observe the scattering of a monochromatic point source by an observer located at a greater distance from the black hole, and examine the wave-optical images by a single source and by multiple sources.

</details>


### [28] [When does entanglement through gravity imply gravitons?](https://arxiv.org/abs/2601.03214)
*Nikolaos Mitrakos,Maria Papageorgiou,T. Rick Perche,Marios Christodoulou*

Main category: gr-qc

TL;DR: The paper critically examines a thought experiment suggesting that gravitons exist based on entanglement via Newtonian potential. It finds that neglecting quantum fluctuations can either violate complementarity or no-signalling, depending on the approximation method. It argues that the experiment doesn't strengthen the case for entanglement through Newtonian gravity but supports gravitons if gravitational retardation is observed.


<details>
  <summary>Details</summary>
Motivation: To evaluate the validity of a thought experiment claiming that entanglement through Newtonian potential implies gravitons by resolving a paradox involving complementarity and causality.

Method: Uses scalar field models to analyze how different approximations of neglecting quantum fluctuations affect the violation of complementarity or no-signalling. Distinguishes between Newtonian action-at-a-distance and quantum no-signalling in causality.

Result: Shows violations depend on approximation methods, with entanglement generated locally. The thought experiment doesn't enhance the epistemological relevance of Newtonian-based entanglement but supports gravitons with detected gravitational retardation.

Conclusion: The experiment isn't sufficient for epistemological support of entanglement via Newtonian gravity but could confirm gravitons if retarded gravitational effects are found in such entanglement.

Abstract: Detection of entanglement through the Newtonian potential has been claimed to support the existence of gravitons, by extrapolating to a thought experiment which demonstrates that complementarity and causality would be in conflict unless quantum fluctuations exist. We critically assess this consistency argument using scalar field models. We show that whether complementarity or no-signalling is violated when quantum fluctuations are neglected, depends on how this approximation is taken, while in both cases entanglement is generated locally in spacetime. We clarify that the correct reading of the paradox requires making a clear distinction between two notions of causality violation: Newtonian action-at-a-distance and the quantum mechanical no-signalling; the latter is pertinent while the former is not. We conclude that the thought experiment (a) does not add to the epistemological relevance of entanglement through Newtonian potentials (b) lends support for the existence of gravitons, if retardation effects are detected in entanglement through gravity.

</details>


### [29] [Discrete gravitational diagram technique in the soft synchronous gauge](https://arxiv.org/abs/2601.03228)
*V. M. Khatsymovsky*

Main category: gr-qc

TL;DR: This paper explores the use of Regge calculus in establishing an optimal edge length scale through a perturbative expansion, utilizing a hypercubic structure and a specific gauge to ensure the measure's stability and analyticity.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of determining a consistent edge length scale in Regge calculus and to ensure the perturbative expansion converges without uncontrolled η dependence.

Method: 1. Employs a hypercubic structure with metric variables fixed at sites.
2. Identifies the edge length scale as the maximum of the measure’s bell-shaped form (∼η^(1/2)), requiring η to be large.
3. Applies a discrete soft synchronous gauge and finite-difference action to stabilize the measure and match continuum propagator properties.

Result: Demonstrates that perturbative expansions remain stable (no growing η terms) near the measure's maximum point, but instability occurs outside this region, dynamically fixing the edge length scale.

Conclusion: The proposed framework dynamically establishes a viable edge length scale via gauge fixing and careful regularization, bridging discrete Regge calculus with continuum physics properties.

Abstract: This paper develops our work on the consequences of the Regge calculus, where some edge length scale arises as an optimal starting point of the perturbative expansion with taking into account a bell-shaped form of the measure obtained using functional integration over connection.
  A "hypercubic" structure is considered (some variables are frozen), it is described by the metric $g_{λμ}$ at the sites.
  The edge length scale as some maximum point of the measure is $\sim η^{1 / 2}$, where $η$ defines the free factor like $ ( - \det \| g_{λμ} \| )^{ η/ 2}$ in the measure and should be a large parameter to ensure true action upon integration over connection. A priori, the perturbative expansion may contain increasing powers of $η$, but this does not happen for the starting point inside some neighborhood of the maximum point of the measure, and it does happen outside this neighborhood. This appears to be a dynamic mechanism for establishing the edge length scale.
  We use a discrete version of the soft synchronous gauge in the principal value type prescription we discuss in a recent paper arXiv:2601.02181. This allows one to fix the timelike length scale at a low level for which the measure is known in closed form. This gauge is considered together with a refined finite-difference form of the action to match the analytical properties of the propagator to the continuum case.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [30] [Research of the Behavior of the Effective Potential in Systems with Phase Transitions through the Prism of A--D--E Type Singularities](https://arxiv.org/abs/2601.02425)
*T. V. Obikhod*

Main category: hep-ph

TL;DR: The paper proposes analyzing the electroweak phase transition via the critical manifold in a scalar singlet Higgs portal model, using topological stability and catastrophe theory to extend beyond traditional methods. Future colliders and LISA will scrutinize the parameter space, ensuring discovery or exclusion of the singlet.


<details>
  <summary>Details</summary>
Motivation: Current particle detection strategies are insufficient for a scalar singlet interacting via the Higgs portal. The study aims to assess the electroweak phase transition's characteristics (presence, intensity, first-order nature) through the critical manifold's Milnor number (μ=9), ensuring topological stability under parameter variations. This approach addresses limitations of mass matrix analyses.

Method: The analysis focuses on the effective potential's landscape, considering the portal potential's non-simple singularity with μ=9. Key parameters include the mixing angle, singlet mass, and cubic interactions. High-precision measurements of Higgs trilinear self-coupling (κ_λ), Higgs coupling rescaling (c_H), and gravitational-wave spectrum (Ω_GW) are used to probe the disaster. Projections for collider experiments (2027-2040) and LISA data are integrated to map out the parameter space.

Result: The scalar singlet's parameter space consistent with experiments shows topological stability (μ=9). Future collider and LISA capabilities will examine all viable regions supporting a strong first-order transition, ensuring either detection of the singlet or exclusion based on vacuum critical structure analysis.

Conclusion: Direct interrogation of the electroweak vacuum's critical structure through improved Higgs coupling measurements and gravitational-wave observations will resolve the singlet's existence. The proposed methods transcends traditional mass matrix approaches, offering a robust framework for testing beyond Standard Model physics.

Abstract: Detecting a scalar singlet interacting through the Higgs portal demands a pivot from conventional particle detection strategies to a comprehensive examination of the effective potential's landscape. The presence, intensity, and first-order nature of the electroweak phase transition are dictated by the critical manifold, with its universal traits encapsulated in the Milnor number $μ$ -- the dimensionality of the local Jacobian algebra. Throughout the parameter space consistent with experimental observations, the portal potential exhibits a non-simple singularity with $μ= 9$, maintaining topological stability amid substantial fluctuations in mixing angle, singlet mass, and cubic interactions. High-precision assessments of the Higgs trilinear self-coupling ($κ_λ$), the uniform rescaling of Higgs couplings ($c_H$), and the stochastic gravitational-wave spectrum ($Ω_{\mathrm{GW}}$) collectively delineate the catastrophe, extending beyond mere mass matrix analysis. Projections for 2027--2040 collider and LISA capabilities indicate that no viable region supporting a strong first-order transition will evade scrutiny; thus, the singlet will either be identified or conclusively dismissed via direct interrogation of the electroweak vacuum's critical structure.

</details>


### [31] [On the Stability of Leading-Power Factorization under Photon Propagator Numerator Modifications](https://arxiv.org/abs/2601.02426)
*Cong Li*

Main category: hep-ph

TL;DR: The paper examines collinear factorization in strong electromagnetic fields using Soft-Collinear Effective Theory (SCET), showing that the leading-power factorization form remains intact and that background effects only appear beyond leading power when using a specific modification.


<details>
  <summary>Details</summary>
Motivation: To understand how strong electromagnetic backgrounds affect collinear factorization in SCET, particularly focusing on the preservation of leading-power results and determining the sensitivity of the cusp kernel to such backgrounds.

Method: Analyze momentum regions and Landau diagrams to identify the leading contributions. Derive the dependence of the cusp kernel on the background via the numerator tensor Δ_{μν}(k), considering transversality conditions for specific modifications like the occupancy-number case with $g_{Tμν}(k)$.

Result: The leading-power factorization structure and pinch surfaces remain unchanged. The LP cusp kernel depends on $n^μar n^νΔ_{μν}(k)$; if this contraction vanishes (e.g., due to transversality), background effects are absent at LP.

Conclusion: Electromagnetic backgrounds only significantly impact the factorization formalism beyond leading power when specific tensor conditions are unfulfilled, implying that standard SCET results hold at leading order under these modifications.

Abstract: We study collinear factorization in strong electromagnetic backgrounds within SCET for a class of modifications where the photon propagator keeps the vacuum pole structure and $i\varepsilon$ prescription, while the background enters only through a numerator tensor $Δ_{μν}(k)$. We show that the set of Landau pinch surfaces and leading momentum regions is unchanged, so the leading-power (LP) factorized form is preserved. Moreover, the LP cusp kernel depends on the background solely through the longitudinal contraction $n^μ\bar n^νΔ_{μν}(k)$ in the soft region; if it vanishes (or is power suppressed), the LP soft kernel reduces to the vacuum. As an application, for an occupancy-number modification with the physical polarization-sum tensor $g_{Tμν}(k;n)$, transversality implies $n^μ\bar n^νΔ_{μν}=0$, so genuine background sensitivity starts only beyond LP.

</details>


### [32] [Fixed points of the renormalisation group running of the CKM and PMNS matrices](https://arxiv.org/abs/2601.02452)
*Brian P. Dolan*

Main category: hep-ph

TL;DR: This paper examines the renormalisation group running of the CKM parameters in the Standard Model, identifying six fixed points under 1-loop massless conditions. These fixed points exhibit CP symmetry and are linked to geometric properties of CKM matrices, suggesting their persistence across all orders and non-perturbative scenarios. The analysis also applies to the leptonic PMNS matrix.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of CKM parameters under renormalisation group evolution, identify fixed points in the parameter space, and explore their symmetry properties and stability across different perturbative orders and non-perturbative scenarios.

Method: The authors analyze the renormalisation group equations for the CKM parameters at the 1-loop level under massless conditions. They determine fixed points by solving the conditions where the beta functions vanish, derive corresponding operator mixing matrices from quark Yukawa couplings, and use differential geometric arguments to argue the persistence of these fixed points beyond 1-loop.

Result: Six fixed points are found with associated operator mixing matrices dependent on quark Yukawa couplings. Each fixed point preserves CP symmetry. The geometric analysis suggests fixed points remain invariant to all orders and non-perturbatively. The framework is shown applicable to both quark CKM and lepton PMNS matrices.

Conclusion: The identified fixed points are fundamental geometric features of CKM/PMNS spaces, robust against higher-order corrections and non-perturbative effects. This geometric perspective provides a new tool for studying flavor physics and understanding symmetries in the Standard Model.

Abstract: The renormalisation group running of the CKM parameters in the Standard Model is re-examined. For the massless 1-loop running six fixed points are found and their associated operator mixing matrices determined, in terms of the Yukawa couplings of the quarks. CP is an exact symmetry at each of the fixed points. An argument is given that the fixed points found at 1-loop must remain fixed points to all orders and even non-perturbatively, as they are associated with certain differential geometric properties of the space of CKM matrices. The analysis is equally applicable to the CKM matrix in the quark sector and the PMNS matrix in the leptonic sector.

</details>


### [33] [GeV-scale QCD Axion](https://arxiv.org/abs/2601.02465)
*Hitoshi Murayama*

Main category: hep-ph

TL;DR: Proposes a model where the Peccei-Quinn symmetry is broken below the QCD scale, resulting in a GeV-scale QCD axion potentially associated with observed η resonances, avoiding quantum gravity corrections and experimental constraints.


<details>
  <summary>Details</summary>
Motivation: To address the strong CP problem and explore axion models that avoid issues like quantum gravity corrections while evading observational limits.

Method: The model assigns a U(1) Peccei-Quinn charge only to the right-handed up quark, breaking the symmetry below the QCD scale to generate a heavy QCD axion. It analyzes implications for flavor physics and collider signatures.

Result: The proposed axion could be part of observed η resonances, avoiding most constraints except π±-π0 mass splitting. LHC could detect dijet resonances in specific quark-antiquark channels.

Conclusion: Such a model provides a viable solution to the strong CP problem with testable predictions at colliders, despite tight constraints from meson mass splittings.

Abstract: In order to solve the strong CP problem, we study the possibility that the Peccei--Quinn symmetry is broken {\it below}\/ the QCD scale. We find that a QCD axion can be above GeV, and may be among the observed $η$ resonances. It is immune to quantum gravity corrections. The only fermion that has a $U(1)$ Peccei--Quinn charge is the right-handed up quark. Flavor-changing neutral currents are surprisingly small, except for a possible but not necessary contribution to $D^0$--$\overline{D}^0$ mixing. All accelerator and astrophysical limits can be evaded. The most significant constraint is the mass splitting between $π^\pm$ and $π^0$. In a UV completed model, LHC can look for a dijet resonance in $u\bar{u}$, $u\bar{d}$, $d\bar{u}$ channels.

</details>


### [34] [Probing Dark Matter-Electron Interactions with Superconducting Qubits](https://arxiv.org/abs/2601.02474)
*Yonit Hochberg,Majed Khalaf,Noah Kurinsky,Alessandro Lenoci,Rotem Ovadia*

Main category: hep-ph

TL;DR: This paper uses transmon qubit decoherence measurements to set the tightest laboratory-based constraints on keV-scale dark matter-electron scattering and competitive limits on dark photon absorption.


<details>
  <summary>Details</summary>
Motivation: To probe dark matter interactions using quantum devices, specifically leveraging transmon qubits' noise-suppressing and sensitive properties. The unexplained residual decoherence in experiments motivates the need for dark matter constraints.

Method: Analyzing transmon qubit decoherence time measurements from various setups to infer dark matter-electron scattering cross-sections and dark photon coupling parameters, comparing with thermal noise and external sources.

Result: Established the most stringent lab-based constraints on dark matter-electron scattering at keV scale and competitive constraints on dark photon absorption.

Conclusion: Transmon qubits offer a promising platform for dark matter detection, demonstrating their sensitivity to low-mass particles and the potential for future improvements in constraining dark matter models.

Abstract: Quantum device measurements are powerful tools to probe dark matter interactions. Among these, transmon qubits stand out for their ability to suppress external noise while remaining highly sensitive to tiny energy deposits. Ambient galactic halo dark matter interacting with electrons can deposit energy in the qubit, leading to changes in its decoherence time. Recent measurements of transmons have consistently measured, in various experimental setups, a residual contribution to the decoherence time unexplained by thermal noise or known external sources. We use such measurements to set the most stringent laboratory-based constraints to date on dark matter-electron scattering at the keV scale and competitive constraints on dark photon absorption.

</details>


### [35] [Resummation of the C-Parameter Sudakov Shoulder Using Effective Field Theory](https://arxiv.org/abs/2601.02484)
*Matthew D. Schwartz*

Main category: hep-ph

TL;DR: The paper presents a resummation analysis of the C-parameter distribution in $e^+e^-$ annihilation using soft-collinear effective theory, addressing logarithms at the kinematic shoulder at $C=3/4$. New jet and soft functions are derived, with validation via Monte Carlo and AI-assisted computations.


<details>
  <summary>Details</summary>
Motivation: To explain the step discontinuity in the C-parameter distribution near $C=3/4$, caused by the transition from three- to four-parton states, and to properly resum large logarithms in this region.

Method: Application of soft-collinear effective theory (SCET) to derive a factorization theorem incorporating new jet and soft functions. The soft function's quadratic dependence on transverse momentum explains the step discontinuity. Loop computations (NLL+NLO) were performed with Monte Carlo validation, using an AI tool named Claude for calculations and analysis.

Result: Successful resummation achieved without Sudakov-Landau poles, differing from thrust and heavy jet mass. Results match Monte Carlo simulations, and the AI-assisted workflow validated the theoretical predictions.

Conclusion: The C-parameter's unique structure allows straightforward momentum-space resummation. The study demonstrates the effectiveness of SCET and AI tools like Claude in theoretical particle physics analysis.

Abstract: The C-parameter distribution in $e^+e^-$ annihilation exhibits a kinematic shoulder at $C = 3/4$, where three-parton final states reach their maximum and a fourth parton is required to exceed it. This boundary generates large logarithms that must be resummed. Using soft-collinear effective theory, we derive a factorization theorem involving new jet and soft functions specific to the C-parameter measurement, in which soft radiation contributes quadratically in transverse momentum. This quadratic structure explains the step discontinuity at leading order. We compute all ingredients at one loop, validate against Monte Carlo, and present matched NLL+NLO results. Unlike thrust and heavy jet mass, the C-parameter has no Sudakov--Landau pole, making momentum-space resummation straightforward. All calculations, numerical analysis, and manuscript preparation were performed by Claude, an AI assistant developed by Anthropic, working under physicist supervision.

</details>


### [36] [Dark matter and scalar sector in a novel two-loop scotogenic neutrino mass model](https://arxiv.org/abs/2601.02503)
*A. E. Cárcamo Hernández,Catalina Espinoza,Juan Carlos Gómez-Izquierdo,Juan Marchant González,Myriam Mondragón*

Main category: hep-ph

TL;DR: The paper presents an extended 3+1 Higgs doublet model with a specific discrete symmetry and right-handed Majorana neutrinos, explaining tiny neutrino masses via a two-loop radiative seesaw mechanism and cobimaximal mixing. It also aligns with SM quark data, dark matter constraints, and predicts non-SM scalars around 95 GeV and sub-TeV scales, potentially explaining a diphoton excess.


<details>
  <summary>Details</summary>
Motivation: To address the shortcomings of the Standard Model regarding active neutrino masses and the origin of dark matter, while explaining observed neutrino oscillation data and the 95 GeV diphoton excess at the LHC.

Method: Extended scalar sector with four Higgs doublets and gauge-singlet scalars, incorporating a Q6×Z2×Z4 discrete symmetry and right-handed Majorana neutrinos. The radiative seesaw mechanism at two-loop level generates neutrino masses, and the model is tested against dark matter constraints and collider data.

Result: The model successfully reproduces SM quark parameters, fits neutrino oscillation data via cobimaximal mixing, and predicts a 95 GeV scalar consistent with dark matter relic density and diphoton excess. Other non-SM scalars are within LHC reach.

Conclusion: The proposed model offers a viable framework explaining neutrino masses, mixing, dark matter, and LHC anomalies, with testable predictions at future collider experiments.

Abstract: We propose an extended $3+1$ Higgs doublet model where the Standard Model (SM) gauge structure is enhanced by the discrete symmetry $Q_6 \times Z_2 \times Z_4$, and the fermion content is extended with right-handed Majorana neutrinos. The scalar sector, besides four $SU(2)$ doublets, incorporates multiple gauge-singlet scalars. In our model, the tiny active neutrino masses arise from a novel radiative seesaw mechanism at two-loop level and the leptonic mixing features the cobimaximal mixing pattern compatible with neutrino oscillation experimental data. Along with this, the proposed model is consistent with SM quark masses and mixings as well as with the constraints arising from dark matter relic density and dark matter direct detection. Our analysis reveals that the best-fit point satisfying dark matter constraints yields a non-SM scalar with mass near $95$ GeV, which could be a possible candidate for the observed $95$ GeV diphoton excess. We further obtain other non SM scalars with masses at the subTeV scale which are within the LHC reach, while successfully complying with the experimental bounds arising from collider searches.

</details>


### [37] [Helicity Dependent Distribution Functions of the Proton and $Λ$ and $Σ^0$ Baryons](https://arxiv.org/abs/2601.02587)
*Yang Yu,Peng Cheng,Hui-Yu Xing,Daniele Binosi,Craig D. Roberts*

Main category: hep-ph

TL;DR: The paper uses continuum Schwinger function methods to predict distribution functions for proton, Λ, and Σ⁰ baryons, exploring diquark correlations and SU(3) flavor symmetry breaking. Key findings include the role of axialvector diquarks in Σ⁰ spin contributions and the significant gluon contribution (40%) to octet baryon spin at modern measurement scales.


<details>
  <summary>Details</summary>
Motivation: To provide a coherent theoretical framework for understanding baryon distribution functions by addressing the effects of diquark correlations and flavor symmetry breaking, which are critical for interpreting experimental measurements of nucleon and hyperon spin structures.

Method: The study employs continuum Schwinger function methods to calculate both helicity-dependent and unpolarised distribution functions for proton, Λ, and Σ⁰ particles, analyzing the impact of axialvector diquarks and SU(3) symmetry breaking on quark and gluon contributions to baryon spin.

Result: The analysis reveals that axialvector diquarks in Σ⁰ are essential for valence strange quarks to contribute to Σ⁰ spin. Gluon distribution functions show that gluons carry ~40% of the octet baryons' spin at typical measurement scales. Comparisons between helicity-dependent and unpolarised valence-quark DFs highlight diquark correlations effects.

Conclusion: The findings emphasize the necessity of considering axialvector diquark contributions and flavor symmetry breaking in baryon structure models. The predicted gluon spin contributions align with experimental expectations, supporting the role of gluons as significant carriers of baryon spin in modern particle physics frameworks.

Abstract: Using continuum Schwinger function methods, a coherent set of predictions for proton, $Λ$ and $Σ^0$ distribution functions (DFs) has been made available -- both helicity dependent and unpolarised. The results and comparisons between them reveal impacts of diquark correlations and SU$(3)$-flavour symmetry breaking, some of which are highlighted in this contribution. For instance: in-proton ratios of helicity-dependent/unpolarised valence-quark DFs are presented; it is highlighted that, were it not for the presence of axialvector diquarks in the $Σ^0$, the valence strange quark would carry none of the $Σ^0$ spin; and the sign and size of polarised gluon DFs is discussed -- at a scale typical of modern measurements, gluon partons carry roughly 40% of each octet baryon's spin.

</details>


### [38] [Higgs boson decays $h\rightarrow Z γ$ and $h\rightarrow m_V Z$ in the $U(1)_X$VLFM](https://arxiv.org/abs/2601.02679)
*Rong-Zhi Sun,Shu-Min Zhao,Yue-Tong Liu,Xing-Xing Dong*

Main category: hep-ph

TL;DR: The paper investigates Higgs boson decays into Zγ and m_VZ in a model with vectorlike fermions and U(1)_X symmetry, showing significant deviations from Standard Model predictions due to exotic Yukawa interactions and particle mixing.


<details>
  <summary>Details</summary>
Motivation: To explore potential new physics signals beyond the Standard Model through deviations in Higgs boson decay rates, particularly in h → Zγ and h → m_VZ channels.

Method: Analyzing loop diagrams involving vectorlike fermions in U(1)_XVLFM to compute corrections to CP-even/odd hγZ couplings, which affect the decay rates. Parameter space scans are performed to identify regions with observable deviations.

Result: The model predicts non-negligible deviations in decay rate ratios Γ_NP/Γ_SM for both decay channels, offering potential signatures of new physics detectable in future experiments.

Conclusion: Higgs decays to vector bosons and photons are sensitive probes for vectorlike fermion models, with observable deviations possible in specific parameter ranges, advocating for experimental searches in these channels.

Abstract: We study the Higgs boson decays $h \to Zγ$ and $h \to m_VZ$ in a model with vectorlike fermions and $U(1)_X$ symmetry ($U(1)_X$VLFM), where $m_V$ is a vector meson ($ρ,\ ω,\ φ,\ J/ψ,\ Υ$). The exotic Yukawa interactions in this model generate mixing between Standard Model (SM) fermions and vectorlike fermions, and this mixing affects the Higgs boson mass and Higgs couplings. The corrections to the CP-even and CP-odd $hγZ$ couplings come from loop diagrams that involve the new particles, and these corrections have a clear impact on the decay rates of $h\to Zγ$ and $h\to m_VZ$. In suitable regions of the parameter space, the model can produce non-negligible deviations in $Γ_{\rm NP}(h\to Zγ)/Γ_{\rm SM}(h\to Zγ)$ and $Γ_{\rm NP}(h\to m_VZ)/Γ_{\rm SM}(h\to m_V Z)$, providing possible signals of new physics (NP) beyond the SM.

</details>


### [39] [The Future of Higgs Physics](https://arxiv.org/abs/2601.02729)
*Michael E. Peskin*

Main category: hep-ph

TL;DR: The lecture discusses the measurements of Higgs boson properties and related observables in the Higgs factory era, highlighting challenges for next-gen particle physicists.


<details>
  <summary>Details</summary>
Motivation: The motivation is to advance understanding of the Higgs boson's role in particle physics through precise measurements in upcoming experimental settings.

Method: Discusses experimental approaches and techniques used in Higgs factory environments to measure Higgs boson properties.

Result: Outlines the expected outcomes and precision goals for Higgs boson measurements in future experiments.

Conclusion: Emphasizes the importance of these measurements for verifying Standard Model predictions and exploring beyond-the-Standard-Model physics.

Abstract: In this lecture, I discuss measurements of the properties of the Higgs boson and related observables in the era of Higgs factories. This highly motivated experimental program is the challenge for the next generation of particle physicists.

</details>


### [40] [Partial $μ-τ$ symmetry from kinetic normalization](https://arxiv.org/abs/2601.02745)
*N. Chamoun,C. Hamzaoui,M. Toharia*

Main category: hep-ph

TL;DR: The paper examines how higher-dimensional operators breaking μ-τ permutation symmetry affect the neutrino mass matrix, finding restricted regions for the lightest neutrino mass and second-octant atmospheric mixing angle in normal hierarchy, while inverted hierarchy is less constrained.


<details>
  <summary>Details</summary>
Motivation: Investigate the impact of higher-dimensional operators on the neutrino mass matrix structure, particularly focusing on the implications for the lightest neutrino mass and atmospheric mixing parameters in different mass hierarchies.

Method: Analyze models where μ-τ permutation symmetry is broken via higher-dimensional operators in lepton kinetic terms, deriving constraints on neutrino mass matrix parameters and their dependence on atmospheric mixing angle θ₂₃ and |Vₘₐ₃|.

Result: In normal hierarchy scenarios, the lightest neutrino mass is tightly constrained within a small range and θ₂₃ is confined to the second octant. Inverted hierarchy shows less restrictive constraints on these parameters.

Conclusion: Higher-dimensional operators modifying lepton kinetic terms impose stringent limits on neutrino mass parameters and mixing angles in normal hierarchy models, supporting further experimental investigation of θ₂₃ and neutrino mass ordering.

Abstract: We consider models with broken $μ$-$τ$ permutation symmetry through higher dimensional operators
  renormalizing the lepton kinetic terms in the action. We study
  the consequences on the structure of the neutrino mass matrix and find in particular that the allowed region for the lightest
  mass in the normal hierarchy plotted as a function of the atmospheric mixing element
  $|V_{\mu3}|$ is very restricted with the atmospheric mixing angle $θ_{23}$ to lie
  in the second octant. On the other hand, the corresponding allowed region in the inverted hierarchy regime
  is less restrictive.

</details>


### [41] [Two Higgs doublet model with a complex singlet scalar and Multi-critical Point Principle](https://arxiv.org/abs/2601.02808)
*Gi-Chol Cho,Chikako Idegawa,Chiaki Nose*

Main category: hep-ph

TL;DR: The paper investigates a two Higgs doublet model with a complex singlet scalar, where the singlet's imaginary part acts as dark matter. A degenerate scalar scenario with nearly equal masses for three neutral Higgs bosons helps meet dark matter and Higgs search constraints. The study applies the tree-level Multiple Point Principle (MPP) to motivate degeneracy in the Higgs spectrum, though this creates tension with required mixing parameters. However, viable parameter regions still exist, and thermal loop effects can enable a first-order electroweak phase transition suitable for baryogenesis.


<details>
  <summary>Details</summary>
Motivation: To theoretically motivate a degenerate Higgs spectrum needed for consistency with dark matter direct-detection limits and Higgs searches, the authors impose the tree-level MPP, requiring degeneracy between electroweak and singlet vacua. This explores implications for scalar potential, dark matter phenomenology, and electroweak phase transitions.

Method: The model extends the two Higgs doublet model with a complex singlet scalar. The tree-level MPP is imposed to enforce vacuum degeneracy, leading to specific conditions on scalar potential parameters. Parameter space is analyzed to resolve tensions between MPP requirements and the degenerate scalar scenario, considering both tree-level and thermal loop effects on phase transitions.

Result: Large SU(2)_L mixing parameters arise from the MPP, conflicting with the degenerate scalar scenario but viable regions exist meeting all constraints. Thermal loop effects can produce a first-order electroweak phase transition forbidden at tree level, supporting electroweak baryogenesis.

Conclusion: The study demonstrates compatibility of the MPP with a viable dark matter model and electroweak phase transition when including thermal loop effects. It shows how theoretical principles like MPP can guide model building despite apparent tensions, highlighting the role of quantum effects in phase transitions.

Abstract: We study a two Higgs doublet model extended by a complex singlet scalar, in which the imaginary part of the singlet serves as a dark matter (DM) candidate. In this model, degenerate masses of the three neutral Higgs bosons are crucial for achieving consistency with current constraints from DM direct-detection experiments and Higgs searches. This is called the degenerate scalar scenario. To provide a theoretical motivation for such a degenerate Higgs spectrum, we impose the tree-level Multiple Point Principle (MPP), which requires the electroweak and singlet vacua to be degenerate, and analyze its implications for the scalar potential, DM phenomenology, and the electroweak phase transition. We show that the tree-level MPP favors large SU(2)$_L$ doublet-singlet mixing parameters, which compete with the degenerate scalar scenario. Nevertheless, we demonstrate that viable parameter regions still exist in which the observed DM constraints are satisfied. Furthermore, although the tree-level MPP forbids a tree-level-driven first-order electroweak phase transition, we show that thermal loop effects can induce a strong first-order transition compatible with electroweak baryogenesis.

</details>


### [42] [Study of $CP$ violation in $Λ_b^0\rightarrow N^*M$ decays with the final-state rescattering mechanism](https://arxiv.org/abs/2601.02887)
*Hui-Qiang Shang,Tian-Liang Feng,Jing Gao,Qin Qin,Fu-Sheng Yu*

Main category: hep-ph

TL;DR: The paper studies charmless non-leptonic two-body Λ_b decays using the final-state rescattering mechanism, focusing on calculating both absorptive and dispersive parts of triangle diagrams. It analyzes decays to N*(1535/1520) states and mesons (K_S, K^*_0(700), etc.), predicting branching ratios, CP asymmetries, and decay asymmetry parameters for these processes relevant to LHCb experiments.


<details>
  <summary>Details</summary>
Motivation: To explore the dynamics of Λ_b baryon decays through rescattering mechanisms, providing theoretical predictions for observables measurable at the LHCb experiment, particularly focusing on intermediate N* resonances and unconventional mesons like K^*_0(700).

Method: The final-state rescattering framework is applied to two-body Λ_b decays. Distinct from Cutkosky's method, both absorptive and dispersive parts of triangle diagrams are computed. Decay processes to N* and various M mesons are analyzed with emphasis on calculating branching ratios and CP violation parameters across different partial waves.

Result: Predictions for branching ratios of ^Λb → N^*M modes and their CP asymmetries, including direct and partial-wave asymmetries for decay channels. Finds that two-body decays dominantly feed into four-body final states like pπ-π+π-, which are experimentally accessible.

Conclusion: The rescattering mechanism successfully explains the dominant contributions of these two-body decays to four-body decay channels at LHCb. The predicted observables, especially CP asymmetries in Λ_b decays, offer testable predictions for future high-energy physics experiments to validate the theoretical framework.

Abstract: In this work, we investigate the charmless non-leptonic two-body $Λ_b$ decays within the framework of final-state rescattering mechanism. In contrast to the Cutkosky cutting method, we compute both the absorptive and dispersive parts of the hadronic rescattering triangle diagrams. Based on the established formalism, we analyze the $Λ_b \to N^*(1535,1520)M$ decay processes with $M =K_S, K^*_0(700)$, $f_0(500,980), ρ(770), \bar{K}^{*0}$, $φ$, and predict various physical observables, such as their branching ratios, direct and partial-wave $CP$ asymmetries, as well as decay asymmetry parameters. These two-body decay processes are expected to contribute primarily to the subsequent four-body decay channels, such as $Λ_b^0 \to p\,π^-\,π^+\,π^-$, whose $CP$ asymmetry measurements will be accessible at the LHCb experiment.

</details>


### [43] [Single-quark electromagnetic form factors of charmonium up to $J=2$](https://arxiv.org/abs/2601.02897)
*Jian Huang,Muyang Chen,Xian-Hui Zhong*

Main category: hep-ph

TL;DR: The paper calculates single-quark electromagnetic form factors for various charmonium states using a relativized quark model, compares results with LQCD, DSE, and BLFQ approaches, and provides predictions for states with no existing data.


<details>
  <summary>Details</summary>
Motivation: To accurately determine electromagnetic properties of charmonium states and validate the relativized quark model by comparing with other theoretical frameworks, while providing predictions where experimental or computational data is lacking.

Method: A relativized quark model is employed to compute electromagnetic form factors. The reference frame dependence is analyzed to estimate computational errors. Results are compared with lattice QCD, Dyson-Schwinger equations, and basis light front quantization methods.

Result: Most results align with those from LQCD, DSE, and BLFQ. Predictions are made for form factors of six charmonium states where direct comparisons are unavailable.

Conclusion: The relativized quark model effectively reproduces form factors for charmonium states, confirming its validity. Predictions offer new insights for states未 studied by other methods, highlighting the model's predictive power.

Abstract: We calculate the single-quark electromagnetic form factors of a broad subset of charmonium, including $η_c(1S)$, $η_c(2S)$, $χ_{c0}(1P)$, $χ_{c0}(2P)$, $J/ψ(1S)$, $J/ψ(2S)$, $χ_{c1}(1P)$, $χ_{c1}(2P)$, $h_c(1P)$, $h_c(2P)$, $χ_{c2}(1P)$ and $χ_{c2}(2P)$, via a relativized quark model. The reference frame dependence of the results is estimated as the computational error. We compare our results with those of the lattice quantum chromodynamics (LQCD), the Dyson-Schwinger equation (DSE) and the basis light front quantization (BLFQ) approaches where available and we find that most of our results agree with the other results. We also predict the single-quark electromagnetic form factors of $χ_{c0}(2P)$, $χ_{c1}(2P)$, $h_c(1P)$, $h_c(2P)$, $χ_{c2}(1P)$ and $χ_{c2}(2P)$, where no direct comparisons are available.

</details>


### [44] [Heavy-quark initiated charged-current deep-inelastic scattering coefficient functions through $\mathcal{O}(α_s^2)$](https://arxiv.org/abs/2601.02916)
*Kirill Kudashkin*

Main category: hep-ph

TL;DR: The paper calculates coefficient functions for heavy-quark initiated charged-current deep-inelastic scattering up to α_s^2 order, using a cut-based method for diagram isolation and analytic solutions of differential equations for master integrals. Results are presented in decoupling scheme for n_L light flavors suitable for ACOT/FONLL variable-flavor schemes.


<details>
  <summary>Details</summary>
Motivation: To provide precise predictions for charged-current DIS processes involving heavy quarks, requiring higher-order QCD corrections (∼α_s^2) and full mass dependence treatment for accurate phenomenological applications in particle physics experiments.

Method: Cut-based approach isolates individual Feynman diagram contributions, reduces to master integrals via differential equations solved analytically with generalized polylogarithms, and employs the decoupling scheme for n_L light quarks.

Result: First complete two-loop O(α_s^2) coefficient functions with full mass dependence computed for heavy-quark charged-current DIS processes in decoupling scheme, enabling direct implementation into ACOT/FONLL variable-flavor scaling schemes.

Conclusion: The results improve precision in analyzing HERA and future collider data by providing accurate heavy-quark contributions, essential for testing Standard Model predictions and probing physics beyond the Standard Model.

Abstract: We compute the coefficient functions for heavy-quark initiated charged-current deep-inelastic scattering through $\mathcal{O}(α_s^2)$, retaining full mass dependence for a single heavy-quark flavor. The calculation employs a cut-based approach to isolate individual diagram contributions and to derive differential equations for the relevant master integrals, which are solved analytically in terms of generalized polylogarithms. The results are presented in the decoupling scheme for $n_L$ light active flavors, facilitating their direct implementation in variable-flavor number schemes such as ACOT and FONLL.

</details>


### [45] [Solutions to axion electrodynamics with electric-magnetic duality in supersymmetric Seiberg-Witten theory](https://arxiv.org/abs/2601.02969)
*Tong Li,Rui-Jia Zhang*

Main category: hep-ph

TL;DR: The paper explores non-linear axion electrodynamics using the SW axion model, derives EM equations, identifies valid parameters, solves equations with external fields, and proposes an LC circuit detection method to measure axion-photon couplings.


<details>
  <summary>Details</summary>
Motivation: To investigate the connection between axions and magnetic monopoles via non-standard axion electrodynamics under electric-magnetic duality, and to develop detection strategies for axion-photon interactions predicted by the SW model.

Method: Deriving equations of motion from the SW axion's infrared Lagrangian, analyzing moduli space parameters, solving axion Maxwell equations with external fields, and simulating detection using an LC circuit.

Result: Analytical and numerical solutions for axion-induced EM fields were obtained, and an LC circuit's sensitivity to SW axion couplings was demonstrated.

Conclusion: The study provides a framework for testing axion-photon interactions through measurable electromagnetic effects and highlights the viability of LC circuits as detectors for beyond-Standard-Model physics.

Abstract: Axion and magnetic monopole are among the most fascinating candidates for physics beyond the Standard Model. The potential connection between axion and magnetic monopole stems from the Witten effect and is revealed by non-standard axion electrodynamics. Non-standard axion electrodynamics under electric-magnetic duality modifies conventional axion Maxwell equations and motivates intriguing axion-photon phenomenology. A calculable ultraviolet model of Peccei-Quinn axion coupled to magnetic monopoles and electric charges was proposed based on $\mathcal{N}=2$ supersymmetric Seiberg-Witten (SW) theory with manifest electric-magnetic duality. In this work, we aim to investigate the solutions to the non-linear axion electrodynamics from SW axion model and propose relevant detection strategies for non-trivial axion-photon couplings. Based on the infrared Lagrangian of SW axion, we derive the electromagetic (EM) equations of motion. We also analyze the moduli space coordinate in SW theory and find out the reliabe parameter space. We then solve the resultant axion Maxwell equations with an external EM field. The observable axion-induced EM fields are obtained analytically and then numerically computed. Finally, we propose the detection strategy with an LC circuit and show the prospective sensitivity to SW axion-photon couplings.

</details>


### [46] [Inflation in Extra-Dimensions with one or two branes](https://arxiv.org/abs/2601.02982)
*Nicolás Bernal,Catarina Cosme,Andrea Donini,Nuria Rius*

Main category: hep-ph

TL;DR: The study analyzes monomial and α-attractor inflation models in three extra-dimensional frameworks (Dark Dimension, RS1, RS2). α-attractor outperforms monomial inflation in fitting observational data, with extra dimensions allowing more flexibility than 4D models.


<details>
  <summary>Details</summary>
Motivation: To investigate how extra-dimensional setups influence inflationary models and their compatibility with cosmic microwave background data, addressing the puzzle of the small 4D cosmological constant.

Method: Derive Friedmann equations for each extra-dimensional scenario, calculate slow-roll parameters, and fit model predictions (n_s, α_s, Δ_s², r) against Planck/BICEP/ACT data.

Result: Monomial inflation is strongly disfavored in all extra-dimensional setups. α-attractor inflation provides excellent observational agreement, with extra dimensions enhancing model flexibility.

Conclusion: α-attractor models in extra-dimensional frameworks are strongly favored observationally, offering advantages over both 4D models and monomial inflation.

Abstract: In this paper, we study two inflationary models, namely, monomial inflation and the simplest $α$-attractor inflation, within extra-dimensional frameworks. We consider three extra-dimensional setups: Dark Dimension, which embeds one flat extra-dimension to explain the observed smallness of the 4D cosmological constant $Λ_4$; and the two Randall-Sundrum scenarios with one warped extra-dimension, namely RS1 with two branes and RS2 with one brane. We derive the corresponding Friedmann equations, compute the slow-roll parameters in each case, and we fit the experimental data for ($n_s - 1$, $α$, $Δ_s^2$, $r$), using Planck, BICEP, and ACT data. We find that monomial inflation is strongly disfavored in all scenarios, while $α$-attractor inflation provides an excellent fit to current observations, with extra-dimensional setups offering additional flexibility compared to the standard 4D case.

</details>


### [47] [Standard Model Higgs Peaks: a Note on the Vacuum Instability during Inflation](https://arxiv.org/abs/2601.03231)
*G. Franciolini,A. Kehagias,A. Riotto*

Main category: hep-ph

TL;DR: This paper addresses the Higgs vacuum instability issue by applying extreme-value statistics to evaluate quantum fluctuations during inflation, leading to a slightly tighter but qualitatively different bound on the Hubble rate than previous methods.


<details>
  <summary>Details</summary>
Motivation: The Standard Model Higgs potential becomes unstable at high field values, risking catastrophic consequences if inflation-driven quantum fluctuations push the Higgs field past a potential barrier.

Method: Applies extreme-value statistics to analyze peaks of Higgs field fluctuations during inflation, contrasted with prior approaches that may have used different statistical treatments.

Result: Derives a Hubble rate bound √2 times stricter than conventional estimates, emphasizing the method’s qualitative distinction over quantitative precision.

Conclusion: Advocates for the importance of extreme-value statistics in cosmological instability analyses due to its more accurate description of fluctuation extrema, despite the modest quantitative improvement.

Abstract: In the Standard Model, the Higgs potential develops an instability at high field values when the quartic self-coupling runs negative. Large quantum fluctuations during cosmic inflation could drive the Higgs field beyond the potential barrier, creating regions that would be catastrophic for our observable universe. We point out that the extreme-value statistics describing the peaks (maxima) of the Higgs values is the correct statistics to infer the condition to avoid vacuum instability. Even if this statistics delivers a bound on the Hubble rate during inflation which is only a factor $\sqrt{2}$ stronger than the one commonly adopted in the literature, it is qualitatively distinct and we believe worthwhile communicating it.

</details>


### [48] [Hyperon neutrinoless double beta decays in chiral perturbation theory](https://arxiv.org/abs/2601.02990)
*Zi-Ying Zhao,Ze-Rui Liang,Feng-Kun Guo,Li-Ping He,De-Liang Yao*

Main category: hep-ph

TL;DR: The paper investigates neutrinoless double beta decays of spin-1/2 hyperons ($B_1^- 	o B_2^+ 
u 
u$) using covariant baryon chiral perturbation theory, calculating decay amplitudes at one-loop level and presenting Dalitz plots for six physical processes.


<details>
  <summary>Details</summary>
Motivation: To explore lepton number violation and study neutrinoless double beta decay mechanisms in hyperon systems, providing theoretical foundations for experimental searches.

Method: Covariant baryon chiral perturbation theory at one-loop order was applied to compute decay amplitudes for the decays $B_1^- 	o B_2^+ 
u 
u$ with specific hyperon combinations.

Result: Dalitz plots for six physical decay processes were generated, highlighting the squared amplitude distributions. Detailed numerical analysis is deferred to a future publication.

Conclusion: The approach demonstrates feasibility of using chiral perturbation theory for analyzing hyperon neutrinoless double beta decays, offering insights for upcoming experimental validations.

Abstract: We study the neutrinoless double beta decays of spin-1/2 hyperons, $B_1^- \to B_2^+ \ell^- \ell^-$ with $B_1^-\in \{Σ^-,Ξ^-\}$ and $B_2^+\in\{p,Σ^+\}$, which violate lepton number by two units. The decay amplitudes are computed within covariant baryon chiral perturbation theory at the one-loop level. Dalitz plots of the squared amplitude for six physical decay processes are presented in this proceeding, while a detailed numerical analysis will be presented in a forthcoming publication.

</details>


### [49] [Sommerfeld Effect and Bound State Formation for Dark Matter Models with Colored Mediators with SE+BSF4DM](https://arxiv.org/abs/2601.03026)
*Mathias Becker,Emanuele Copello,Julia Harz,Martin Napetschnig*

Main category: hep-ph

TL;DR: The paper examines how mediator annihilation and quantum effects like Sommerfeld enhancement and bound state formation impact relic abundance calculations in simplified $t$-channel dark matter models with mass-degenerate dark matter and mediator particles. It compares four models, shows these effects can cause order-one corrections to dark matter mass constraints, and introduces a computational tool SE+BSF4DM.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of standard thermal relic calculations by incorporating quantum corrections (Sommerfeld enhancement and bound state formation) in models where dark matter and mediator masses are nearly degenerate, which dominates relic abundance through mediator annihilation.

Method: Analyzed four $t$-channel models with scalar/fermionic mediators, applied Sommerfeld enhancement and bound state formation corrections, compared model behaviors against direct detection constraints, developed computational tool SE+BSF4DM for automated calculations.

Result: Observed significant (order one) corrections to dark matter mass constraints in coannihilation regimes due to quantum effects; noted model-dependent variations; confirmed that subdominant bound state effects must be included; provided a publicly available tool for general $t$-channel DM model analysis.

Conclusion: Quantum effects like Sommerfeld enhancement and bound state formation critically affect relic density predictions in degenerate mass scenarios, necessitating their inclusion in model analysis. The developed tool enables accurate, automated assessments of these effects across simplified $t$-channel dark matter frameworks.

Abstract: In the universal framework of simplified $t$-channel dark matter models, the calculation of the relic abundance can be dominated by mediator annihilation when the dark matter and mediator masses are almost degenerate. We analyze four representative models with scalar and fermionic mediators, confront them with direct detection limits and highlight the differences and common features between them. The mediator annihilations are considerably enhanced by the Sommerfeld effect and bound state formation. Albeit their effect is subdominant in the coannihilation regime, excited bound state levels are included as well. We find that Sommerfeld and bound-state effects can lead to order one corrections to the constraints on the DM mass in the coannihilating regime, with the precise magnitude depending on the specific model realization. In addition we provide SE+BSF4DM, an intuitive and easy to use add-on to micrOMEGAs, allowing for an automated inclusion of these effects for a generic $t$-channel Dark Matter Model, which is publicly available on Github.

</details>


### [50] [Anatomy of Roper Resonance](https://arxiv.org/abs/2601.03102)
*Igor Strakovsky*

Main category: hep-ph

TL;DR: The paper discusses the 60-year evolution of understanding the first excited state of a proton/neutron, highlighting its uniqueness as a single resonance with two pole positions on different Riemann sheets and reflecting on the historical progression of this knowledge.


<details>
  <summary>Details</summary>
Motivation: To commemorate the 60th anniversary of the discovery of the proton/neutron first excited state and clarify its unique properties through a historical retrospective, addressing potential misconceptions in the historical narrative.

Method: The paper employs a historical analysis approach, reviewing key developments and theoretical advancements over six decades to trace the understanding of the resonance's dual pole positions.

Result: It confirms the uniqueness of the proton/neutron excited state's two pole positions on separate Riemann sheets, emphasizing its significance in nuclear physics and the importance of accurate historical context in scientific understanding.

Conclusion: The study underscores the importance of revisiting historical scientific milestones to ensure accurate narratives and highlights the proton/neutron excited state as a fundamental case study for resonance theory in modern physics.

Abstract: Sixty years ago, the first excited state of a proton/neutron was ``born.'' During this time, we learned a lot about it, specifically - how unique this case is: a single resonance with two pole positions on different Riemann sheets. Let me present a brief history to remind readers how development progressed. Sure, history is sometimes something that never happened, described by those who were never there...

</details>


### [51] [Searches for extra-dimensional excitations in light-by-light scattering](https://arxiv.org/abs/2601.03110)
*Malak Ait Tamlihat,Ghizlane Ez-Zobayr,Laurent Schoeffel,Yahya Tayalati*

Main category: hep-ph

TL;DR: The paper analyzes the Radion in the Randall-Sundrum model through light-by-light scattering in LHC proton-proton collisions, exploring its couplings and mixing with the Higgs to set exclusion limits.


<details>
  <summary>Details</summary>
Motivation: To investigate the detectability of the Radion, which is challenging due to its weak gravitational coupling, by considering Higgs-mixing scenarios that enhance signals through constructive interference.

Method: Derive effective couplings of the Radion to Standard Model fields, use forward proton tagging for event selection, and reinterpret ALP experimental limits to create exclusion contours.

Result: Non-minimal Higgs mixing significantly boosts Radion signals, allowing the first exclusion limits in the (Λ_r, ξ) parameter space using current LHC data.

Conclusion: Higgs-mixed Radion models are now constrained by LHC data, highlighting the importance of considering kinetic mixing effects for future particle searches.

Abstract: We present a comprehensive phenomenological analysis of the Radion in the Randall-Sundrum model, focusing on its production via light-by-light scattering in ultra-peripheral proton-proton collisions at the LHC. We provide a consistent derivation of the effective couplings to Standard Model fields, clarifying the normalization of the trace anomaly-induced coupling to photons and the role of kinetic mixing with the Higgs boson. We demonstrate that while the pure gravitational coupling is loop-suppressed relative to Axion-Like Particles (ALPs), making the unmixed Radion elusive, the non-minimal mixing with the Higgs sector can induce constructive interference that enhances the signal by orders of magnitude. Using forward proton tagging to select exclusive high-mass events, we reinterpret recent experimental limits on ALPs to derive the first exclusion contours for the Radion in the $(Λ_r, ξ)$ plane, showing that mixing scenarios are beginning to be constrained by current LHC data.

</details>


### [52] [Study of sub-GeV Dipolar Dark States at SND@LHC within Invisible Bounds on Meson Decays](https://arxiv.org/abs/2601.03186)
*Debajyoti Biswas*

Main category: hep-ph

TL;DR: This paper analyzes the sensitivity of the SND@LHC detector to detect Feebly Interacting Particles (FIPs) with magnetic/electric dipole moments via photon-mediated interactions.


<details>
  <summary>Details</summary>
Motivation: Electromagnetic form factors provide a viable path to discover beyond-Standard Model particles. Dimension-5 dipole operators offer a simple framework for FIP detection, and SND@LHC's unique forward region offers high-momentum conditions optimal for such searches.

Method: Simulates FIP production from Drell-Yan and meson decay processes. Calculates cross-sections considering LHC's forward kinematics. Compares SND@LHC sensitivity with existing experiments (direct detection, beam dump, etc) while validating effective field theory bounds.

Result: SND@LHC can probe new parameter space for electric/magnetic dipole moments beyond existing constraints. Effective field theory validity established through coupling bounds. Sensitivity plots show competitive reach with leading experiments for GeV-scale FIPs.

Conclusion: SND@LHC offers a promising probe of dipolar dark sector interactions, complementary to other experiments. Further collider and astrophysical studies are encouraged to fully exploit this promising discovery potential.

Abstract: Electromagnetic form factors constitute a natural portal for accessing states beyond the Standard Model. In particular, dimension-5 magnetic and electric dipole moment operators offer a minimal and predictive framework for Feebly Interacting Particles (FIPs). In this work, we perform a study of the sensitivity reach of the Scattering and Neutrino Detector (SND@LHC) in the detection of dipolar dark states through photon-mediated interactions with the Standard Model particles. The far-forward region of the LHC provides FIPs with large momenta that scatter off electrons and nuclei inside the target. Production of dark states from meson decays is constrained by invisible decay widths, while the Drell-Yan process offers a production channel in the GeV range. We present sensitivity plots for magnetic and electric dipole moment interactions at SND@LHC and compare them with constraints from direct detection, beam dump, fixed-target, and collider experiments. The validity of the effective theory that describes the dipole model is also studied by considering conservative bounds on the couplings.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [53] [Predictability of bursts of a recurrent nova using topological data analysis and machine learning](https://arxiv.org/abs/2601.02403)
*Ignacio Morales-Gil*

Main category: astro-ph.IM

TL;DR: The study uses persistent homology and machine learning to predict nova outbursts in RS Oph with high accuracy.


<details>
  <summary>Details</summary>
Motivation: To develop a method for predicting whether RS Oph will burst within a year, leveraging topological data analysis on its lightcurve data.

Method: Trained a supervised learning model using featurizations (persistence landscapes, Carlsson coordinates, persistent images, template functions) of persistence diagrams from RS Oph's lightcurve sections. Evaluated with tenfold cross validation, focusing on persistence landscapes for high recall and accuracy.

Result: The model using persistence landscapes achieved consistently high recalls and accuracies in predicting nova outbursts.

Conclusion: Persistent homology combined with ML effectively predicts RS Oph's outbursts, offering a novel approach for analyzing cataclismic variable star behavior.

Abstract: RS Oph is a recurrent nova, a kind of cataclismic variable that shows bursts in a period approximately shorter than a century. Persistent homology, a technique from topological data analysis, studies the evolution of topological features of a simplicial complex composed of the data points or an embedding of them, as some distance parameter is varied. For this work I trained a supervised learning model based on several featurizations, namely persistence landscapes, Carlsson coordinates, persistent images, and template functions, of the persistence diagrams of sections of the lightcurve of RS Oph. A tenfold cross validation of the model based on one of the featurizations, persistence landscapes, consistently shows high recalls and accuracies. This method serves the purpose of predicting whether RS Oph is bursting within a year.

</details>


### [54] [Experiments in binary evolution](https://arxiv.org/abs/2601.02448)
*Stephan Geier,Thomas Kupfer,Pierre Maxted,Veronika Schaffenroth*

Main category: astro-ph.IM

TL;DR: The paper emphasizes the importance of observational studies on binary star systems to understand stellar interactions, proposing that next-generation spectroscopic surveys need optimized scheduling and flexible instruments to effectively determine orbital parameters of large binary populations.


<details>
  <summary>Details</summary>
Motivation: To address the observational constraints on stellar interactions in binary systems and compile volume-complete samples of progenitor and post-mass transfer systems for studying their evolution.

Method: Proposing the use of next-generation time-resolved spectroscopic surveys with real-time schedulers and flexible multi-object spectrographs or telescope networks to efficiently gather orbital phase coverage.

Result: Such an approach would enable accurate determination of orbital parameters for thousands of binary systems across a wide range of periods, facilitating detailed studies of stellar interactions.

Conclusion: Efficient scheduling and advanced instrumentation are critical to overcoming observational bottlenecks and advancing our understanding of binary star evolution through comprehensive data collection.

Abstract: The majority of stars more massive than the Sun is found in binary or multiple star systems and many of them will interact during their evolution. Specific interactions, where progenitors and post-mass transfer (MT) systems are clearly linked, can provide yet missing observational constraints. Volume-complete samples of progenitor and post-MT systems are well suited to study those processes. To compile them, we need to determine the parameters of thousands of binary systems with periods spanning several orders of magnitude. The bottleneck are the orbital parameters, because accurate determinations require a good coverage of the orbital phases. The next generation of time-resolved spectroscopic surveys should be optimized to follow-up and solve whole populations of binary systems in an efficient way. To achieve this, a scheduler predicting the best times of the next observation for any given target in real time should be combined with a flexibly schedulable multi-object spectrograph or ideally a network of independent telescopes.

</details>


### [55] [Validation of Satellite Lifetime Predictions at Leonid Space](https://arxiv.org/abs/2601.02453)
*Scott Shambaugh*

Main category: astro-ph.IM

TL;DR: The paper validates a satellite lifetime prediction pipeline using a large dataset of 934 satellites, achieving significantly improved accuracy over existing tools like ESA's DRAMA & DISCOS, with a custom propagator enabling rapid analysis.


<details>
  <summary>Details</summary>
Motivation: To establish a validated performance baseline for operational lifetime prediction tools that rely on forecasted space weather rather than historical data, improving accuracy and enabling better LEO mission planning and regulatory compliance.

Method: The pipeline uses TLE data and space weather records across six solar cycles in a three-stage validation to reduce hindsight bias. It combines ballistic coefficient estimation with probabilistic orbit propagation and employs a custom semianalytic propagator for efficiency.

Result: Achieved median CRPS scores of 6.0, 18.6, and 45.5 days under different conditions. The custom propagator offers 3500x speedup over Orekit and 4.5x over DRAMA, with solar cycle forecasting being the main error source after ballistic coefficients.

Conclusion: The method sets a new accuracy standard for satellite lifetime prediction, critical for operational services, and highlights that improving solar forecasting is key to further reducing errors beyond better propagators or atmosphere models.

Abstract: We validate Leonid Space's satellite lifetime prediction pipeline through comprehensive backtesting against 934 non-maneuvering satellites that deorbited from LEO between 1961 and 2024. This represents the first large-scale validation of lifetime prediction tooling using forecasted space weather conditions rather than historical hindsight. Our toolchain combines ballistic coefficient estimation from on-orbit data with probabilistic orbit propagation under varying environmental conditions. Using TLE data and space weather records spanning six solar cycles, our three-stage validation approach progressively removes hindsight bias to arrive at fully predictive operational conditions. We achieve 1-year prediction accuracy (median continuously ranked probability score) of 6.0 days (1.6%) under perfect knowledge conditions, 18.6 days (5.1%) with estimated ballistic coefficients and known space weather, and 45.5 days (12.4%) under fully predictive conditions. Comparison against ESA's standard DRAMA & DISCOS toolchain demonstrates a 4x improvement in state-of-the-art accuracy for well-characterized satellites. A custom semianalytic propagator provides a >3500x speedup over Orekit and 4.5x speedup over DRAMA, enabling rapid Monte Carlo analysis across large satellite populations. Our analysis reveals that solar cycle forecasting dominates error budgets after ballistic coefficient estimation, with higher-fidelity propagators and atmosphere models providing marginal benefit. These results establish a validated performance baseline for operational lifetime prediction services supporting LEO mission planning and regulatory compliance.

</details>


### [56] [Exposure-averaged Gaussian Processes for Combining Overlapping Datasets](https://arxiv.org/abs/2601.02462)
*Jacob K. Luhn,Ryan A. Rubenzahl,Samuel Halverson,Lily L. Zhao*

Main category: astro-ph.IM

TL;DR: The paper introduces an adjusted Gaussian process framework that accounts for exposure averaging in extreme precision radial velocity data, allowing for accurate modeling of stellar variability across instruments with different exposure times.


<details>
  <summary>Details</summary>
Motivation: Existing Gaussian process kernels assume instantaneous covariance, which is insufficient when exposure times are comparable to signal timescales. This is critical for EPRV spectrographs with varying exposure times, as different instruments observe binned versions of the same underlying signal.

Method: Develops integrated kernel forms to model exposure-averaged signals and incorporates instrumental drift correction for overlapping observations. Uses solar EPRV data as a case study, with generalizable methods for datasets where exposure time affects signal measurement.

Result: Framework successfully models latent stellar signals and instrumental drifts, validated with solar data. Enables accurate combination of data from instruments with different exposure times and geographical overlaps.

Conclusion: The method provides a robust solution for handling exposure time effects and instrumental variations in EPRV data, expanding applicability to asteroseismology and other fields requiring precise signal separation.

Abstract: Physically motivated Gaussian process (GP) kernels for stellar variability, like the commonly used damped, driven simple harmonic oscillators that model stellar granulation and p-mode oscillations, quantify the instantaneous covariance between any two points. For kernels whose timescales are significantly longer than the typical exposure times, such GP kernels are sufficient. For time series where the exposure time is comparable to the kernel timescale, the observed signal represents an exposure-averaged version of the true underlying signal. This distinction is important in the context of recent data streams from Extreme Precision Radial Velocity (EPRV) spectrographs like fast readout stellar data of asteroseismology targets and solar data to monitor the Sun's variability during daytime observations. Current solar EPRV facilities have significantly different exposure times per-site, owing to the different design choices made. Consequently, each instrument traces different binned versions of the same "latent" signal. Here we present a GP framework that accounts for exposure times by computing integrated forms of the instantaneous kernels typically used. These functions allow one to predict the true latent oscillation signals and the exposure-binned version expected by each instrument. We extend the framework to work for instruments with significant time overlap (i.e., similar longitude) by including relative instrumental drift components that can be predicted and separated from the stellar variability components. We use Sun-as-a-star EPRV datasets as our primary example, but present these approaches in a generalized way for application to any dataset where exposure times are a relevant factor or combining instruments with significant overlap.

</details>


### [57] [Scalable Gaussian Processes for Integrated and Overlapping Measurements Via Augmented State Space Models](https://arxiv.org/abs/2601.02527)
*Ryan A. Rubenzahl,Soichiro Hattori,Simo Särkkä,Will M. Farr,Jacob K. Luhn,Megan Bedell*

Main category: astro-ph.IM

TL;DR: The paper introduces smolgp, a scalable Gaussian Process (GP) method that uses a state space model (SSM) to efficiently handle integrated astronomical measurements with varying exposure times, enabling O(N) computation and GPU parallelization.


<details>
  <summary>Details</summary>
Motivation: Astronomical data with finite exposures can obscure underlying variability. Traditional GPs become computationally infeasible due to non-quasiseparable covariance matrices when exposures vary or overlap. The need for scalable methods to analyze large datasets with exposure effects is critical.

Method: Extends GP-SSM equivalence by adding an integral state in the SSM that tracks exposure integrals. The state resets at exposure starts and is observed at ends, allowing exact GP posterior computation with O(N) cost via Kalman filter/RTS smoother. Implements this in the smolgp package, compatible with tinygp and supporting various covariance kernels.

Result: Achieves O(N) CPU and O(N/T + log T) GPU performance, enabling scalability for large N. Demonstrates applicability to non-quasiseparable kernels like the quasiperiodic kernel, crucial for astronomical datasets with exposure overlaps.

Conclusion: Smolgp enables efficient analysis of massive, complex astronomical datasets with exposure-aware GP modeling. This unlocks potential for cross-instrument comparisons and high-dimensional analyses previously limited by computational costs.

Abstract: Astronomical measurements are often integrated over finite exposures, which can obscure latent variability on comparable timescales. Correctly accounting for exposure integration with Gaussian Processes (GPs) in such scenarios is essential but computationally challenging: once exposure times vary or overlap across measurements, the covariance matrix forfeits any quasiseparability, forcing O($N^2$) memory and O($N^3$) runtime costs. Linear Gaussian state space models (SSMs) are equivalent to GPs and have well-known O($N$) solutions via the Kalman filter and RTS smoother. In this work, we extend the GP-SSM equivalence to handle integrated measurements while maintaining scalability by augmenting the SSM with an integral state that resets at exposure start times and is observed at exposure end times. This construction yields exactly the same posterior as a fully integrated GP but in O($N$) time on a CPU, and is parallelizable down to O($N/T + \log T$) on a GPU with $T$ parallel workers. We present smolgp (State space Model for O(Linear/log) GPs), an open-source Python/JAX package offering drop-in compatibiltiy with tinygp while supporting both standard and exposure-aware GP modeling. As SSMs provide a framework for representing general GP kernels via their series expansion, smolgp also brings scalable performance to many commonly used covariance kernels in astronomy that lack quasiseparability, such as the quasiperiodic kernel. The substantial performance boosts at large $N$ will enable massive multi-instrument cross-comparisons where exposure overlap is ubiquitous, and unlocks the potential for analyses with more complex models and/or higher dimensional datasets.

</details>


### [58] [Using 23 Years of ACS/SBC Data to Understand Backgrounds: Significant Reductions in Expected Background Levels](https://arxiv.org/abs/2601.02551)
*Christopher. J. R. Clark,Roberto J. Avila,Alyssa Guzman,Norman A. Grogin*

Main category: astro-ph.IM

TL;DR: Analysis of 23 years of Hubble Space Telescope ACS/SBC data reveals significant variation in background levels across different filters, attributed primarily to airglow fluctuations. Certain filters show backgrounds lower than ETC predictions, prompting updates to exposure time calculations.


<details>
  <summary>Details</summary>
Motivation: To understand the actual background variations in Hubble's ACS/SBC observations and assess discrepancies with existing ETC models, enabling more accurate exposure time estimates.

Method: Analyzed approximately 23 years of Hubble ACS/SBC data across various filters (F115LP, F122M, F125LP, PR110L, PR130L, F150LP, F165LP). Compared observed background levels with ETC v33.2 predictions, identified dominant sources (airglow vs. dark rate), and evaluated factors causing discrepancies.

Result: Observed backgrounds for airglow-dominated filters were 2.5-10.5x lower than ETC predictions. F150LP/F165LP showed minimal variation due to dark rate dominance. ETC v34.1 now includes empirical background percentile options for exposure time estimation.

Conclusion: ETC predictions overestimated airglow contributions, leading to overly conservative exposure time calculations. Updated models using empirical data improve observational efficiency for SBC filters.

Abstract: We have used 23 years of Hubble Space Telescope ACS/SBC data to study what background levels are encountered in practice and how much they vary. The backgrounds vary considerably, with F115LP, F122M, F125LP, PR110L, and PR130L all showing over an order of magnitude of variation in background between observations, apparently due to changes in airglow. The F150LP and F165LP filters, which are dominated by dark rate, not airglow, exhibit a far smaller variation in backgrounds. For the filters where the background is generally dominated by airglow, the backgrounds measured from the data are significantly lower than what the ETC predicts (as of ETC v33.2). The ETC predictions for `average' airglow are greater than the median of our measured background values by factors of 2.51, 2.64, 105, and 3.64, for F115LP, F122M, F125LP, and F140LP, respectively. A preliminary analysis suggests this could be due to certain OI airglow lines usually being fainter than expected by the ETC. With reduced reduced background levels, the shorter-wavelength SBC filters can conduct background-limited observations much more rapidly than had previously been expected. As of ETC v34.1, a new option will be included for SBC calculations, allowing users to employ empirical background percentiles to estimate required exposure times.

</details>


### [59] [The Lazuli Space Observatory: Architecture & Capabilities](https://arxiv.org/abs/2601.02556)
*Arpita Roy,Stuart Feldman,Pete Klupar,John DiPalma,Saul Perlmutter,Ewan S. Douglas,Greg Aldering,Gabor Furesz,Patrick Ingraham,Gudmundur Stefansson,Douglas Kelly,Fan Yang Yang,Thomas Wevers,Nicole Arulanantham,James Lasker,Mickael Rigault,Everett Schlawin,Sander R. Zandbergen,S. Pete Worden,Ramya Anche,Heejoo Choi,Ian J. M. Crossfield,Kevin Derby,Jerry Edelstein,Mike Eiklenborg,Suvi Gezari,Paul Giuliano,Justin Hom,Taylor J. Hoyt,Hyukmo Kang,Daewook Kim,Keerthi Kunnumkai,Leander Lacroix,Jared R. Males,Thomas J. Maccarone,Kian Milani,Timothy N. Miller,Kelsey Lynn Miller,Pierre Nicolas,Antonella Palmese,Jason Pero,Laurent Pueyo,Stephanie Rinaldi,David J. Sand,Christian Schneider,Sanchit Sabhlok,Arfon Smith,Irina I. Stefan,Saraswathi Kalyani Subramanian,Kyle Van Gorkom,Andre F. Wong,Jaegun Yoo,Md Abdullah Al Zaman,the Lazuli Science Team*

Main category: astro-ph.IM

TL;DR: The Lazuli Space Observatory is a 3-meter telescope designed for rapid-response and precision astrophysics, equipped with three instruments to support time-domain astronomy, exoplanet studies, and cosmology. It achieves diffraction-limited performance and fast target acquisition.


<details>
  <summary>Details</summary>
Motivation: To enable unprecedented temporal responsiveness (sub-4-hour target acquisition) and precise measurements for transient events, multi-messenger astronomy, exoplanet imaging/spectroscopy, and cosmological studies using Type Ia supernovae.

Method: Off-axis freeform telescope with 3 instruments: Wide-field Context Camera (high-cadence imaging), Integral Field Spectrograph (broad-spectrum spectroscopy), and ExtraSolar Coronagraph (high-contrast imaging). Operates in lunar-resonant orbit for rapid repointing.

Result: Expected capabilities include Strehl >0.8 at 633 nm, 10^-9 post-processed contrast for exoplanets, and continuous 400-1700 nm spectroscopy. Designed to support both prioritized science programs and open时间为全球社区. Results pending actual mission operations.

Conclusion: Lazuli's architecture balances specialized science goals with general observatory functionality, poised to deliver transformative data in transient astronomy and exoplanet research after launch. Its rapid-response capability sets a new standard for mid-sized space telescopes.

Abstract: The Lazuli Space Observatory is a 3-meter aperture astronomical facility designed for rapid-response observations and precision astrophysics across visible to near-infrared wavelengths (400-1700 nm bandpass). An off-axis, freeform telescope delivers diffraction-limited image quality (Strehl $>$0.8 at 633 nm) to three instruments across a wide, flat focal plane. The three instruments provide complementary capabilities: a Wide-field Context Camera (WCC) delivers multi-band imaging over a 35' $\times$ 12' footprint with high-cadence photometry; an Integral Field Spectrograph (IFS) provides continuous 400-1700 nm spectroscopy at R $\sim$ 100-500 for stable spectrophotometry; and an ExtraSolar Coronagraph (ESC) enables high-contrast imaging expected to reach raw contrasts of $10^{-8}$ and post-processed contrasts approaching $10^{-9}$. Operating from a 3:1 lunar-resonant orbit, Lazuli will respond to targets of opportunity in under four hours--a programmatic requirement designed to enable routine temporal responsiveness that is unprecedented for a space telescope of this size. Lazuli's technical capabilities are shaped around three broad science areas: (1) time-domain and multi-messenger astronomy, (2) stars and planets, and (3) cosmology. These capabilities enable a potent mix of science spanning gravitational wave counterpart characterization, fast-evolving transients, Type Ia supernova cosmology, high-contrast exoplanet imaging, and spectroscopy of exoplanet atmospheres. While these areas guide the observatory design, Lazuli is conceived as a general-purpose facility capable of supporting a wide range of astrophysical investigations, with open time for the global community. We describe the observatory architecture and capabilities in the preliminary design phase, with science operations anticipated following a rapid development cycle from concept to launch.

</details>


### [60] [Optomechanical platform for high-frequency gravitational wave and vector dark matter detection](https://arxiv.org/abs/2601.02576)
*David Rousso,Moritz Bjoern Kristiansson Kunze,Christoph Reinhardt*

Main category: astro-ph.IM

TL;DR: The paper proposes a nanomechanical membrane resonator integrated into an optical cavity to detect high-frequency gravitational waves and vector dark matter, offering a unified platform for beyond-Standard-Model physics.


<details>
  <summary>Details</summary>
Motivation: To create a versatile detector that can simultaneously search for high-frequency gravitational waves and vector dark matter, addressing gaps in current detection methods and extending sensitivity beyond existing limits.

Method: A silicon membrane resonator integrated with a gallium-arsenide mirror in a moderate-finesse optical cavity. Uses radiation-pressure force to modulate cavity length, resonantly drive membrane motion, and tune resonance frequency. Differential acceleration between materials enables vector dark matter detection.

Result: Achieves peak strain sensitivity of 2×10⁻²³/√Hz at 40 kHz for gravitational waves and surpasses existing limits for vector dark matter detection (2×10⁻¹² to 2×10⁻¹⁰ eV/c² over a year).

Conclusion: The platform provides a unified approach to probe both phenomena, advancing searches for physics beyond the Standard Model through its dual-sensitivity capability.

Abstract: We present a proposal for a nanomechanical membrane resonator integrated into a moderate-finesse ($\mathcal{F}\sim 10$) optical cavity as a versatile platform for detecting high-frequency gravitational waves and vector dark matter. Gravitational-wave sensitivity arises from cavity-length modulation, which resonantly drives membrane motion via the radiation-pressure force. This force also enables in situ tuning of the membrane's resonance frequency by nearly a factor of two, allowing a frequency coverage from 0.5 to 40 kHz using six membranes. The detector achieves a peak strain sensitivity of $2\times 10^{-23}/\sqrt{\text{Hz}}$ at 40 kHz. Using a silicon membrane positioned near a gallium-arsenide input mirror additionally provides sensitivity to vector dark matter via differential acceleration from their differing atomic-to-mass number ratios. The projected reach surpasses the existing limits in the range of $2\times 10^{-12}$ to $2\times 10^{-10}$ $\text{eV}/c^2$ for a one-year measurement. Consequently, the proposed detector offers a unified approach to searching for physics beyond the Standard Model, probing both high-frequency gravitational waves and vector dark matter.

</details>


### [61] [The Study of a Cosmic Ray Candidate Detected by the Askaryan Radio Array](https://arxiv.org/abs/2601.02718)
*Shoukat Ali,Dave Z. Besson*

Main category: astro-ph.IM

TL;DR: The study investigates a cosmic-ray-induced air shower event detected by the ARA Station 2, analyzing dual radio signals (geomagnetic and Askaryan) via simulations to optimize event reconstruction and calibration.


<details>
  <summary>Details</summary>
Motivation: To characterize cosmic-ray signals as both a background source and calibration tool for the ARA neutrino detector, enhancing its sensitivity to ultra-high energy neutrinos.

Method: Used FAERIE (for shower simulation) and AraSim (detector simulation) to model the event's dual-pulse signals, comparing vertex reconstructions and time delays between geomagnetic and Askaryan emissions.

Result: Optimized event topology reconstruction by analyzing signal timing and spatial features, validating the method for future neutrino detection calibration.

Conclusion: Demonstrates the feasibility of using CR-induced signals for detector calibration and Background modeling, improving ARA's capability to distinguish neutrino events from cosmic-ray backgrounds.

Abstract: Experiments designed to detect ultra-high energy (UHE) neutrinos using radio techniques are also capable of detecting the radio signals from cosmic-ray (CR) induced air showers. These CR signals are important both as a background and as a tool for calibrating the detector. The Askaryan Radio Array (ARA), a radio detector array, is designed to detect UHE neutrinos. The array currently comprises five independent stations, each instrumented with antennas deployed at depths of up to 200 meters within the ice at the South Pole.
  In this study, we focus on a candidate event recorded by ARA Station 2 (A2) that shows features consistent with a downward-going CR-induced air shower. This includes distinctive double-pulse signals in multiple channels, interpreted as geomagnetic and Askaryan radio emissions arriving at the antennas in sequence. To investigate this event, we use detailed simulations that combine a modern ice-impacting CR shower simulation framework, FAERIE, with a realistic detector simulation package, AraSim. We will present results for an optimization of the event topology, identified through simulated CR showers, comparing the vertex reconstruction of both the geomagnetic and Askaryan signals of the event, as well as the observed time delays between the two signals in each antenna.

</details>


### [62] [SpaceWire-based Data Acquisition Network for the Solar Flare Sounding Rocket Experiment FOXSI-4 and FOXSI-5](https://arxiv.org/abs/2601.02788)
*Shunsaku Nagasawa,Athanasios Pantazides,Kristopher Cooper,Riko Shimizu,Savannah Perez-Piel,Takahiro Minami,Yixian Zhang,Hunter Kanniainen,Shin Watanabe,Tadayuki Takahashi,Noriyuki Narukage,Juan Camilo Buitrago Casas,Lindsay Glesener*

Main category: astro-ph.IM

TL;DR: A SpaceWire-based DAQ system was developed for the FOXSI-4 rocket mission to observe solar flares with advanced detectors. The modular design using SpaceWire/RMAP enabled scalable communication and distributed development, while FPGA-based boards and a ground system supported real-time monitoring. The system will also be used for the upcoming FOXSI-5 mission.


<details>
  <summary>Details</summary>
Motivation: To enable high-sensitivity and dynamic-range observations of solar flares using direct X-ray focusing optics, requiring a DAQ system capable of handling high photon flux and diverse detectors, supporting distributed collaboration and scalable integration.

Method: Developed a modular DAQ architecture using SpaceWire/RMAP, FPGA-based readout boards (SPMU-001/002), and a real-time ground system for telemetry/command during flight. The system accommodates multiple detectors including CMOS, CdTe-DSDs, and Quad-Timepix3, enabling flexible control and data transfer.

Result: Successfully observed a GOES M1.6 class flare during FOXSI-4 (April 2024), achieving first direct focusing observation with imaging spectroscopy. The modular design simplified integration among collaborators and supports scalability for future missions.

Conclusion: The SpaceWire-based DAQ system demonstrated robustness for solar flare missions, facilitating adaptive on-board adjustments and enabling reuse for FOXSI-5 in 2026. It underscores the benefits of standardized modular architectures in multi-institutional space projects.

Abstract: We developed a SpaceWire-based data acquisition (DAQ) system for the FOXSI-4 and FOXSI-5 sounding rocket experiments, which aim to observe solar flares with high sensitivity and dynamic range using direct X-ray focusing optics. The FOXSI-4 mission, launched on April 17, 2024, achieved the first direct focusing observation of a GOES M1.6 class solar flare with imaging spectroscopy capabilities in the soft and hard X-ray energy ranges, using a suite of advanced detectors, including two CMOS sensors, four CdTe double-sided strip detectors (CdTe-DSDs), and a Quad-Timepix3 detector. To accommodate the high photon flux from a solar flare and these diverse detector types, a modular DAQ network architecture was implemented based on SpaceWire and the Remote Memory Access Protocol (RMAP). This modular architecture enabled fast, reliable, and scalable communication among various onboard components, including detectors, readout boards, onboard computers, and telemetry systems. In addition, by standardizing the communication interface and modularizing each detector unit and its associated electronics, the architecture also supported distributed development among collaborating institutions, simplifying integration and reducing overall complexity. To realize this architecture, we developed FPGA-based readout boards (SPMU-001 and SPMU-002) that support SpaceWire communication for high-speed data transfer and flexible instrument control. In addition, a real-time ground support system was developed to handle telemetry and command operations during flight, enabling live monitoring and adaptive configuration of onboard instruments in response to the properties of the observed solar flare. The same architecture is being adopted for the upcoming FOXSI-5 mission, scheduled for launch in 2026.

</details>


### [63] [A New Collision Avoidance Fiber Assignment Algorithm for Robotic Fiber Positioners in Multi-Object Spectroscopy](https://arxiv.org/abs/2601.02795)
*Minseong Kwon,Ho Seong Hwang,Jong Chul Lee,Jae-Woo Kim,Hyeonguk Bahk,Young-Man Choi,Moo-Young Chun,Sang-Hyun Chun,Haeun Chung,Sungwook E. Hong,Minhee Hyun,Donghui Jeong,Kang-Min Kim,Dachan Kim,Dongkok Kim,Yunjong Kim,Jongwan Ko,Ho-Gyu Lee,Yongseok Lee,Hyunho Lim,Heeyoung Oh,Changbom Park,Hyunmi Song,Mingyeong Yang,Yongmin Yoon*

Main category: astro-ph.IM

TL;DR: The paper introduces a novel fiber assignment algorithm for robotic fiber positioner systems in multi-object spectroscopy, achieving a 10% increase in target assignment completeness compared to a simple ranking-based method in the A-SPEC survey.


<details>
  <summary>Details</summary>
Motivation: To maximize the number of observable targets in fiber spectroscopy by addressing fiber collision issues caused by overlapping patrol regions in modern robotic systems.

Method: The algorithm has three steps: (1) initial assignment of maximum targets without collision consideration, (2) grouping and resolving collisions within each group, and (3) comparison against a rank-based collision-aware method. Designed for A-SPEC but applicable to similar systems.

Result: Achieved 10% higher completeness in target assignments compared to the simple algorithm in a 150-fiber field scenario.

Conclusion: The new algorithm effectively improves target acquisition efficiency in fiber-based surveys with overlapping positioners, offering broader applicability beyond its initial A-SPEC implementation.

Abstract: We present a new fiber assignment algorithm for a robotic fiber positioner system in multi-object spectroscopy. Modern fiber positioner systems typically have overlapping patrol regions, resulting in the number of observable targets being highly dependent on the fiber assignment scheme. To maximize observable targets without fiber collisions, the algorithm proceeds in three steps. First, it assigns the maximum number of targets for a given field of view without considering any collisions between fiber positioners. Then, the fibers in collision are grouped, and the algorithm finds the optimal solution resolving the collision problem within each group. We compare the results from this new algorithm with those from a simple algorithm that assigns targets in descending order of their rank by considering collisions. As a result, we could increase the overall completeness of target assignments by 10% with this new algorithm in comparison with the case using the simple algorithm in a field with 150 fibers. Our new algorithm is designed for the All-sky SPECtroscopic survey of nearby galaxies (A-SPEC) based on the K-SPEC spectrograph system, but can also be applied to similar fiber-based systems with heavily overlapping fiber positioners.

</details>
