<div id=toc></div>

# Table of Contents

- [astro-ph.IM](#astro-ph.IM) [Total: 13]
- [hep-ph](#hep-ph) [Total: 14]
- [gr-qc](#gr-qc) [Total: 12]
- [astro-ph.HE](#astro-ph.HE) [Total: 15]


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [1] [Habitable from the start: How initial planetary formation conditions may create habitable worlds](https://arxiv.org/abs/2511.16714)
*Benjamin J. Farcy,Darryl Z. Seligman,Kathleen E. Mandt,John W. Noonan,Sarah E. Anderson*

Main category: astro-ph.IM

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The breadth of topics that encompass the search for life has expanded and evolved significantly since the emergence of the field of astrobiology. Initial astrobiology centered investigations focused on detecting biosignatures in the Martian soil with the Viking lander. The field now encompasses identification of biosignatures throughout the galaxy and habitable worlds, planets with sufficient liquid water and prebiotic chemistry to support life. This evolution mirrors the improvement in our understanding of environments that may harbor life. The bulk planetary chemistry governs the habitability of a planet, which is in turn set by the early solar system environment and planet formation processes. Therefore, investigations of solar and exoplanetary systems as a whole would provide insights into the factors that make a planet habitable. Bulk planetary chemistry govern planetary atmospheres, core sizes, magnetic fields, heat engines, volatile inventories, and silicate mantle compositions. We therefore advocate for investigations of formation conditions that establish planetary chemistry, and by extension, habitability.

</details>


### [2] [Validation of optical pathlength stability in a LISA test-bench demonstrator](https://arxiv.org/abs/2511.16749)
*Shivani Harer,Maxime Vincent,Hubert Halloin,Ouali Acef,Nisrine Arab,Romain Arguel,Axel Arhancet,Damien Bachet,Nathalie Besson,Sébastien Bize,Sara Bruhier,Christelle Buy,Michael Carle,Jean-Pierre Coulon,Nicoleta Dinu-Jaeger,Mathieu Dupont,Christophe Fabron,Rémi Granelli,David Holleville,Dominique Huet,Eric Kajfasz,Mickael Lacroix,Matthieu Laporte,Michel Lintz,Christophe Meessen,Mourad Merzougui,Alexis Mehlman,Marco Nardello,Laure Oudda,Benjamin Pointard,Pierre Prat,Emmanuelle Rivière,Jérôme Royon,Aurélia Secroun,Samuel Sube,Thomas Zerguerras,Julien Zoubian*

Main category: astro-ph.IM

TL;DR: This paper presents the development and testing of the Zerodur interferometer (ZIFO) for the LISA mission, demonstrating picometer-level optical path stability and identifying noise sources.


<details>
  <summary>Details</summary>
Motivation: To validate critical technology for LISA's interferometric core, ensuring the mission's capability to detect gravitational waves with required precision.

Method: Development and testing of the ZIFO optical demonstrator, including measurements of optical path length stability, analysis of bench noise reduction, and characterization of noise sources from phasemeters and beam tilt correlations.

Result: Successful reduction of bench noise to meet the 10 pm/$\sqrt{\text{Hz}}$ specification across the 1 mHz to 1 Hz band, with identification of dominant noise sources.

Conclusion: The ZIFO validates key LISA technologies, but further optimization of phasemeters and beam tilt management is needed for future missions.

Abstract: The Laser Interferometer Space Antenna (LISA) observatory is a future L3 mission of the European Space Agency (ESA) to detect gravitational waves, set to launch in 2035. The detector constellation will conduct interferometry to picometer stability over an unprecedented arm length of 2.5 million km. In this paper, we present the development and testing results for the Zerodur interferometer (ZIFO), an optical demonstrator built to validate critical technology for the test setup of the interferometric core of LISA. Optical path length stability measurements on the ZIFO demonstrate successful reduction of bench noise to maintain the 10 pm/$\sqrt{\text{Hz}}$ specification across the 1 mHz to 1 Hz frequency band. We also identify and characterize dominant noise sources from phasemeters and correlations of beam tilt into the path length that were observed during the test campaign.

</details>


### [3] [CLAWDIA: A dictionary learning framework for gravitational-wave data analysis](https://arxiv.org/abs/2511.16750)
*Miquel Llorens-Monteagudo,Alejandro Torres-Forné,José A. Font*

Main category: astro-ph.IM

TL;DR: CLAWDIA is an open-source Python framework integrating sparse dictionary learning (SDL) for gravitational-wave data analysis, offering denoising and classification under realistic noise conditions, with applications demonstrated on GW170817 and glitch classification.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in gravitational-wave data analysis where deep learning requires large datasets and lacks interpretability, while SDL excels in scarce data scenarios and provides physically meaningful representations.

Method: CLAWDIA uses SDL-based approaches including LASSO-regularized sparse coding for denoising and Low-Rank Shared Dictionary Learning for classification. It includes a modular framework with GWADAMA for dataset preparation, supporting time-domain analysis under realistic detector noise.

Result: Demonstrated successful denoising of GW170817's signal and effective classification of instrumental glitches at low signal-to-noise ratios, showcasing robust performance in challenging conditions.

Conclusion: CLAWDIA provides a community-oriented, interoperable tool for gravitational-wave tasks, extending beyond existing methods to support detection, parameter estimation, and other analysis needs through modularity and extensibility.

Abstract: Deep-learning methods are becoming increasingly important in gravitational-wave data analysis, yet their performance often relies on large training datasets and models whose internal representations are difficult to interpret. Sparse dictionary learning (SDL) offers a complementary approach: it performs well in scarce-data regimes and yields physically interpretable representations of gravitational-wave morphology. Here we present CLAWDIA (Comprehensive Library for the Analysis of Waves via Dictionary-based Algorithms), an open-source Python framework that integrates SDL-based denoising and classification under realistic detector noise. We systematise previously isolated SDL workflows into a unified, modular environment with a consistent, user-friendly interface. The current release provides several time-domain denoising strategies based on LASSO-regularised sparse coding and a classifier based on Low-Rank Shared Dictionary Learning. A companion toolbox, GWADAMA, supports dataset construction and realistic conditioning of real and simulated interferometer data. We demonstrate CLAWDIA's performance by denoising the signal from binary neutron star event GW170817 and by classifying families of instrumental glitches from LIGO's third observing run, highlighting robustness at low signal-to-noise ratios. CLAWDIA is intended as a community-driven, interoperable library extensible to additional tasks, including detection and parameter estimation.

</details>


### [4] [Data-Driven Stellar Spectral Modelling with GSPICE](https://arxiv.org/abs/2511.16754)
*Douglas P. Finkbeiner,Joshua S. Speagle,Tanveer Karim*

Main category: astro-ph.IM

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Spectral data reduction pipelines deal with a wide variety of challenges including masking cosmic rays, calibrating wavelength solutions, and estimating background noise while trying to remain model-agnostic. Traditional methods rely on hardware-specific code or pre-calculated stellar model templates to solve this problem, making them model-dependent and not suitable for large datasets that may contain new classes of objects. To solve this problem, we present a flexible, data-driven method: the GausSian PIxelwise Conditional Estimator (GSPICE) that models an ensemble of spectra as a multivariate Gaussian and estimates the expected value and expected variance of each pixel in each spectrum conditional on others. GSPICE compares observed fluxes and errors to its own flux and error estimates to reveal outliers, which then can be completely masked or replaced by their estimates. We apply GSPICE to 3.9 million stellar spectra from the LAMOST survey, and show that variations of the method can directly identify and correct both individual pixel-level outliers (e.g., from cosmic ray hits) as well as extended systematic features (e.g., from incorrect wavelength calibrations), while still providing a novel characterization of the true per-pixel measurement uncertainties. We also demonstrate how GSPICE can take advantage of data partitioning with an application to diffuse interstellar bands. Implementations of GSPICE in both Python and IDL can be found here http://github.com/dfink/gspice.

</details>


### [5] [Lord of the (sub-)Rings : Mapping the surface reflectance and spin-axis of Ajisai](https://arxiv.org/abs/2511.16780)
*Robert J. S Airey,Paul Chote,James A. Blake,James McCormac,Billy Shrive,Don Pollacco,Benjamin F. Cooke*

Main category: astro-ph.IM

TL;DR: The paper demonstrates that streak photometry from ground-based telescopes can effectively determine the spin-state of fast-spinning space debris like Ajisai, using an MCMC method to align modeled mirror positions with derived reflectivity maps.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of constrained rotational motion understanding for active debris removal, the study uses Ajisai as a test case due to its simple geometry and known spin behavior.

Method: Four observations of Ajisai from SuperWASP in 2019 were analyzed using an MCMC-driven method comparing modeled mirror positions with a novel reflectivity map derived from streaked images.

Result: The method successfully recovered Ajisai's spin-axis and rotation period, with results aligning well within model expectations and uncertainties.

Conclusion: Streak photometry with modest ground-based instruments is scalable for determining rotational states of debris, aiding in active debris removal efforts.

Abstract: Active debris removal techniques are posed to become an important tool in maintaining the safety of the near-Earth space environment. These techniques rely on a clear understanding of the rotational motion of the debris targets, which is challenging to constrain from unresolved imaging. The Ajisai satellite provides an ideal test case for developing and demonstrating these techniques due to its simple geometry and well constrained spin behaviour. We present four observations of the Ajisai satellite taken with SuperWASP in August of 2019, where high cadence photometry was extracted from streaked images as a part of a larger survey of Low Earth Orbit. We develop an MCMC-driven method to determine the spin-state of Ajisai by comparing the alignment between a map of modelled mirror positions and a novel derived map of surface reflectivity. We generally find good agreement within the expectation and uncertainties set by empirical models and our determined spin-state solutions align the surface reflectivity map and modelled mirror location well. Our results show that streak photometry can be used to recover the spin-axis and rotation period of fast-spinning objects such as Ajisai using modest ground-based instrumentation, making it readily scalable to a wider range of targets and observatories.

</details>


### [6] [GSpyNetTreeS: a machine learning solution for glitch localization in time and frequency](https://arxiv.org/abs/2511.16861)
*Man Leong Chan,Jess McIver,Yannick Lecoeuche,Dhatri Raghunathan,Sofía Álvarez-López,Julian Ding,Annudesh Liyanage,Raymond Ng,Heather Fong*

Main category: astro-ph.IM

TL;DR: The paper introduces GSpyNetTreeS, an automated tool based on the You Only Look Once algorithm, to detect, classify, and locate detector noise transients in LIGO data, aiming to reduce human bias in gravitational wave candidate vetting.


<details>
  <summary>Details</summary>
Motivation: Ground-based gravitational wave detectors face challenges from non-Gaussian instrumental artifacts and noise transients. Current human-driven vetting processes are inconsistent and non-reproducible due to expert judgment variations, necessitating an automated solution.

Method: Extends GSpyNetTree with GSpyNetTreeS using the You Only Look Once algorithm for automated detection and classification of glitches like Blip, Low frequency blip, Low frequency line, and Scratchy. Tested on LIGO's third observing run data.

Result: GSpyNetTreeS accurately identifies glitch classes and captures their time-frequency information, proving potential as an automatic validation tool for observatory data.

Conclusion: This tool addresses the need for consistent, reproducible artifact detection, potentially replacing human-centric vetting in future gravitational wave observations.

Abstract: Data from ground-based gravitational wave detectors are often contaminated by non-Gaussian instrumental artifacts or detector noise transients. Unbiased source property estimation relies on the ability to correctly identify and characterize these artifacts and remove them if necessary. To this end, the LIGO-Virgo-KAGRA Collaboration has implemented candidate vetting for all significant candidates to identify the presence of artifacts and assess the need for mitigation. The current candidate vetting process requires human experts to identify the frequency ranges and the time windows associated with any data quality issues present. Differences in judgment between human experts may cause inconsistency, making results difficult to reproduce across gravitational wave events. We present GSpyNetTreeS, an extension to GSpyNetTree based on the You Only Look Once algorithm, for the automatic detection, classification, and time-frequency localization of detector noise transients. As a proof of concept, we tested GSpyNetTreeS's performance on the data collected by the LIGO detectors during the third observing run for gravitational waves as well as common detector glitch classes included in GSpyNetTree: Blip, Low frequency blip, Low frequency line and Scratchy. We also demonstrated that GSpyNetTreeS is capable of accurately identifying common glitch classes and capturing the frequency and time information associated with detected detector noise transients, establishing its potential as an automatic event validation tool for LIGO-Virgo-KAGRA's observing runs.

</details>


### [7] [Fast far-sidelobe modeling for centimeter to sub-millimeter astrophysical observations](https://arxiv.org/abs/2511.16967)
*Oliver Jeong,Jacques Delabrouille,Michel Piat*

Main category: astro-ph.IM

TL;DR: This paper presents a diffraction-based method for rapid calculation of far-sidelobe responses in telescopes, validated on BICEP3, and demonstrates its use in evaluating temperature pickup from Instrumental sidelobes during observations near Cerro Toco.


<details>
  <summary>Details</summary>
Motivation: To enable cost-effective optimization of next-generation telescopes' far-sidelobe performance, addressing computational limitations of existing electromagnetic modeling methods.

Method: Development of a new diffraction-based beam modeling technique for rapid far-sidelobe simulation, applied to BICEP3 hardware.

Result: Good qualitative agreement between simulated far-sidelobes and actual measurements; demonstrated capability to predict temperature pickup from terrain interactions during observational scenarios.

Conclusion: The proposed method provides a valuable predictive tool for optimizing baffling design and observational strategies during early telescope design stages.

Abstract: Next-generation centimeter to sub-millimeter telescopes require exquisite control over instrumental far-sidelobe response to accurately measure faint signals like the Cosmic Microwave Background B modes. Because existing electromagnetic modeling methods are computationally expensive, we developed a novel, diffraction-based beam modeling method for rapid and low-cost calculations. We applied this methodology to model the BICEP3 far-sidelobes and found good qualitative agreement with in situ beam measurements. Using this validated simulated beam, we calculated the sidelobe temperature pickup for a specific observation scenario: scanning near the slopes of Cerro Toco in the Atacama Desert. This rapid, predictive framework is most valuable as a tool for optimizing instrument baffling and identifying efficient scan strategies during the conceptual design phase.

</details>


### [8] [A novel double-rim forebaffle design for centimeter to sub-millimeter astrophysical observations](https://arxiv.org/abs/2511.16970)
*Jacques Delabrouille,Oliver Jeong,Michel Piat,Alexander Steier*

Main category: astro-ph.IM

TL;DR: The paper proposes a double-rim forebaffle design to reduce stray radiation impact in astronomical observations, enhancing the detection of faint cosmic signals and mapping of diffuse emissions.


<details>
  <summary>Details</summary>
Motivation: Stray radiation degrades centimeter to sub-millimeter observations, hindering detection of faint signals like cosmic microwave background B modes and mapping of large-scale diffuse emissions.

Method: Proposed double-rim forebaffle design analyzed through qualitative arguments and numerical simulations.

Result: Simulations show the design can significantly improve observation quality by reducing stray radiation contamination.

Conclusion: The design is a promising solution for future astronomical instruments to mitigate stray light issues effectively.

Abstract: Stray radiation of various origin is a major source of degradation of centimeter to sub-millimeter astronomical observations. This is particularly problematic for the detection of signals such as faint cosmic microwave background polarization B modes, or for mapping large-scale extragalactic or Galactic diffuse emission. In this paper, we propose a double-rim forebaffle design to reduce the impact of such stray radiation contamination. Using qualitative arguments and numerical simulations, we show that such a design has the potential to substantially improve the quality of future observations.

</details>


### [9] [Morphological Image Similarity Search on the ALMA Science Archive Query Interface Using Deep Unsupervised Contrastive Representation Learning](https://arxiv.org/abs/2511.17061)
*Felix Stoehr,Andrea Farago,Stefan Curiban,Alisdair Manning,Jorge Garcia,Pei-Ying Hsieh,Andrew Lipnicky,Adele Plunkett*

Main category: astro-ph.IM

TL;DR: The paper introduces a morphological image similarity search feature in the ALMA Science Archive using self-supervised deep learning, allowing astronomers to refine searches by selecting similar images interactively.


<details>
  <summary>Details</summary>
Motivation: The exponential growth of astronomical data makes it hard to find specific observations. Existing archives only search metadata, not content, so enabling content-based searches is crucial.

Method: They use self-supervised contrastive learning with affine-transformation-independent representation of source morphologies via a deep neural network. The ASA web interface displays similar images, which update in real-time as astronomers select additional images to refine results.

Result: First implementation of image similarity search in an astronomical archive, providing an interactive way for astronomers to explore data by morphology.

Conclusion: This method enhances data discovery in the ASA, opening new avenues for scientific exploration by enabling content-based searches and iterative refinement of results.

Abstract: With the exponential growth of astronomical data over time, finding the needles in the haystack is becoming increasingly difficult. The next frontier for science archives is to enable searches not only on observational metadata, but also on the content of the observations themselves. As a step in this direction, we have implemented morphological image similarity search into the ALMA Science Archive (ASA). To achieve this we use self-supervised contrastive affine-transformation-independent representation learning of source morphologies with a deep neural network. For a given image on the ASA web interface, astronomers are presented with a summary view of the morphologically most similar images. Each time an astronomer selects an additional image from that view, the display is instantly updated to show the images most similar to the combination of the selected images. Each selection thus refines the similarity display according to the scientific needs of the astronomer. This is the first time image similarity search has been offered in an astronomical science archive.

</details>


### [10] [From Ground to Space: An Overview of the JEM-EUSO Program for the Study of UHECRs and Astrophysical Neutrinos](https://arxiv.org/abs/2511.17139)
*Zbigniew Plebaniak*

Main category: astro-ph.IM

TL;DR: The JEM-EUSO collaboration uses advanced UV camera technology and a multi-platform strategy, including space-based missions like the upcoming POEMMA and M-EUSO, to study ultra-high-energy cosmic rays and neutrinos by detecting fluorescence and Cherenkov light from space and high altitudes.


<details>
  <summary>Details</summary>
Motivation: To detect rare ultra-high-energy cosmic rays (above 10^20 eV) which are difficult to observe due to their low flux, and to understand extreme astrophysical processes through hybrid fluorescence/Cherenkov detection techniques.

Method: Development of ultra-fast UV cameras for Earth-skimming technique evaluation, hybrid detection combining fluorescence and Cherenkov methods across ground-based (EUSO-TA), balloon (EUSO-Balloon, SPB1/2), and space platforms (ISS Mini-EUSO), with future missions like POEMMA and M-EUSO.

Result: Demonstrated capability of UV cameras for atmospheric detection, validated models via ground experiments, gathered space-based data on UV backgrounds and TLEs, and advanced technology readiness for future missions.

Conclusion: The multi-platform approach successfully progresses towards space-based observations, enabling increased exposure to UHECRs and multi-messenger astrophysics via upcoming missions which will enhance understanding of extreme cosmic phenomena.

Abstract: The JEM-EUSO (Joint Exploratory Missions for Extreme Universe Space Observatory) collaboration is an international initiative studying ultra-high-energy cosmic rays and related phenomena. These particles, with energies exceeding 10$^{20}$~eV, provide insights into extreme astrophysical processes but remain challenging to detect due to their low flux. At the heart of JEM-EUSO's technology is an ultra-fast, highly sensitive UV camera capable of detecting EASs in the atmosphere with exceptional spatial and temporal resolution. A dedicated Cherenkov camera has been developed to evaluate the viability of the Earth-skimming technique from high altitudes. Fluorescence and Cherenkov detectors can be used together to create a hybrid detection surface. This innovative approach enables detailed studies of fluorescence and Cherenkov light from cosmic ray and neutrino interactions. The JEM-EUSO technology will allow for observations from space to significantly increase the exposure to these rare phenomena. The collaboration employs a multi-platform strategy with ground-based experiments like EUSO-TA calibrating detection systems and validating models, and balloon-borne missions such as EUSO-Balloon and EUSO-SPB1/SPB2 demonstrating observations from the stratosphere and testing technologies. Space-based missions, particularly Mini-EUSO on the ISS, have provided valuable data on UV backgrounds, TLEs, and meteoroids, as well as demonstrating the potential for future space-based detection. While we are developing a cross-platform methodology, we are ultimately moving towards space-based measurements. Future efforts include the POEMMA space mission, designed for stereoscopic observations of UHECRs and multi-messenger phenomena, the PBR experiment, which integrates radio detection and is scheduled to fly in 2027, and the M-EUSO satellite mission, proposed to ESA.

</details>


### [11] [Deep Investigation of Neutral Gas Origins (DINGO): Options for robust Deep Spectral Line Imaging in the SKA-Era](https://arxiv.org/abs/2511.17307)
*Jonghwan Rhee,Richard Dodson,Alexander Williamson,Martin Meyer,Kristóf Rozgony,Pascal J. Elahi,Matthew Whiting,Daniel Mitchell,Tobias Westmeier*

Main category: astro-ph.IM

TL;DR: The paper introduces a novel uv-grid stacking method for processing deep spectral line observations with next-generation radio interferometers, demonstrating superior flux recovery (99%) compared to the traditional image-stacking approach (92%) and avoiding artefacts. This method is proposed for use in the DINGO survey and future facilities.


<details>
  <summary>Details</summary>
Motivation: The growing data storage and computational challenges from next-gen instruments like ASKAP and SKA require alternatives to the default daily image-stacking method, which risks propagating systematic errors and degrades data quality. The traditional all-data joint processing is too resource-intensive, necessitating an efficient yet accurate intermediate solution.

Method: The proposed uv-grid stacking halts the imaging pipeline after forming daily residual visibility grids. These grids are then stacked and jointly deconvolved. This contrasts with the default image-stacking (stacking final images) and the resource-prohibitive traditional method of processing all data together.

Result: Compared to image-stacking which recovers 92% of true HI flux with harmful artefacts, uv-grid stacking achieves 99% recovery matching the traditional benchmark. It eliminates negative bowls around sources, proving better deconvolution and preservation of physical information.

Conclusion: The uv-grid stacking method provides a computationally feasible and scientifically superior alternative for large radio astronomy datasets. It is recommended for the DINGO survey on ASKAP and future facilities like the SKA.

Abstract: The data storage requirements for deep spectral line observations with next-generation radio interferometers like the Australian Square Kilometre Array Pathfinder (ASKAP) and the Square Kilometre Array (SKA) are extremely challenging. The default strategy is to reduce data after each daily observation and stack the resulting images. Although this approach is computationally efficient, it risks propagating systematic errors and significantly degrades the final data quality. However, storage and computation requirements for a traditional way to image the entire deep dataset together are prohibitively expensive. We present an alternative \textit{uv}-grid stacking method and compare its scientific outcomes with both the traditional approach, which processes all data jointly and serves as the best-possible result, and the default image-stacking method. Our technique involves halting the standard imaging pipeline after the daily residual visibility grids are formed. These grids are then stacked and jointly deconvolved to combine many epochs of data. Using the traditional approach as a benchmark, we show that image-stacking recovers only 92\% of the true {\HI} flux. In contrast, our \textit{uv}-grid stacking method recovers 99\%, which is in excellent agreement with the traditional method within the noise limits. Furthermore, image-stacking introduces significant non-physical artefacts, such as negative bowls around strong sources, indicating poor deconvolution and a loss of physical information. Based on these findings, we intend to apply the \textit{uv}-grid stacking to the Deep Investigation of Neutral Gas Origins (DINGO) survey on ASKAP and strongly recommend this or a similar approach for future radio astronomy facilities.

</details>


### [12] [Accelerating the CLEAN algorithm of radio interferometry with convex optimization](https://arxiv.org/abs/2511.17410)
*Hendrik Müller,Mingyu Hsieh,Sanjay Bhatnagar*

Main category: astro-ph.IM

TL;DR: The paper accelerates the traditional Cotton-Schwab CLEAN algorithm in radio interferometry by applying optimization techniques like Nesterov acceleration and conjugate gradient methods, achieving faster convergence and deeper residual reduction without major framework changes.


<details>
  <summary>Details</summary>
Motivation: Next-gen radio interferometers require faster data processing due to higher data rates and image sizes. Current methods like CLEAN are too slow for these demands, necessitating acceleration techniques.

Method: Reformulate CLEAN's major loop as a Newton scheme, then apply Nesterov acceleration and conjugate gradient orthogonalization. These modifications extend the traditional framework while maintaining simplicity.

Result: Proposed algorithms converge multiple times faster than传统 methods, significantly reduce residuals, and match performance gains from advanced minor loop replacements but with lower computational cost.

Conclusion: Accelerating the major loop via optimization techniques greatly improves CLEAN's efficiency. Combining this with enhanced minor loops offers the best performance, keeping CLEAN as the fastest and most robust method with easy implementation for greater speed.

Abstract: In radio-interferometry, we recover an image from an incompletely sampled Fourier data. The de-facto standard algorithm, the Cotton-Schwab CLEAN, is iteratively switching between computing a deconvolution (minor loop) and subtracting the model from the visibilities (major loop). The next generation of radio interferometers is expected to deal with much higher data rates, image sizes and sensitivity, making an acceleration of current data processing algorithms necessary. We aim to achieve this by evaluating the potential of various well-known acceleration techniques in convex optimization to the major loop. For the present manuscript, we limit the scope to study these techniques only in the CLEAN framework. To this end, we identify CLEAN with a Newton scheme, and use this chain of arguments backwards to express Nesterov acceleration and conjugate gradient orthogonalization in the major and minor loop framework. The resulting algorithms are simple extensions of the traditional framework, but converge multiple times faster than traditional techniques, and reduce the residual significantly deeper. These improvements achieved by accelerating the major loop are competitive to well-known improvements by replacing the minor loop with more advanced algorithms, but at lower numerical cost. The best performance is achieved by combining these two developments.CLEAN remains among the fastest and most robust algorithms for imaging in radio interferometry, and can be easily extended to an almost an order of magnitude faster convergence speed and dynamic range. The procedure outlined in this manuscript is relatively straightforward and could be easily extended.

</details>


### [13] [Performance Simulations for Kola: Achieving High-Resolution, Visible-Light AO Correction Over a 1 Arcminute Field](https://arxiv.org/abs/2511.17488)
*Brianna Peck,Jessica R. Lu,Lianqi Wang,Brooke DiGia,Richard Dekany,Antonin H. Bouchez,Peter Wizinowich,Maxwell A. Millar-Blanchaer,Mark Chun,Philip Hinz,Charles-Antoine Claveau*

Main category: astro-ph.IM

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present performance simulations for a proposed visible-light, multi-conjugate adaptive optics system for the 10-meter W. M. Keck I telescope that aims to deliver near diffraction-limited angular resolution at optical wavelengths. Our proposed architecture, the Keck Optical Laser Guide Star Adaptive Optics System (KOLA), combines multiple laser guide stars (LGS) and deformable mirrors to enable wide-field correction across a 60 arcsecond field of view. Simulations were conducted using the open-source Multi-Threaded Adaptive Optics Simulator (MAOS), which we validated against on-sky data for the current Keck I adaptive optics system. We evaluated KOLA performance across a range of design parameters and report key point spread function metrics, including Strehl ratio, full width at half maximum, and encircled energy radius. Example science-driven requirements include resolving black hole spheres of influence, probing crowded stellar fields, and imaging protoplanetary disks. Trade studies on actuator count and laser guide star configuration help inform future design decisions. We present a nominal KOLA design (10 LGS, 3 tip-tilt natural guide stars (TTNGS), and 3600 actuators on the adaptive secondary mirror). Performance simulations show a 15 mas angular resolution with a Strehl ratio of 34% at 652 nm on-axis. More work is needed to explore alternative LGS/TTNGS asterisms, optimize conjugation heights for high-altitude deformable mirrors, and test performance under poorer seeing conditions.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [14] [A new suite of Lund-tree observables to resolve jets](https://arxiv.org/abs/2511.16723)
*Melissa van Beekveld,Luca Buonocore,Silvia Ferrario Ravasio,Pier Francesco Monni,Alba Soto-Ontoso,Gregory Soyez*

Main category: hep-ph

TL;DR: The paper introduces Lund-Tree Shapes (LTS), a new class of collider observables derived from declustering trees in the Lund jet plane. These observables can assess jet radiation geometry, structure, and multiplicity, useful for higher-order calculations and parton shower comparisons, showing simplicity in all-order structure without non-global logarithmic corrections.


<details>
  <summary>Details</summary>
Motivation: To develop a versatile set of observables for multi-jet final states that can bridge fixed-order calculations and parton showers while avoiding complexities like non-global logarithms, enhancing precision in collider physics analyses.

Method: LTS are built using Lund jet plane representations to create declustering trees. They operate both as differential event shape-like variables and as integrated n-jet rates. Analytical predictions are derived at NNLL accuracy and matched to NLO for LHC applications.

Result: Successful derivation of LTS predictions at NNLL for two QCD leg processes across colliders, demonstrating their utility in precision calculations and shower algorithm validation.

Conclusion: LTS observables offer a powerful tool for probing multi-jet dynamics, improving theoretical frameworks through logarithmic accuracy checks, and enabling robust collider phenomenological studies with reduced computational complexity.

Abstract: We introduce a class of collider observables, named Lund-Tree Shapes (LTS), defined from declustering trees originating from the Lund jet plane representation of the QCD radiation pattern in multi-jet scattering processes. At the differential level, they are continuous global variables akin classical event shapes and $n\to n+1$ jet-resolution parameters, which probe the geometry and hierarchical structure of the radiation in an event. At the integrated, cumulative level, they naturally define $n$ jet rates, providing a jet-multiplicity-based characterisation of multi-jet final states. Their definition applies to scattering processes with any number of resolved jets in the final state, as well as to groomed jets. They are thus usable as resolution variables in the context of higher-order calculations via phase-space slicing, matching fixed-order calculations to parton showers, and testing the logarithmic accuracy of shower algorithms. From a theoretical viewpoint, such observables feature a simple all-order structure and are free of non-global logarithmic corrections. As an initial application, we derive next-to-next-to-leading-logarithmic accurate predictions for processes with two QCD legs at $e e$, $pp$ and $e p$ colliders, and matched predictions to next-to-next-to-leading order for the LHC, discussing aspects of collider phenomenology.

</details>


### [15] [WIMP Meets ALP: Coherent Freeze-Out of Dark Matter](https://arxiv.org/abs/2511.16731)
*Steven Ferrante,Maxim Perelstein,Bingrong Yu*

Main category: hep-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We consider the cosmological history of a weakly interacting massive particle (WIMP) coupled to a light axion-like particle (ALP) via a quadratic coupling. Although the coupling is too feeble to thermalize the ALP, coherent forward scattering between the two sectors induces temperature-dependent mass shifts that substantially modify both WIMP freeze-out and ALP misalignment dynamics, giving rise to a novel coherent freeze-out mechanism. At high temperatures, the WIMP thermal bath spontaneously breaks the symmetry of the ALP potential, displacing the field to a new vacuum. The resulting back-reaction reduces the WIMP effective mass and delays its freeze-out. Depending on the strength of the coupling, symmetry restoration occurs via either a first-order phase transition (FOPT) or a crossover. In the FOPT regime, dark matter consists solely of WIMPs, whose delayed freeze-out permits annihilation cross sections up to two (five) orders of magnitude above the standard value for $s$-wave ($p$-wave) annihilation, while still yielding the correct relic density. In the crossover regime, both WIMP and ALP can contribute to dark matter. Remarkably, we find an "ALP miracle": a Planck-suppressed quadratic coupling yields an ALP abundance comparable to the observed dark matter density, largely independent of its initial displacement and mass.

</details>


### [16] [Radiative Neutrino Mass Generation and Dark Matter through Vector-like Leptons](https://arxiv.org/abs/2511.16762)
*Mohamed Amin Loualidi,Salah Nasri,Maximiliano A. Rivera*

Main category: hep-ph

TL;DR: This paper proposes a three-loop radiative model for neutrino masses using asymmetric Yukawa couplings between new scalar doublets and vector-like leptons, which also provides dark matter candidates and is compatible with experimental data.


<details>
  <summary>Details</summary>
Motivation: To explain neutrino masses and address the dark matter problem while avoiding excessive flavor-changing processes, the model employs a radiative mechanism and asymmetric couplings to ensure compatibility with experimental constraints.

Method: The model introduces two $SU(2)_L$ scalar doublets interacting with vector-like leptons via asymmetric Yukawa couplings. Neutrino masses arise at three loops through scalar mixing involving dark matter candidates, with flavor-violation suppressed by the structure. Parameter space is constrained using dark matter, neutrino data, and $μ→eγ$ limits.

Result: The model generates three distinct neutrino masses from a single vector-like lepton generation and satisfies current experimental bounds. It predicts testable signatures for future particle physics experiments.

Conclusion: The framework successfully unifies neutrino mass generation and dark matter within a loop-induced mechanism, offering a viable and testable model consistent with observational and experimental data.

Abstract: This study presents a radiative three-loop framework for neutrino mass generation, employing an asymmetric Yukawa coupling between two new scalar $SU(2)_L$ doublets and vector-like lepton doublets. Dark matter candidates arise from the scalar sector of one of the doublets and play a crucial role in the generation of neutrino masses through their nonzero scalar mixing. The singly charged scalar sector undergoes an analogous mixing structure. A single generation of vector-like leptons yields three nondegenerate neutrino masses as a consequence of the asymmetric Yukawa combinations entering the neutrino mass matrix. The model is tested against dark matter phenomenology, neutrino mass and mixing data, and the charged lepton flavor-violating process $μ\rightarrow e γ$, showing compatibility with current bounds and leading to experimentally accessible predictions.

</details>


### [17] [Competition of $χ_{c}(2P)$ quarkonia and continuum in $e^+ e^- \to e^+ e^- D \bar{D}$](https://arxiv.org/abs/2511.16862)
*Izabela Babiarz,Piotr Lebiedowicz,Wolfgang Schäfer,Antoni Szczurek*

Main category: hep-ph

TL;DR: The paper analyzes the production of D meson pairs ($D ar{D}$) in $e^+ e^-$ collisions via continuum mechanisms involving $D^*$ meson exchanges. It concludes that a previously observed bump at 3.8 GeV likely originates from continuum processes rather than a $χ_{c0}(3860)$ resonance. It also estimates branching fractions and decay widths for the $χ_{c2}(3930)$ resonance, finding agreement with experimental data.


<details>
  <summary>Details</summary>
Motivation: The study aims to clarify the nature of a resonance-like bump observed in Belle and BaBar experiments and to evaluate the production mechanisms of $χ_{c2}(3930)$ through continuum and resonance contributions, providing predictions for future Belle II experiments.

Method: Using $t/u$-channel vector-meson $D^*$ exchanges in $e^+e^−$ collisions, calculations incorporate $D^* D γ$ coupling constants derived from $D^* 	o D γ$ decays. Theoretical models like the Buchmüller-Tye potential in the light-front approach (NRQCD limit) are employed to estimate decay widths and branching fractions.

Result: The $D^0 ar{D}^0$ channel shows a large contribution compared to the smaller $D^+ D^-$ channel. The bump at 3.8 GeV is attributed to continuum production rather than a resonance. For $χ_{c2}(3930)$, branching fraction into $D\bar{D}$ is found as 58±13% with $Γ_{γγ} × B = 0.32 ±0.07$ keV, aligning with experiment data.

Conclusion: The observed 3.8 GeV bump likely stems from continuum processes, not the $χ_{c0}(3860)$ resonance. The analysis supports the $χ_{c2}(3930)$ as a $χ_{c2}(2P)$ state with validated decay parameters. Predictions for Belle II measurements are provided based on model calculations.

Abstract: We discuss the production of $D \bar{D}$ pairs in $e^+ e^-$ collisions, where $D$ refers to either $D^0$ or $D^+$. The continuum mechanism with the $t/u$-channel vector-meson $D^*$ exchanges are considered. The results of the calculation depend on the parameter of the off-shell form-factor for the virtual $D^*$ mesons. The $D^* D γ$ coupling constants are found from the $D^* \to D γ$ decays. We find relatively large contribution for the $D^0 {\bar D}^0$ channel and much smaller contribution in the $D^+ D^-$ channel. In the second case we consider also the $D^{\pm}$ exchanges. We conclude that the bump at $M_{D^0 {\bar D}^0} = 3.8$ GeV observed by the Belle and BaBar Collaborations has rather continuum origin than it corresponds to the broad resonance $χ_{c0}(3860)$. We discuss also production of the $χ_{c2}(3930)$ resonance which is a candidate for the $χ_{c2}(2P)$ state. This state can decay into both $D \bar{D}$ channels, however the branching fractions are not well known at present. From a comparison of our model results to the BaBar data we find $B(χ_{c2}(3930) \to D \bar{D}) = 0.58 \pm 0.13$ using the two-photon width $Γ_{γγ} = 0.544$ keV (obtained for the Buchmüller-Tye potential) evaluated within the light-front approach (NRQCD limit). Our finding of $Γ_{γγ} \times B(χ_{c2}(3930) \to D \bar{D}) = 0.32 \pm 0.07$ keV is close to the Belle and BaBar results. Realistic predictions of the differential distributions in several variables and integrated cross-sections are given for the Belle II kinematics.

</details>


### [18] [Next-to-leading order analysis of $J/ψ+ γ$ production in photon-photon collisions at CEPC](https://arxiv.org/abs/2511.16881)
*Ying-Zhao Jiang,Zhan Sun*

Main category: hep-ph

TL;DR: The paper investigates J/ψ+γ production in γγ collisions at CEPC energy using NRQCD, showing direct photon processes dominate, polarization sensitive to color-octet effects and specific LDMEs.


<details>
  <summary>Details</summary>
Motivation: To test LDME universality and resolve J/ψ polarization puzzles using precise e⁺e⁻ collision data.

Method: Next-to-leading order α_s calculations in NRQCD factorization for γγ collisions at CEPC energy, analyzing resolved vs direct photon contributions and polarization effects from color-octet mechanisms.

Result: Direct photon process dominates with significant annual J/ψ yields; polarization parameters are strongly influenced by ^3P_J^{[8]} LDME but not ^1S_0^{[8]} or ^3S_1^{[8]} LDMEs.

Conclusion: γγ collisions at CEPC provide a high-precision environment to test NRQCD LDME models and address J/ψ polarization discrepancies through polarization measurements.

Abstract: We systematically investigate the production of $J/ψ+ γ$ in $γγ$ collisions at next-to-leading order in $α_s$ within nonrelativistic QCD (NRQCD) factorization. Calculations for CEPC energy region show the resolved photon contribution is negligible, while the direct photon process dominates, yielding substantial annual $J/ψ$ yields. Significant modifications to $J/ψ$ polarization parameters emerge from color-octet mechanisms, and different NRQCD long distance matrix elements (LDMEs) further yield distinct polarization patterns. Furthermore, the polarization predictions are highly sensitive to the $^3P_J^{[8]}$ LDME, while being insensitive to the $^1S_0^{[8]}$ and $^3S_1^{[8]}$ LDMEs. Leveraging the cleaner environment of $e^+e^-$ collisions versus hadronic processes, the production of $J/ψ$ associated with a photon in $γγ$ collisions provides a high-precision platform to test LDMEs universality and resolve longstanding $J/ψ$ polarization puzzles.

</details>


### [19] [Unstable Particles in Quantum Field Theory](https://arxiv.org/abs/2511.16941)
*Scott Willenbrock*

Main category: hep-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In honor of Dave Roper's 90th birthday, I present a pedagogical introduction to our modern understanding of unstable particles in Quantum Field Theory, paying close attention to the analytic structure of the propagator, with occasional remarks on the Roper resonance. I discuss the mass and decay rate of unstable particles, Breit-Wigner resonance formulae and width, poles and branch cuts, and pole trajectories.

</details>


### [20] [Rescaled Leptonic Unitary Triangles and Rephasing Invariants](https://arxiv.org/abs/2511.16942)
*Shu Luo*

Main category: hep-ph

TL;DR: This paper explores the effects of matter density on CP-conserving quartet observables (R) and the Jarlskog invariant (J) in neutrino oscillations, showing that matter-induced R_invariants are linear combinations of vacuum R values and how they evolve with increasing matter density.


<details>
  <summary>Details</summary>
Motivation: To understand how neutrino oscillation parameters, particularly CP-conserving quartets and CP violation measures, are affected by matter effects, extending beyond the known Naumov relation for J to analogous relations for R quartets.

Method: Systematic analysis of nine CP-conserving quartets R_{γk} and the Jarlskog invariant J. Examined their vacuum and matter-coupled (tilde R) versions, derived relationships between them, performed numerical studies using latest neutrino mixing parameters and global fit data.

Result: Found that matter-effective R_invariants are linear combinations of vacuum R values. Established new relations paralleling the Naumov relation for J. Quantified the evolution of these invariants with varying matter density through numerical simulations.

Conclusion: Matter effects systematically alter CP-conserving quartets but preserve underlying relationships, offering insights for future long-baseline neutrino experiments aiming to disentangle CP violation signals from matter effects in oscillation probabilities.

Abstract: The field of neutrino physics has made significant progress in measuring the strength and frequency of neutrino and antineutrino oscillations in the past two decades. It is clear that the amplitudes involved in the neutrino oscillation probabilities are all phase-reshaping invariants of the quartet forms of the elements of the PMNS mixing matrix. We show in this paper how these quartet observables can be directly linked to the rescaled leptonic unitarity triangles within the framework of three active neutrinos. We provide a systematic discussion of the nine CP-conserving quartets ${\cal R}^{}_{γk} \equiv {\rm Re} \left [ V^{}_{αi} V^{}_{βj} V^{*}_{αj} V^{*}_{βi}\right ] $ along with the universal Jarlskog invariant of CP violation ${\cal J} \equiv \sum_γε^{}_{αβγ} \sum_k ε^{}_{ijk} \; {\rm Im} \left [ V^{}_{αi} V^{}_{βj} V^{*}_{αj} V^{*}_{βi} \right ]$, and place particular emphasis on the matter effect on these quartets. In addition to the well-known Naumov relation for the Jarlskog invariant ${\cal J}$, similar relations connecting ${\cal R}$ in vacuum and its effective counterparts $\widetilde{\cal R}$ in matter are introduced and examined in detail. We find that the effective CP-conserving invariants $\widetilde{\cal R}^{}_{αi}$ in matter can be regarded as linear combinations of their vacuum counterparts. With the latest global fit data of neutrino masses and mixing elements, numerical analyses are carried out to give an intuitive understanding of how these phase-rephasing invariants evolve as the matter density increases.

</details>


### [21] [Charm associated production of the pseudoscalar Higgs boson $h''$ in the MCPM' at the LHC](https://arxiv.org/abs/2511.17026)
*Rafał Maciuła,Markos Maniatis,Otto Nachtmann,Antoni Szczurek*

Main category: hep-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We discuss charm associated production of the pseudoscalar Higgs boson $h''$ in the MCPM'. In our analysis we assume $m_{h''}$ = 95.4 GeV, which corresponds to an enhancement observed by the CMS collaboration in the $γγ$ channel. As discussed recently, the MCPM' is consistent with the CMS enhancement. In this model the $h''$ Higgs boson of this mass decays dominantly into $c \bar c$ jets. The cross section for the associated $h''$ production in $p p \to h'' c \bar c$ is found to be almost as big as that for the Drell-Yan (DY) $c \bar c \to h''$ production, much larger than that for two-gluon fusion $g g \to h''$. Since in the MCPM' the dominant $h''$ decay is into the $c \bar c$ channel we have to deal with the $p p \to c \bar c c \bar c$ process with $c$ and $\bar c$ jets. In addition to the signal processes, we have sizeable single parton scattering (SPS) and double parton scattering (DPS) nonresonant background. In our analysis we include diagrams of gluon-gluon fusion $g g \to h''$ which lead to the $c \bar c c \bar c$ final state. We discuss also the possible role of the reaction $p p \to h'' h''$. Assuming 100\% c and $\bar c$ jet tagging we discuss in detail how to improve the signal-to-background ratio. Our analysis shows promising resonance-like enhancements and relatively large cross sections in the $c \bar c c \bar c$ channel within the MCPM'. The ALICE-3 project would be ideal in the future to perform the experimental studies corresponding to the theoretical studies discussed here.

</details>


### [22] [Formation of primordial black holes through Q-balls](https://arxiv.org/abs/2511.17075)
*Shinta Kasuya,Masahiro Kawasaki,Alexander Kusenko,Shunsuke Neda*

Main category: hep-ph

TL;DR: This paper examines whether Q-balls, non-topological solitons from scalar field theories, can form primordial black holes (PBHs). By analyzing density perturbations from Q-ball charge distributions and updating PBH formation criteria, they show that Q-balls from gauge-mediated SUSY breaking could produce enough PBHs (10^-15 to 5×10^-12 solar masses) to account for all dark matter, linking this to a SUSY breaking scale of ~10^6 GeV and TeV-scale SUSY particles.


<details>
  <summary>Details</summary>
Motivation: PBHs are candidates for dark matter, but their formation mechanisms need exploration. Q-balls, which can form in SUSY models, offer a potential source of density perturbations leading to PBH formation.

Method: Developed a formula for calculating density perturbations from Q-ball charge distributions. Re-examined PBH formation conditions in the matter-dominated era, extending existing super-horizon formulas to sub-horizon perturbations. Applied this framework to gauge-mediated SUSY breaking Q-balls using lattice simulation data.

Result: Showed that Q-balls in gauge-mediated SUSY breaking produce density perturbations sufficient to form PBHs in the mass range 10^-15 to 5×10^-12 M☉. These PBHs could constitute all dark matter. The required SUSY breaking scale (~10^6 GeV) aligns with TeV-scale SUSY particle predictions.

Conclusion: Q-ball-induced PBHs are viable dark matter candidates in certain SUSY models, bridging particle physics and cosmology. This supports gauge-mediated SUSY breaking scenarios and provides a target for future dark matter and SUSY detection experiments.

Abstract: We study the primordial black hole (PBH) formation from Q-balls that are non-topological solitons in scalar field theories. We develop a formula for calculating the density perturbations from the Q-ball charge distribution. We also re-examine the condition for the PBH formation in the matter-dominated era and show that the previously derived formula for super-horizon density fluctuations can be applied to the sub-horizon density perturbations. As an example, we consider the Q-balls in the case of gauge-mediated supersymmetry (SUSY) breaking, whose charge distribution was obtained by the lattice simulation. We find that the density perturbations are large enough to produce a significant number of PBHs with mass $10^{-15}\,M_\odot -5\times 10^{-12}\, M_\odot$, which can explain all the dark matter in the universe. In the context of supersymmetry, this mass range corresponds to the SUSY breaking scale $\sim 10^6$ GeV, which is consistent with the SUSY particle masses $\sim 10$ TeV.

</details>


### [23] [Short-flow-time expansion of non-singlet twist-two operators at next-to-next-to-leading order QCD](https://arxiv.org/abs/2511.17145)
*Robert V. Harlander,Jonas T. Kohnen,Andrea Shindler*

Main category: hep-ph

TL;DR: The paper presents a method to convert moments of parton distribution functions (PDFs) calculated using lattice QCD with the gradient-flow formalism into the MSbar scheme. It calculates the necessary matching coefficients for the first six non-singlet PDF moments up to next-to-next-to-leading order in perturbation theory.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide a precise conversion mechanism between the gradient-flow scheme and the MSbar scheme for PDF moments, enabling reliable extraction of PDFs from lattice QCD simulations. This is crucial for improving the accuracy of particle physics predictions that rely on PDFs.

Method: The authors use the gradient-flow formalism combined with a short-flow-time expansion. They compute perturbative matching coefficients for the first six non-singlet PDF moments up to NNLO in the strong coupling constant. The calculations involve high-order QCD perturbation theory and lattice QCD techniques.

Result: They successfully determine the matching coefficients for the first six non-singlet PDF moments up to next-to-next-to-leading order. These coefficients reduce theoretical uncertainties in converting gradient-flow results to the MSbar scheme, enhancing the precision of PDF determinations.

Conclusion: The derived coefficients enable more accurate conversions of lattice QCD PDF results to the widely-used MSbar scheme. This advancement supports higher precision in QCD analyses and particle physics predictions dependent on PDFs.

Abstract: The gradient-flow formalism provides a framework for the direct determination of moments of parton distribution functions (PDFs) from lattice QCD calculations. Their conversion from the gradient-flow scheme to $\overline{\text{MS}}$ requires the matching coefficients of the short-flow-time expansion, which can be computed perturbatively. We determine these coefficients for the first six non-singlet PDF moments up to next-to-next-to-leading order in the strong coupling.

</details>


### [24] [Investigation of the Spectator Effect on Light Nuclei Production in Nucleus-Nucleus Collisions at High Baryon Density Region](https://arxiv.org/abs/2511.17273)
*Li'Ang Zhang,Hongcan Li,Junyi Han,Yaping Wang,Junlin Wu,Guannan Xie,Gao-Chan Yong*

Main category: hep-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The light nuclei yields and yield ratios, as sensitive probes of the QCD phase transition, have been precisely measured at various collision energies.However, due to limited detector acceptance, the full $p_{\rm T}$ integral yield often requires extrapolation of the measured transverse momentum spectra to the unmeasured low-$p_{\rm T}$ region via functional fitting.Simulations with the AMPT-HC transport model and an after-burner coalescence approach indicate a significant low-$p_{\rm T}$ enhancement in peripheral collisions or forward rapidities, primarily originating from spectator nucleons.Consequently, conventional extrapolations tend to underestimate the true light nuclei yields.

</details>


### [25] [Cascades of gluons at high energies and their QI measures](https://arxiv.org/abs/2511.17288)
*Krzysztof Kutak,Michał Praszałowicz*

Main category: hep-ph

TL;DR: Thepaperextends1Ddipolecascademodelstoincludesaturationandtransitiontovacuum,usingquantuminformationtoolstoanalyzemodelproperties.ItalsodescribesthehadronicentropymeasuredbyLHCbandprovidespredictionsforpuritymeasurements.


<details>
  <summary>Details</summary>
Motivation: Toimproveexistingone-dimensionaldipolecascademodelsbyincorporatingsaturationeffectsandvacuumtransition,andtoapplyquantuminformationmethodsforanalysis.Additionally,toexplainLHCb'sentropymeasurementsandpredictpurityobservations.

Method: Extending1Ddipolecascademodelswithsaturationmechanisms;applyingquantuminformationtheorytoolsformodelanalysis;describinghadronicentropydatafromLHCb;developingpredictiveframeworkforpuritymeasurements.

Result: Successfulincorporationofsaturationandvacuumtransitionintothemodel.Quantuminformationanalysisprovidesnewinsights.ModelaccuratelydescribesLHCbentropymeasurementsandoffersconcretepredictionsforupcomingpurityexperiments.

Conclusion: Extendedmodelseffectivelyhandlesaturationregimesandvacuumtransitions,confirmingtheutilityofquantuminformationmethodsinhigh-energyphysics.PredictionsforpuritymeasurementsprovidetestableoutcomesforfutureLHCbruns.

Abstract: In this contribution we report on recent extension of one dimensional dipole cascade models to account for saturation and transition to vacuum \cite{Kutak:2025syp}. We analyze properties of the models using Quantum Information tools. Furthermore, we present a description of the hadronic entropy measured by LHCb and predictions for the purity measurement. %we provide description of hadronic entropy as measured by LHCb and provide predictions for measurement of purity.

</details>


### [26] [Parameter Inference from Final-State Entanglement in Higgs Decays](https://arxiv.org/abs/2511.17321)
*Jia Liu,Masanori Tanaka,Xiao-Ping Wang,Jing-Jun Zhang,Zifan Zheng*

Main category: hep-ph

TL;DR: The authors propose using entanglement entropy from Higgs decays to probe Standard Model parameters, finding it peaks near observed Higgs and W masses, supporting its use as a new method for parameter determination.


<details>
  <summary>Details</summary>
Motivation: To explore an intrinsic quantum-information approach using entanglement entropy in particle decay processes as a probe of fundamental parameters within the Standard Model, offering a new perspective beyond traditional methods.

Method: Compute entanglement entropy among final-state spins and colors in all Higgs decay channels after tracing out kinematics, then apply a near-maximal entanglement-entropy criterion to identify optimal parameter regions.

Result: The entanglement entropy reaches a global maximum near the experimentally observed Higgs mass and W boson mass (equivalent to the SU(2)_L coupling). In a two-parameter model, it favors an SM-like coupling balance, constraining parameter ratios.

Conclusion: Entanglement extremality is shown to be a viable complementary tool for constraining fundamental parameters in particle physics models, suggesting its broader applicability in theoretical analyses.

Abstract: The decay out-states of unstable Standard Model (SM) particles provide a unique, well-defined intrinsic quantum-information probe of the SM parameter space. We use Higgs decays as a test case: after tracing out kinematics, we compute entanglement among final-state spins and colors across all decay channels and impose a near-maximal entanglement-entropy criterion. This criterion yields quantitative indications for fundamental parameters. Within the SM, the entanglement entropy exhibits a global maximum close to the observed Higgs mass and the measured $W$ mass, the latter being equivalent to the $SU(2)_L$ gauge coupling. In a two-parameter kappa framework, applying the same criterion points to an SM-like balance between vector and fermion couplings, constraining the ratio of the sector-wide rescalings. These results suggest that entanglement extremality can serve as a complementary handle on fundamental parameters.

</details>


### [27] [POPxf: An Exchange Format for Polynomial Observable Predictions](https://arxiv.org/abs/2511.17348)
*Ilaria Brivio,Ken Mimasu,Peter Stangl,Anke Biekötter,Ana R. Cueto Gómez,Charlotte Knight,Luca Mantani,Eleonora Rossi,Alejo N. Rossia,Aleks Smolkovič*

Main category: hep-ph

TL;DR: POPxf is a new machine-readable format for sharing semi-analytical predictions in high energy physics, designed to enhance reproducibility and facilitate global fits by encoding observables as polynomials of model parameters with detailed metadata and uncertainty handling.


<details>
  <summary>Details</summary>
Motivation: To address the need for standardized, transparent sharing of theoretical predictions that allow precise reproduction, reinterpretation, and integration into global analyses, especially for Effective Field Theories involving polynomial model dependencies.

Method: Develop a structured data format (POPxf) that encodes observables as polynomials in model parameters, includes explicit metadata about assumptions, uncertainties, and correlations, ensuring flexibility and machine-readability.

Result: A format that enables researchers to easily reuse and build upon theoretical predictions, supporting parameter-dependent uncertainty treatments and promoting interoperability across particle physics experiments and analyses.

Conclusion: POPxf improves scientific rigor and efficiency in high energy physics by providing a common framework for sharing complex theoretical predictions with full transparency, aiding both individual research and collaborative global efforts.

Abstract: We introduce the Polynomial Observable Prediction Exchange Format, POPxf, a structured, machine-readable data format for the publication and exchange of semi-analytical theoretical predictions in high energy physics. The format is designed to encode observables that can be expressed in terms of polynomials in model parameters, with particular emphasis on Effective Field Theory applications. All relevant assumptions and metadata are recorded explicitly, and the treatment of uncertainties and correlations is flexible enough to capture parameter-dependent effects. The format aims to improve reproducibility, facilitate global fits and reinterpretations, and streamline the use of theoretical predictions across the particle physics community.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [28] [Exploring the gauge flexibility of the linear-in-spin effective-one-body Hamiltonian at the 5.5 post-Newtonian order](https://arxiv.org/abs/2511.16747)
*Andrea Placidi,Luca Sebastiani,Gianluca Grignani*

Main category: gr-qc

TL;DR: The paper derives the gauge-general expressions for two gyro-gravitomagnetic functions in the EOB Hamiltonian up to 5.5PN order, including both local and nonlocal contributions. It computes gauge-invariant observables like binding energy and periastron advance, comparing DJS and anti-DJS gauges. The anti-DJS gauge shows slight improvement for inspiral dynamics in equal-mass, equal-spin binaries.


<details>
  <summary>Details</summary>
Motivation: To provide a general analytical framework for spin-orbit dynamics in EOB formalism, accounting for different spin-gauge choices, and to validate which gauge better aligns with test-particle limits and numerical data.

Method: Deriving gauge-general expressions for gyro-gravitomagnetic functions up to 5.5PN order within EOB. Calculating gauge-invariant observables (binding energy, periastron advance) for quasi-circular orbits. Comparing DJS and anti-DJS gauges by evaluating their performance in reproducing Kerr background interactions in test-mass limits.

Result: The derived expressions unify the spin-orbit sector with full gauge dependence. The anti-DJS gauge yields a marginally better description of inspiral dynamics for equal-mass, aligned-spin binaries compared to DJS gauge.

Conclusion: The anti-DJS gauge may be more advantageous for future EOB waveform models due to its improved accuracy in certain scenarios, particularly when aligning with test-particle limits and numerical relativity results.

Abstract: We derive the gauge-general expressions of the two gyro-gravitomagnetic functions entering the spin-orbit sector of the effective-one-body (EOB) Hamiltonian up to the fifth-and-half post-Newtonian (5.5PN) order. Our results include both local and nonlocal-in-time contributions, providing the most general analytical formulation of the linear-in-spin conservative dynamics within the EOB framework. These expressions are then employed to compute two gauge-invariant observables for quasi-circular orbits: the binding energy and the fractional periastron advance. We also use them to compare two spin-gauge choices: the well-known Damour-Jaranowski-Schäfer ($\rm DJS$) gauge, in which the gyro-gravitomagnetic functions are independent of the orbital angular momentum, and the alternative anti-$\rm DJS$ (or $\overline{\rm DJS}$) gauge, designed to reproduce in the test-mass limit the spin-orbit interaction of a spinning test particle in a Kerr background. For a circular, equal-mass, equal-spin binary, our analysis indicates that the $\overline{\rm DJS}$ gauge provides a slightly improved description of the inspiral dynamics, suggesting potential advantages for its use in future EOB waveform models.

</details>


### [29] [LISA as a probe of pre-big-bang physics: a nested sampling analysis](https://arxiv.org/abs/2511.16748)
*Xoán Vilas Currás,Gianluca Calcagni*

Main category: gr-qc

TL;DR: The paper analyzes the gravitational-wave background (GWB) predicted by pre-big-bang cosmology using nested sampling, showing LISA's potential to constrain model parameters with 18% uncertainties, offering empirical evidence for string theory.


<details>
  <summary>Details</summary>
Motivation: To investigate how LISA can constrain pre-big-bang cosmology parameters through GWB analysis, providing observational tests for string theory predictions.

Method: Nested sampling analysis applied to simulated LISA data, incorporating foregrounds and studying both minimal and non-minimal pre-big-bang models. Parameters include H₁, m, σᵢ, β.

Result: Most stringent constraints on H₁, σᵢ, β occur with left-bend GWB features, while m is best constrained with right-bend features. 68% confidence uncertainty reaches ~18% for H₁ and m under favorable conditions.

Conclusion: LISA观测可显著限制预大爆炸模型参数，若检测到信号，可能为弦理论提供经验依据。

Abstract: Using a nested sampling analysis, we study the gravitational-wave background (GWB) predicted by pre-big-bang cosmology, both in its minimal and non-minimal version. Within the LISA sensitivity range, the GWB signal is a flat or a broken power law, parametrized by four fundamental quantities: the Hubble parameter at the curvature bounce $H_1$, the axion mass $m$, the initial amplitude of the axion field $σ_i$ and the exponent $β$ governing the high-energy growth of the dilaton and the dynamics of the internal dimensions. We determine the posterior distributions of these parameters based on how LISA would detect such signal. Including the galactic and extra-galactic foregrounds in the analysis, the most stringent constraints on $H_1$, $σ_i$ and $β$ are obtained when the signal exhibits a left-bend feature, while for $m$ this happens for a right-bend feature. Relative uncertainties reach $ΔH_1/H_1 ,\,Δm/m \sim 18\%$ at $68\%$ confidence level under favourable conditions. LISA will thus be capable of placing significant constraints on the pre-big-bang model, potentially providing empirical hints of string theory in the case of detection.

</details>


### [30] [Numerical tiling-based simulations of decoherence in multifield models of inflation](https://arxiv.org/abs/2511.16801)
*Johor D. Peñalba Quispitupa,Guillermo F. Quispe Peña,Jose T. Galvez Ghersi*

Main category: gr-qc

TL;DR: The paper presents a numerical method to study the evolution of primordial scalar perturbations under decoherence effects using Lindblad equation corrections, providing a flexible and efficient computational framework compatible with nonlinear codes.


<details>
  <summary>Details</summary>
Motivation: The motivation is to extend previous work on two-point correlators by enabling numerical studies of decoherence effects during reheating without relying on slow-roll approximations, allowing arbitrary state deformations and control over decoherence parameters.

Method: The method involves numerically implementing a mode decomposition into fast/slow components and applying controlled state deformations via Lindblad equation-based environment corrections. It generalizes to any number of degrees of freedom and allows configuring decoherence event sequences with adjustable parameters (duration, shape, amplitude, wavelength).

Result: The computational routine is efficient and outputs compatible with nonlinear codes, facilitating analysis of decoherence effect propagation through reheating phases.

Conclusion: This framework advances the study of quantum-to-classical transitions in cosmology by offering a versatile tool to model decoherence impacts on primordial perturbations beyond simplistic approximations.

Abstract: In previous work, we developed a method for computing two-point correlators by decomposing the mode degrees of freedom into fast and slow components. Building on this framework, we present a numerical implementation to study the evolution of primordial scalar perturbations under controlled state deformations induced by the simplest environment corrections from the Lindblad equation. Our approach generalizes to an arbitrary number of degrees of freedom and does not rely on the slow-roll approximation. The computational routine is numerically efficient and allows users to configure arbitrary sequences of decoherence events, with full control over their duration, shape, amplitude and effective wavelength range. The resulting outputs are compatible with nonlinear numerical codes, enabling studies of how decoherence effects propagate during reheating.

</details>


### [31] [All-sky search for continuous gravitational-wave signals from unknown neutron stars in binary systems in the first part of the fourth LIGO-Virgo-KAGRA observing run](https://arxiv.org/abs/2511.16863)
*The LIGO Scientific Collaboration,the Virgo Collaboration,the KAGRA Collaboration*

Main category: gr-qc

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present the results of a blind all-sky search for continuous gravitational-wave signals from neutron stars in binary systems using data from the first part of the fourth observing run (O4a) using LIGO detectors data. Rapidly rotating, non-axisymmetric neutron stars are expected to emit continuous gravitational waves, whose detection would significantly improve our understanding of the galactic neutron star population and matter under extreme conditions, while also providing valuable tests of general relativity. Neutron stars in binary systems likely constitute a substantial fraction of the unobserved galactic population and, due to potential mass accretion, may emit stronger gravitational-wave signals than their isolated counterparts. This search targets signals from neutron stars with frequencies in the 100-350 Hz range, with orbital periods between 7 and 15 days and projected semi-major axes between 5 and 15 light-seconds. The analysis employs the GPU-accelerated fasttracks pipeline. No credible astrophysical signals were identified, and, in the absence of a detection, we report search sensitivity estimates on the population of neutron stars in binary systems in the Milky Way.

</details>


### [32] [Accelerating parameter estimation for parameterized tests of general relativity with gravitational-wave observations](https://arxiv.org/abs/2511.16879)
*Dhruv Kumar,Ish Gupta,Bangalore Sathyaprakash*

Main category: gr-qc

TL;DR: The paper introduces an improved method using relative binning within the TIGER framework to reduce computational costs in parameterized tests of General Relativity (GR) using gravitational wave signals. This method maintains accuracy while significantly speeding up likelihood evaluations, enabling efficient analysis of high-SNR signals from next-generation observatories.


<details>
  <summary>Details</summary>
Motivation: Current GR tests with GWs are computationally intensive due to the expanded parameter space from deviation parameters. This limits large-scale studies needed to assess degeneracies, waveform systematics, and noise effects. Future XG observatories will exacerbate these challenges with longer signals and higher SNRs, necessitating computational efficiency improvements.

Method: The authors apply relative binning to the TIGER framework, replacing dense frequency evaluations with adaptive frequency bins. This reduces the number of likelihood evaluations required while preserving posterior accuracy. Simulations of binary black hole signals were used to test the method, focusing on GR-consistent and non-GR scenarios. Binning resolution's impact was analyzed, particularly around the -1 post-Newtonian term.

Result: The method achieves unbiased parameter recovery in both GR-consistent and non-GR cases. High-SNR XG signals showed accurate results with tight posteriors. Application to real events (GW150914, GW250114) produced GR-consistent bounds and reduced computational time by a factor of 10 to 100 without accuracy loss.

Conclusion: Relative binning in TIGER enables computationally feasible GR tests for current and future GW observatories. This advancement allows systematic large-scale studies, critical for robust analyses under various noise conditions and waveform systematics, supporting next-generation gravitational wave science.

Abstract: Tests of general relativity (GR) with gravitational waves (GWs) introduce additional deviation parameters in the waveform model. The enlarged parameter space makes inference computationally costly, which has so far limited systematic, large-scale studies that are essential to quantify degeneracies, check effect of waveform systematics, and assess robustness across non-stationary and non-Gaussian noise effects. The need is even sharper for next-generation (XG) observatories where signals are longer, signal-to-noise ratios (SNRs) are higher, and likelihood evaluations increase substantially. We address this by applying relative binning to the TIGER framework for parameterized tests of GR. Relative binning replaces dense frequency evaluations with evaluations on adaptively chosen frequency bins, reducing the cost per likelihood call while preserving posterior accuracy. Using simulated binary black hole signals, we demonstrate unbiased recovery for GR-consistent cases and targeted non-GR deviations, and we map how bin resolution controls accuracy, with fine binning primarily required for the $-1$ post-Newtonian term. A high-SNR simulated signal at next-generation sensitivity further shows accurate recovery with tight posteriors. Applied to GW150914 and GW250114, both single and multi-parameter TIGER analyses finish within a day, yielding deviation bounds consistent with GR at 90\% credibility and in agreement with previous results. Across analyses, the method reduces wall time by factors of $\mathcal{O}(10)$ to $\mathcal{O}(100)$, depending on frequency range and binning, without degrading parameter estimation accuracy.

</details>


### [33] [Exceptional line and pseudospectrum in black hole spectroscopy](https://arxiv.org/abs/2511.17067)
*Li-Ming Cao,Ming-Fei Ji,Liang-Bi Wu,Yu-Sen Zhou*

Main category: gr-qc

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We investigate the exceptional points (EPs) and their pseudospectra in black hole perturbation theory. By considering a Gaussian bump modification to the Regge-Wheeler potential with variable amplitude, position, and width parameters, $(\varepsilon,d,σ_0)$, a continuous line of EPs (exceptional line, EL) in this three-dimensional parameter space is revealed. We find that the vorticity $ν=\pm1/2$ and the Berry phase $γ=π$ for loops encircling the EL, while $ν=0$ and $γ=0$ for those do not encircle the EL. Through matrix perturbation theory, we prove that the $ε$-pseudospectrum contour size scales as $ε^{1/q}$ at an EP, where $q$ is the order of the largest Jordan block of the Hamiltonian-like operator, contrasting with the linear $ε$ scaling at non-EPs. Numerical implements confirm this observation, demonstrating enhanced spectral instability at EPs for non-Hermitian systems including black holes.

</details>


### [34] [The Two-Measure Theory and an Overview of Some of its Manifestations](https://arxiv.org/abs/2511.17093)
*Alexander B. Kaganovich*

Main category: gr-qc

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The Two-Measure theory (TMT) has been developing since 1998 and has yielded a number of highly interesting results, including those not realized in traditional field theory models. The most important advantage of TMT as an alternative theory is that, under the conditions under which all classical tests of general relativity are performed, TMT models are able to accurately reproduce Einstein's general relativity. Despite this, TMT is still often perceived as something too exotic to be relevant to reality. In fact, the fundamental idea underlying TMT seems undeniable: if we truly believe in the effectiveness of mathematics in studying nature, we must agree that there must be a correspondence between the fundamental laws of nature and the structure of the mathematical apparatus necessary to adequately describe them. It then turns out that there is no reason to ignore the volume measure existing on the differentiable manifold on which the theory of gravity and matter fields is built. This idea has far-reaching implications. The goals of this paper are: 1) to provide a clear mathematical and conceptual justification for TMT; 2) to collect in a single article some of the main results of TMT obtained over the past 25 years.

</details>


### [35] [Cosmological perturbations on an averaged background](https://arxiv.org/abs/2511.17160)
*Marco Galoppo,Pierre Mourier*

Main category: gr-qc

TL;DR: This paper investigates the impact of nonlinear inhomogeneities' backreaction on the linear growth of large-scale structures in cosmology using covariant methods. It introduces an effective fluid model to represent backreaction and derives evolution equations for perturbations, analyzing closure conditions and their implications on structure formation predictions.


<details>
  <summary>Details</summary>
Motivation: To address how nonlinear inhomogeneities backreact on late-time cosmic expansion, particularly their influence on the linear growth of large-scale structures—a critical aspect for precision cosmology. Existing models may underestimate backreaction effects, leading to biased structure growth predictions despite matching background observables.

Method: Combines covariant spatial averaging with gauge-invariant perturbation theory applied to irrotational dust spacetimes. Formulates backreaction as an effective perfect fluid with pressure, establishes an effective background including both averaged dust and this fluid, then derives linear perturbation equations. Closes the system by choosing specific conditions for effective-fluid perturbations, analyzing two physically motivated options.

Result: Shows that coupling between structure growth and effective-fluid perturbations cannot always be neglected. Applying the framework to four models (three of which fit observations well) demonstrates significant backreaction effects on linear structure growth. Neglecting these leads to biased predictions even when background data matches.

Conclusion: Backreaction effects are crucial for accurate predictions of large-scale structure development in models that effectively describe the largest cosmic scales. Ignoring these may invalidate structure growth forecasts even if background dynamics align with observations, emphasizing needs for corrected observational analyses and model interpretations.

Abstract: In relativistic cosmology, the formation of nonlinear inhomogeneities can induce non-negligible backreaction on late-time expansion. Among the important consequences for precision cosmology is the potential impact on the linear growth of large-scale structures. We address this impact by combining covariant spatial averaging with covariant and gauge-invariant perturbation theory. We focus on irrotational dust model spacetimes. The effects of backreaction and nontrivial dynamical curvature on the average cosmological dynamics are formulated as the addition of an effective perfect fluid with pressure. We then introduce an effective background driven by both the averaged dust density and the emergent effective fluid, and derive the general evolution equations for linear perturbations of this system. The residual freedom in this framework amounts to specifying the properties of the effective-fluid perturbations as a closure condition. We analyse two physically motivated choices for this condition. In addition, we clarify the conditions under which the coupling between linear structure growth and perturbations of the effective fluid can be neglected. Finally, we apply this formalism to four examples of averaged cosmological models from the literature, three of which -- intended as effective full descriptions of the largest scales -- have been shown to provide a good fit to observational data. Our results highlight the importance of backreaction effects in shaping linear structure growth in such models. Neglecting these effects may thus lead to biased predictions for the development of large structures, even when the models provide a good description of the general background observables.

</details>


### [36] [Emergence of critical phenomena from the black hole interior](https://arxiv.org/abs/2511.17193)
*Caiying Shao,Junqi Guo,Yu Tian,Hongbao Zhang*

Main category: gr-qc

TL;DR: The study investigates the emergence of the r=0 singularity within charged black holes using numerical methods, finding a universal power law scaling similar to Choptuik's critical phenomena.


<details>
  <summary>Details</summary>
Motivation: To understand the dynamics of singularity formation inside black holes and explore universality in gravitational collapse.

Method: Numerical simulations of the Einstein-Maxwell-real scalar model with varying initial scalar field parameters to observe critical phenomena at the threshold of singularity emergence.

Result: Discovered a power law scaling (γ≈0.5) for the areal radius at the singularity's tip near critical parameter p_*, analogous to Choptuik's black hole formation scaling.

Conclusion: Confirmed universality in singularity emergence dynamics, providing new insights into strong-field gravitational phenomena.

Abstract: The emergence of the $r=0$ singularity inside a spherically symmetric charged black hole, is studied numerically within the Einstein-Maxwell-real scalar model. When the scalar field reaches a critical strength, the $r=0$ singularity emerges inside of the black hole at the tip of the causal diamond. By varying the parameter $p$ of the initial profile for the scalar field towards the critical value ${p_*}$, we observe the areal radius at the tip follows a power law scaling, ${r_S } \propto {| {p - {p_*}}|^γ}$, with a universal critical exponent $γ\approx 0.5$. This remarkable discovery, analogous to Choptuik's critical phenomena for the black hole formation, provides the first evidence of the universality and scaling for the emergence of the $r=0$ singularity inside black holes, offering new insights into the nonlinear dynamics of strong gravitational field.

</details>


### [37] [Implications of GW241011 for rotating exotic compact objects](https://arxiv.org/abs/2511.17341)
*N. V. Krishnendu,Tamara Evstafyeva,Aditya Vijaykumar,William E. East,Rimo Das,Sayantani Datta,Nils Siemonsen,Nami Uchikata,Poulami Dutta Roy,Anuradha Gupta,Ish Gupta,Syed U. Naqvi,Manuel Piarulli,Muhammed Saleem,Elise M. Sänger,Pratyusava Baral,Sajad A. Bhat,Thomas A. Callister,Mattia Emma,Carl-Johan Haster*

Main category: gr-qc

TL;DR: The study uses gravitational wave data from GW241011 to constrain the nature of horizonless compact objects, ruling out many exotic models but allowing those with sufficient compactness (C ≳ 0.24).


<details>
  <summary>Details</summary>
Motivation: To determine whether horizonless compact objects (e.g., boson stars) could explain observations resembling black holes, focusing on spin-induced quadrupole moment signatures.

Method: Analyze tight bounds on the spin-induced quadrupole moment from GW241011 to test exotic compact object models against black hole predictions.

Result: Many exotic models (like rotating boson stars) are excluded, but objects with compactness ≥0.24 remain viable.

Conclusion: GW241011's primary is likely a black hole or a highly compact object with C ≳ 0.24, narrowing possibilities for horizonless alternatives.

Abstract: A number of theoretical proposals have been made for horizonless compact objects with masses and spins similar to those of black holes. While gravitational wave signatures from their mergers can resemble those of black holes, features like the spin-induced quadrupole moment may reveal their distinct nature. Using the tight bounds on the spin-induced quadrupole moment of GW241011, we place gravitational wave constraints on the nature of its primary. We find that large classes of exotic compact objects (including rotating boson stars) cannot explain its nature, however, models of sufficiently large compactness of $C \gtrsim 0.24$ may still be viable contenders.

</details>


### [38] [Entanglement and correlations between local observables in de Sitter spacetime](https://arxiv.org/abs/2511.17382)
*Patricia Ribes-Metidieri,Ivan Agullo,Béatrice Bonga*

Main category: gr-qc

TL;DR: The paper challenges the notion that de Sitter curvature enhances entanglement by showing that while correlations increase, entanglement decreases between local field modes using a metric tensor and complex structure approach.


<details>
  <summary>Details</summary>
Motivation: To reevaluate prior conclusions that curvature enhances entanglement in de Sitter space, which were based on von Neumann entropy of non-local Fourier modes, by employing a fully local analysis of compactly supported field modes.

Method: The authors use a local approach examining field modes within the cosmological patch, focusing on the metric tensor and complex structure induced by the Bunch-Davies vacuum on the classical phase space.

Result: Increased curvature strengthens correlations between local modes but reduces their quantum entanglement, and a cosmological constant fundamentally alters the vacuum's entanglement structure.

Conclusion: De Sitter curvature does not enhance entanglement as previously thought; instead, the cosmological constant qualitatively changes entanglement distribution, compatible with earlier entropy studies when properly interpreted, with implications for inflation-era entanglement dynamics.

Abstract: Studies of quantum field entanglement in de Sitter space based on the von Neumann entropy of local patches have concluded that curvature enhances entanglement between regions and their complements. Similar conclusions about entanglement enhancement have been reached in analyses of Fourier modes in the cosmological patch of de Sitter space. We challenge this interpretation by adopting a fully local approach: examining entanglement between pairs of field modes compactly supported within de Sitter's cosmological patch. Our approach is formulated in terms of the properties of a metric tensor and an associated complex structure induced by the Bunch-Davies vacuum on the classical phase space. We find that increasing curvature increases correlations between local modes but, somewhat counterintuitively, decreases their entanglement. Our methods allow us to characterize how entanglement is spatially distributed, revealing that a cosmological constant, even if tiny, qualitatively alters the vacuum's entanglement structure. We show that our results are compatible with previous entropy-based studies when properly interpreted. Our findings have implications for entanglement between observables generated during cosmic inflation.

</details>


### [39] [A battle of designs: triangular vs. L-shaped detectors and parity violation in the gravitational-wave background](https://arxiv.org/abs/2511.17422)
*Hannah Duval,Charles Badger,Mairi Sakellariadou*

Main category: gr-qc

TL;DR: The study examines the ability of third-gen detector networks, including ET and CE, to detect parity-violating gravitational-wave backgrounds, finding that L-shaped ET designs outperform triangular ones in constraining parity violation.


<details>
  <summary>Details</summary>
Motivation: To assess how detector geometry and scale affect the capability to detect parity-violating GWBs, crucial for testing fundamental physics beyond general relativity.

Method: Simulated networks with varied ET designs (L-shaped vs. triangular), detector orientations, and arm lengths; analyzed their performance in constraining parity violation parameters.

Result: L-shaped ET-based networks showed superior sensitivity to parity violations compared to triangular ET configurations, especially when ET operates independently.

Conclusion: Detector geometry significantly impacts the ability to detect parity-violating GWBs, suggesting optimized designs like L-shaped ET could enhance tests of gravitational physics.

Abstract: We investigate the prospects for detecting a parity-violating gravitational-wave background (GWB) with third-generation ground-based detector networks. We focus on a network consisting of one Einstein Telescope (ET) and two Cosmic Explorer (CE) detectors. In our analysis we vary the ET design, detector orientations, and arm lengths, in order to assess the impact of geometry and scale on detection capabilities. We demonstrate that networks with an L-shaped ET design have stronger parity violation constraining power than networks with a triangular ET design, particularly seen when studying ET designs on their own.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [40] [No Pulsar Detected in Reprocessed Archival Parkes Observations of SNR 1987A](https://arxiv.org/abs/2511.16706)
*Fronefield Crawford,Haoyang Xu*

Main category: astro-ph.HE

TL;DR: The authors reprocessed archival radio observations of SNR 1987A using the Parkes telescope, performing periodicity and single pulse searches. No convincing pulsar signals were found, but derived luminosity limits remain compatible with the existence of a pulsar similar in luminosity to known young, energetic ones.


<details>
  <summary>Details</summary>
Motivation: The motivation was to investigate whether a pulsar exists within SNR 1987A by reanalyzing previously unexamined radio data, as prior observations had not definitively detected one.

Method: They reprocessed archival radio data from Parkes telescope observations, conducted standard periodicity searches and single pulse searches across various dispersion measures to detect potential pulsar signals.

Result: No significant pulsar candidates were identified. Calculated flux density, luminosity, and single pulse fluence limits were comparable to luminosities of three well-known young pulsars, meaning a similar luminosity pulsar might still exist in SNR 1987A.

Conclusion: While no pulsar was detected, the luminosity limits do not exclude the possibility of a pulsar in SNR 1987A with radio luminosity comparable to the Crab pulsar or other young, energetic pulsars.

Abstract: We have reprocessed the available archival radio pulsar search observations of SNR 1987A taken with the Parkes 64-m telescope, some of which have not been previously published. We conducted a standard periodicity search on these data as well as a single pulse search at a range of dispersion measures. We found no convincing candidate signals, and we calculate flux density, luminosity, and single pulse fluence limits from these observations. The derived luminosity limits are comparable to the luminosities of three young, energetic pulsars (the Crab pulsar, PSR B0540$-$69, and PSR J0537$-$6910), and so we cannot rule out the existence of a pulsar in SNR 1987A with a similar radio luminosity.

</details>


### [41] [Turbulence in Core-Collapse Supernovae](https://arxiv.org/abs/2511.16755)
*David Calvert,Michael Redle,Bibek Gautam,Charles J. Stapleford,Carla Fröhlich,James P. Kneller,Matthias Liebendorfer*

Main category: astro-ph.HE

TL;DR: Different definitions of turbulence in core-collapse supernova simulations lead to varying conclusions about its importance for explosion success and its interaction with enstrophy, highlighting the need for standardized measurement methods.


<details>
  <summary>Details</summary>
Motivation: Quantifying turbulence in supernova simulations is challenging due to ambiguous definitions, which affect the assessment of its role in explosion revival and the effective equation of state.

Method: 3D magnetohydrodynamic simulations using ELEPHANT code, comparing multiple turbulence definitions to analyze kinetic energy ratios, enstrophy correlations, and turbulent adiabatic indices.

Result: Turbulence definitions yield differing conclusions on its contribution to explosions, varying turbulent kinetic energy levels, and correlations with enstrophy. The turbulent adiabatic index is sensitive to measurement methods in low enstrophy regions.

Conclusion: Turbulence's impact on supernova explosions depends critically on its definition, necessitating standardized approaches to accurately quantify its role.

Abstract: It is understood in a general sense that turbulent fluid motion below the shock front in a core-collapse supernova stiffens the effective equation of state of the fluid and aids in the revival of the explosion. However, when one wishes to be precise and quantify the amount of turbulence in a supernova simulation, one immediately encounters the problem that turbulence is difficult to define and measure. Using the 3D magnetohydrodynamic code ELEPHANT, we study how different definitions of turbulence change one's conclusions about the amount of turbulence in a supernova and the extent to which it helps the explosion. We find that, while all the definitions of turbulence we use lead to a qualitatively similar growth pattern over time of the turbulent kinetic energy in the gain region, the total amount of turbulent kinetic energy, and especially the ratios of turbulent to total kinetic energy, distinguish them. Some of the definitions appear to indicate turbulence is a necessary contributor to the explosion, and others indicate it is not. The different definitions also produce turbulence maps with different correlations with maps of the enstrophy, a quantity widely regarded as also indicating the presence of turbulence. We also compute the turbulent adiabatic index and observe that in regions of low enstrophy, this quantity is sensitive to the definition used. As a consequence, the effective adiabatic index depends upon the method used to measure the turbulence and thus alter one's conclusions regarding the impact of turbulence within the supernova.

</details>


### [42] [An Investigation of Systematic Effects from Background Priors on PSR J0740$+$6620 Radius Estimates using Synthetic NICER and XMM-Newton Data](https://arxiv.org/abs/2511.16759)
*Isiah M. Holt,M. Coleman Miller,Alexander J. Dittmann,Frederick K. Lamb*

Main category: astro-ph.HE

TL;DR: The study demonstrates that even with significant underestimation of background components in X-ray data from NICER and XMM-Newton, neutron star radius estimates remain robust, showing only ~1σ deviation from true values when using accurate models.


<details>
  <summary>Details</summary>
Motivation: To assess the impact of background model inaccuracies on neutron star radius measurements, a crucial but understudied systematic error, ensuring reliable density measurements of neutron star cores.

Method: Synthetic data from NICER and XMM-Newton were analyzed using PSR J0740+6620 as a case study, comparing scenarios with correct versus underestimated background models.

Result: Underestimating the background by up to fivefold caused only ~1σ radius shifts, and correct models showed higher Bayesian evidence. This supports NICER data's reliability if models properly fit data.

Conclusion: Neutron star radius measurements via NICER-like data remain robust against major background model errors, reinforcing confidence in their use for dense matter studies.

Abstract: Accurate and precise measurements of neutron star radii provide invaluable information about the cold, dense matter in neutron star cores. Analyses of synthetic X-ray pulse waveform data similar to the data obtained from non-accreting neutron stars using the Neutron star Interior Composition Explorer (NICER) have indicated that mass and radius estimates made using such data are robust against some systematic errors that may be made when modeling these data, such as errors in the assumed pattern of the thermal X-ray emission from the surface of these stars. A potentially important but so far unexplored source of systematic error is misparameterization of unmodulated background components, which can bias the inferred radius, particularly when data from different telescopes are used in the analysis. In this study, we investigate the effects of the background model on radius estimates by jointly analyzing synthetic NICER and XMM-Newton data, using the $\sim 2.1~M_\odot$ pulsar PSR~J0740$+$6620 as a prototypical example. Our analysis shows that even if the background assumed in the model underestimates the actual background by a factor of more than five, the resulting shift of the radius posterior from the true value of the radius corresponds to only $\sim1σ$. In all the cases we examined, the Bayesian evidence for the correct background model is greater than for the incorrect background model. These results add to the evidence that analyses of NICER-like data provide accurate measurements of neutron star radii when the statistical sampling is thorough and the model fits the data well.

</details>


### [43] [The nature of UHE source 1LHAASO J1740+0948u and its connection to PSR J1740+1000](https://arxiv.org/abs/2511.16764)
*Seth Gagnon,Yichao Lin,Alexander Lange,Hui Yang,Noel Klingler,Jeremy Hare,Oleg Kargaltsev*

Main category: astro-ph.HE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present multi-wavelength analysis of 1LHAASO J1740+0948u and its surroundings including the pulsar wind nebula of middle-aged pulsar PSR J1740+1000. Although a dozen X-ray sources are found within the UHE emission site, careful analysis shows that they are unlikely to produce the observed UHE emission. The most likely particle accelerator is pulsar J1740+1000 which if offset by 13' north of the UHE source but appears to be connected to it by an extended feature seen in X-rays. For a plausible pulsar distance of 1.2 kpc, 1LHAASO J1740+0948u must be located about 5 pc away which requires rapid transport of electrons along the feature to avoid radiative losses. This poses several challenges for standard pulsar theory. Firstly, being produced $\lesssim$ 10 kyrs ago, particles must have been accelerated to the energy corresponding to a large fraction of the pulsar's full potential drop across the polar cap. Secondly, due to the lack of TeV emission extension toward the pulsar, particles must be accumulating in the UHE region. In this context, we discuss two possible scenarios: a tail filled with pulsar wind and confined by the bow-shock due to the fast pulsar's motion and an ISM filament filled by the most energetic pulsar wind particles escaping from the apex of the bow-shock.

</details>


### [44] [In-depth analysis of the clustering of dark matter particles around primordial black holes. Part II. Analytical prescriptions for spikes](https://arxiv.org/abs/2511.16800)
*Julien Lavalle,Pierre Salati*

Main category: astro-ph.HE

TL;DR: The paper explores how primordial black holes (PBHs) and self-annihilating dark matter (DM) interact, combining analytical methods to predict density spike profiles and their implications for cosmic microwave background (CMB) and gamma-ray observations. It reveals that sub-solar PBHs could tightly constrain DM annihilation cross-sections, a previously overlooked insight.


<details>
  <summary>Details</summary>
Motivation: To investigate the coexistence of PBHs and self-annihilating DM, address how DM forms spikes around PBHs during the radiation era, and refine constraints on DM properties using astrophysical observations.

Method: Developed analytical solutions for density profiles with power-law scaling (indices 3/4, 3/2, 9/4 across 4 regimes), derived annihilation rate formulae for self-annihilating DM, and analyzed detection prospects in CMB and extragalactic gamma-rays.

Result: Created an approximate analytical model accurate to ±15% for DM annihilation rates, demonstrated that sub-solar PBH populations impose stringent limits on s-wave DM annihilation cross-sections, and highlighted the interplay between PBHs and DM spike dynamics.

Conclusion: PBHs and self-annihilating DM are more mutually exclusive than previously thought; the presence of small PBHs strongly constrains DM annihilation parameters, essential for reconciling astrophysical observations with DM models.

Abstract: Primordial black holes (PBHs) are very appealing dark matter (DM) candidates. It is highly plausible though, should they exist, that they would not make up all of the DM. Several studies showed that if the rest of DM is made of thermal particles, then these should accumulate around such PBHs, leading to the formation of very dense spikes in the radiation era. We contributed a detailed analytical study about this phenomenon, providing clear explanations as for the origin of scaling relations in the form of power-law density profiles with up to 3 different spectral indices, i.e. $3/4$, $3/2$, and $9/4$, and 4 asymptotic regimes. Here, we further derive an approximate analytical solution that enables fast numerical predictions for the density profiles of these spikes. We also address the specific case of self-annihilating DM species and derive new approximate analytical formulae. Our approximate density yields the correct annihilation rate within $\pm 15\%$ precision. We then focus on indirect detection in the cosmic microwave background and in extragalactic gamma-rays. We shed new and subtle light on how mutually exclusive PBHs and self-annihilating DM species can really be. In particular, the discovery of a population of sub-solar PBHs would set stringent constraints on the $s$-wave annihilation cross-section of these particles, a point so far missed in the literature.

</details>


### [45] [Which active galaxies might be neutrino emitters?](https://arxiv.org/abs/2511.16869)
*Shuying Zhou,Mouyuan Sun,Guobin Mou,Da-bin Lin,Tong Liu,Ming-Xuan Lu,Yongquan Xue*

Main category: astro-ph.HE

TL;DR: The IceCube Observatory found that high-confidence neutrino emitters among AGNs have higher X-ray fluxes and more MIR variability, suggesting long-term accretion/jet changes influence neutrino production. This challenges previous models and offers new clues about cosmic ray acceleration and neutrino sources.


<details>
  <summary>Details</summary>
Motivation: To determine which AGN types are likely neutrino emitters and understand the mechanisms behind neutrino production in these objects.

Method: Analyzing data from the Swift BAT AGN survey, comparing high-confidence neutrino emitters with other AGNs in terms of X-ray flux and MIR variability. Observational correlations and theoretical considerations were used to explore possible production mechanisms.

Result: Found that neutrino emitters have higher X-ray flux and greater MIR variability than non-emitters, indicating that long-term accretion disk/jet fluctuations play a role. This supports the need for reevaluating current models involving AGN structure/activity.

Conclusion: MIR variability and X-ray flux are key indicators for neutrino emission. Neutrino production likely linked to instability in AGN central engines (corona, jets), requiring episodic processes like magnetic reconnection or small-scale outflows. These findings guide future IceCube counterpart studies and cosmic ray research.

Abstract: The IceCube Neutrino Observatory has identified several individual neutrino emitters associated with supermassive black hole accretion phenomena, including blazars, tidal disruption events, and, unexpectedly, Seyfert galaxies. A key open question is which types of active galactic nuclei (AGNs) are most likely to be neutrino emitters. Here we show that high-confidence extragalactic neutrino emitters tend not only to have higher hard X-ray fluxes but also to be more variable in mid-infrared (MIR) than other AGNs in the \textit{Swift} BAT AGN Spectroscopic Survey. MIR variations effectively trace long-term fluctuations in AGN accretion disks and/or jets. In addition to the role of X-ray flux emphasized in previous studies, we speculate that long-term central engine fluctuations may also be critical for neutrino production. This hypothesis may inform IceCube neutrino-electromagnetic counterpart association studies and provide new insights into cosmic ray acceleration sites. First, the observed neutrinos are unlikely to originate from AGN host galaxies or from interactions between large-scale (dozens of parsecs) winds/outflows and the surrounding interstellar medium. Second, if neutrinos are produced in the X-ray corona, the corona should exhibit strong magnetic turbulence dissipation or magnetic reconnection whose rate changes substantially on timescales of years. Third, the relativistic jets of blazar neutrino emitters may be intrinsically unstable over years. Finally, if neutrinos are related to interactions between small-scale winds/outflows and torus clouds, such winds/outflows must be highly episodic.

</details>


### [46] [Self-bound quark stars with a first-order two-to-three flavor phase transition](https://arxiv.org/abs/2511.16874)
*G. Teruya,G. Lugones,A. G. Grunfeld*

Main category: astro-ph.HE

TL;DR: The paper explores self-bound quark stars using a flavor-dependent quark-mass density-dependent model with excluded-volume correction, analyzing their properties and comparing them with hadronic EOSs.


<details>
  <summary>Details</summary>
Motivation: To understand how self-bound quark stars (including hybrid stars) can satisfy observational constraints on mass, radius, and tidal deformability while distinguishing them from hadronic equations of state (EOS) using multimessenger data.

Method: The study uses a QMDD model with excluded-volume parameter κ to construct stellar sequences via Tolman-Oppenheimer-Volkoff equations. They analyze parameter space, EOS stiffness effects, and dimensionless trends in compactness, moments of inertia, and tidal deformabilities.

Result: Intermediate excluded-volume parameters (κ) reconcile high maximum masses (~2 solar masses) with observational radius/tidal constraints. Hybrid self-bound stars better match observations than pure strange-star models. Three dimensionless trends show reduced κ-dependence, offering EOS-insensitive discriminants.

Conclusion: The findings provide priors for interpreting multimessenger observations and tools to distinguish self-bound quark stars from hadronic EOSs based on observed stellar properties like compactness and tidal deformability.

Abstract: We investigate self-bound quark stars in a flavor-dependent quark-mass density-dependent (QMDD) model with an excluded-volume correction. We chart the parameter space at zero pressure to identify self-bound regimes, including cases with a first-order $ud \to uds$ transition, and construct cold, $β$-equilibrated stellar sequences via the Tolman-Oppenheimer-Volkoff equations. The excluded-volume parameter $κ$ controls the stiffness of the equation of state and thus masses, radii, tidal deformabilities, and moments of inertia. Intermediate repulsion typically reconciles $M_{\max} \gtrsim 2\,M_\odot$ with current radius/tidal constraints, with hybrid self-bound objects more compatible than pure strange-quark stars. We identify three EOS-insensitive trends $-$ dimensionless moment of inertia vs. compactness, tidal deformability vs. compactness, and gravitational vs. baryonic compactness $-$ whose explicit $κ$-dependence largely disappears in dimensionless form (the first two being notably tighter than the third). These results provide model-guided priors and tools for discriminating between hadronic and self-bound EOS families with multimessenger data.

</details>


### [47] [Exploring Emission Line Variability and Jet-Broad Line Region Interaction in the Blazar TON 599](https://arxiv.org/abs/2511.16891)
*Jonhatan U. Guerrero-González,Vahram Chavushyan,Víctor M. Patiño-Álvarez*

Main category: astro-ph.HE

TL;DR: The study investigates the multiwavelength variability of the blazar TON 599, focusing on the Mg II emission line and its connection to jet-induced effects. Results show synchrotron emission dominance during active states, quasi-simultaneous optical-Mg II variability indicating jet-driven photoionization, and minimal time lags between gamma-ray and optical emissions supporting a synchrotron self-Compton origin.


<details>
  <summary>Details</summary>
Motivation: To explore physical processes in blazar jets and emission regions by analyzing how jet activity influences emission line behavior and multiwavelength variability in TON 599.

Method: Multiwavelength analysis of gamma-ray (0.1-300 GeV), X-ray (0.2-10 keV), optical (3000 Å continuum, polarization), millimeter, and Mg II emission line light curves. Cross-correlation methods assessed temporal relationships between emission line/continuum across wavelengths. Non-Thermal Dominance (NTD) parameter evaluated synchrotron dominance.

Result: 1. Synchrotron emission dominates continuum during active states via NTD. 2. Mg II line varies quasi-simultaneously with optical continuum, pointing to jet-driven photoionization. 3. Minimal gamma-ray/optical time lags support synchrotron self-Compton origin for variable gamma-ray component.

Conclusion: Emission line variability and multiwavelength observations are critical for understanding jet-BLR interactions in blazars. Findings advance models of AGN emission mechanisms and jet-environment dynamics.

Abstract: Blazars, a highly variable Active Galactic Nuclei (AGNs) subclass, provide a unique opportunity to explore the physical processes within their relativistic jets and emission regions. In this study, we investigate the multiwavelength variability of the blazar TON 599, a Flat Spectrum Radio Quasar (FSRQ), with a particular emphasis on its emission line behavior. We focus on the Mg II $λ$2798 Å emission line, a key tracer of the ionized gas in the broad-line region (BLR), and its role in jet-induced variability. In addition to optical emission lines, we analyze gamma-rays (0.1-300 GeV), X-rays (0.2-10 keV), optical continuum ($λ$3000 Å), optical polarization, and millimeter-wavelength light curves. Three cross-correlation methods are employed to investigate temporal relationships between the emission line and continuum across various wavelengths. Using the Non-Thermal Dominance (NTD) parameter, our analysis confirms that synchrotron emission dominates the continuum during active states, highlighting the jet's primary role in the observed variability. The Mg II emission line exhibits quasi-simultaneous variability with the optical continuum, suggesting photoionization driven by the jet's non-thermal radiation. Additionally, the minimal time lag between gamma-ray and optical/near-ultraviolet emissions supports a synchrotron self-Compton origin for the most variable component of the gamma-ray emission. These findings highlight the importance of emission line variability and multiwavelength observations in constraining the interaction between jets and the BLR in blazars. The results contribute to a deeper understanding of AGN emission mechanisms and the complex interplay between jets and their surrounding environments.

</details>


### [48] [Spectral synthesis techniques for supernovae and kilonovae](https://arxiv.org/abs/2511.17084)
*Anders Jerkstrand*

Main category: astro-ph.HE

TL;DR: The paper reviews computational methods for modeling the spectra of supernovae and kilonovae, tracing the evolution from stellar wind models to advanced simulations of these cosmic explosions and outlining future improvements.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of decoding spectra from cosmic explosions by systematically reviewing and comparing existing spectral synthesis models and their computational techniques.

Method: A historical analysis of methodological developments in spectral modeling, starting from stellar winds, then SNe, and extending to KNe. The study examines computational approaches used in current codes, their approximations, and numerical schemes.

Result: Highlights similarities and differences in numerical methods among existing codes, identifies current limitations, and proposes pathways for enhancing the accuracy and efficiency of future models.

Conclusion: Emphasizes the need for refined computational models to better understand element production and explosion dynamics, guiding future research toward interdisciplinary collaboration and advanced computing techniques.

Abstract: Supernovae (SNe) and kilonovae (KNe) are the most violent explosions in cosmos, signalling the destruction of a massive star (core-collapse SN), a white dwarf (thermonuclear SN) and a neutron star (KN), respectively. The ejected debris in these explosions is believed to be the main cosmic source of most elements in the periodic table. However, decoding the spectra of these transients is a challenging task requiring sophisticated spectral synthesis modelling. Here, the techniques for such modelling is reviewed, with particular focus on the computational aspects. We build from a historical review of how methodologies evolved from modelling of stellar winds, to supernovae, to kilonovae, studying various approximations in use for the central physical processes. Similarities and differences in the numeric schemes employed by current codes are discussed, and the path towards improved models is laid out.

</details>


### [49] [Magnetized particle motion and accretion process with shock cone morphology around a decoupled hairy black holes](https://arxiv.org/abs/2511.17137)
*G. Mustafa,Faisal Javed,S. K. Maurya,A. Ditta,Orhan Donmez,Tayyab Naseer,Abdelmalek Bouzenada,Farruh Atamurotov*

Main category: astro-ph.HE

TL;DR: The study examines how magnetized particles behave and accrete around a 'decoupled hairy' black hole using a specific geometric method, showing enhanced energy efficiency and clear observational signs for future astronomy.


<details>
  <summary>Details</summary>
Motivation: To explore the unique effects of a decoupled hairy black hole's geometry on accretion processes, aiming to improve our understanding of energy efficiency and observational signatures in extreme astrophysical environments.

Method: Using extended geometric deformation to model a decoupled hairy black hole with two parameters that maintain horizon structure and satisfy weak energy conditions. Derived equations for magnetized particle motion, effective potentials, stable orbits, radial velocities, and accretion rates.

Result: Found smaller innermost stable circular orbits and increased energy efficiency/emissivity compared to standard black holes. The decoupling parameter strongly influences oscillations and accretion dynamics, with analytical predictions matching numerical simulations.

Conclusion: Decoupled hairy black holes offer higher energy conversion efficiency, providing testable observational signatures in gravitational wave and X-ray observations, advancing studies on black hole physics and accretion mechanisms.

Abstract: Relativistic accretion onto compact objects such as black holes and neutron stars is one of the most efficient known mechanisms for converting gravitational potential energy into radiation. In the case of rapidly spinning black holes, up to $40\%$ of the rest-mass energy of accreting matter can be released, far exceeding the efficiency of nuclear fusion. In this work, we investigate magnetized particle motion and relativistic accretion processes around a decoupled hairy black hole via extended geometric deformation. The developed geometry involves two hairy parameters that preserve the horizon structure with the additional feature of the fulfillment of weak energy conditions outside the event horizon. We provide the foundation with necessary formalism for magnetized particle motion around a decoupled black hole. The effective potential and innermost stable circular orbits are then derived, which demonstrate a significant reduction of the radius of the latter quantity under the hairy parameters for the magnetized particle. Afterwards, we obtain exact analytical expressions for radial velocity profiles, mass accretion rates, and a few others which reveal improved energy efficiency and emissivity as compared to the standard black hole. Furthermore, the decoupling parameter shows strong influence on oscillations, accretion presenting fantastic agreement between analytical predictions and numerical simulations, and thus offering noticeable observational signatures for future gravitational wave and X-ray astronomy.

</details>


### [50] [An Alternative Explanation for the Helium Star Pulsar Binary J1928$+$1815: The Most Heavyweight Black Widow System to Date](https://arxiv.org/abs/2511.17248)
*Hang Gong,Alexey Bobrick,Francisco Garzón,Deven Bhakta,Thomas Maccarone,Sangita Kumari,Nieves Castro Rodríguez,Antonio Cabrera-Lavers,Arash Bahramian,Jifeng Liu*

Main category: astro-ph.HE

TL;DR: The paper presents deep near-infrared observations of the helium star pulsar binary J1928+1815, which did not detect the companion star, leading the authors to propose it could be a heavyweight black widow system with a massive ablated white dwarf instead.


<details>
  <summary>Details</summary>
Motivation: To investigate the nature of the companion star in J1928+1815 given the lack of observational detection, challenging previous assumptions of a helium star and exploring alternative models.

Method: Deep near-infrared imaging (J=23.7, H=22.2) to search for the companion star, comparison with other pulsar binaries, and theoretical modeling of irradiation effects on white dwarf companion behavior.

Result: No detection of the companion; proposed alternative scenario involving a white dwarf with radio eclipse mechanism due to binary-scale haze.

Conclusion: J1928+1815 may represent a new type of heavyweight black widow system, emphasizing the need for spectroscopic confirmation and suggesting Hubble Space Telescope imaging for further constraints.

Abstract: We present the results of deep near-infrared imaging of the recently discovered helium star pulsar binary J1928$+$1815 situated in the Galactic plane. Our observations did not achieve significant detections, providing limiting magnitudes of J=23.7 and H=22.2, which are both 2.4\,magnitudes deeper than the expected J and H magnitudes for a modeled stripped helium star with a mass of 1\,$\rm M_{\odot}$ after extinction. Although we cannot completely rule out the possibility of more significant extinction and the exact evolutionary status of the supposed helium star is uncertain, by comparing J1928$+$1815 with other pulsar binaries, we propose a natural alternative solution: that J1928$+$1815 is a heavyweight black widow system with a massive ablated white dwarf. Due to the pulsar's relatively high spin-down power and short orbital separation, the irradiation heating timescale is uniquely shorter than the cooling timescale for the WD companion. As a result, the WD effectively boils, with its outer layers expanding, overfilling the Roche lobe and producing low-density binary-scale haze opaque in the radio band. If this interpretation is correct, J1928$+$1815 would represent a new category distinct from canonical lightweight black widow systems. Radio eclipses can occur in pulsar binaries across a wider range of WD companion masses than previously thought. Therefore, they do not serve as a definitive indicator of a helium star without its direct detection. We contend that a spectroscopic identification remains the smoking gun for its existence. Given the crowding in this field, an \textit{HST} imaging in the near-infrared band would provide even better constraints.

</details>


### [51] [Covariance spectrum of MAXI J1820+070: On the nature of the Comptonizing flow](https://arxiv.org/abs/2511.17285)
*Shuai-Kang Yang,Bei You,Niek Bollemeijer,Phil Uttley,A. J. Tetarenko,Andrzej A. Zdziarski,Liang Chen,P. Casella,J. A. Paice,Yang Bai,Sai-En Xu*

Main category: astro-ph.HE

TL;DR: The paper examines the covariance spectrum of the black hole X-ray binary MAXI J1820+070 in its hard state, extending coherence studies to 150 keV. It finds a coherence drop above 30 keV and attributes this to multiple seed photon sources. A two-Comptonization model explains differing electron temperatures between short and long timescales, linking geometry and luminosity effects.


<details>
  <summary>Details</summary>
Motivation: To investigate the origin of coherent X-ray variability in black hole binaries and understand the physical mechanisms behind spectral and timing properties during the hard state.

Method: Extended coherence and covariance analysis up to 150 keV, simultaneous fitting of covariance data and spectra with a Comptonization model, and a two-zone Comptonization framework to explain temperature differences.

Result: Detected coherence drop above 30 keV; electron temperature higher for long-timescale variability, constant over time, while short-timescale temperature varies with luminosity. Multiple seed photon sources (blackbody and synchrotron) and a vertically structured Comptonization region explain observations.

Conclusion: The results support a two-Comptonization model where geometry and photon fields influence electron temperature and coherence. The inner region's elevation causes illumination by cooler photons, leading to lower short-timescale temperatures, with height changes explaining temperature evolution.

Abstract: We present an analysis of the covariance spectrum of the black hole X-ray binary MAXI J1820+070 during its hard state. For the first time, we extend coherence and covariance studies into the hard X-ray band up to 150 keV. We detect a clear drop in coherence above 30 keV on both short- and long-timescales relative to the 2-10 keV reference band. To investigate the origin of the coherent variability, we simultaneously fit the short- and long-timescale covariances and the time-averaged spectra with a Comptonization model. Surprisingly, the electron temperature associated with long-timescale variability is significantly higher than that on short timescales. Moreover, the temperature on long timescales remains relatively constant throughout the hard state, whereas the short-timescale temperature evolves with X-ray luminosity. We attribute the drop in coherence to multiple sources of seed photons, i.e., the blackbody and synchrotron photons. The independence between these two photon fields leads to the drop in coherence. To explain the lower electron temperature on short timescales, we propose a two-Comptonization framework in which short-timescale variability arises from a vertically extended central region, while long-timescale variability originates at larger radii. The elevated geometry of the inner region leads to illumination primarily by cooler outer-disk photons, yielding a lower electron temperature. In this case, the evolution of the height of the elevated region could explain the evolution of the electron temperature associated with the coherent variability throughout the hard state.

</details>


### [52] [A XRISM View of Relativistic Reflection in Cygnus X-1](https://arxiv.org/abs/2511.17338)
*Paul A. Draghis,Jon M. Miller,Erin Kara,Elisa Costantini,Oluwashina Adegoke,Javier A. Garcia*

Main category: astro-ph.HE

TL;DR: The first high-resolution XRISM/Resolve observation of Cygnus X-1 reveals a relativistic Fe K line consistent with emission near the innermost stable orbit of a rapidly rotating black hole with spin a≈0.98 and disk inclination θ≈63°, supporting previous spin measurements and indicating disk-orbit misalignment.


<details>
  <summary>Details</summary>
Motivation: To investigate the nature of relativistic broadening in the Fe K line of Cygnus X-1 and constrain black hole spin and accretion disk geometry using XRISM's superior spectral resolution.

Method: Analyzing high-resolution XRISM/Resolve data to isolate the relativistic Fe K line from continuum and narrow features, then modeling the line shape to determine black hole spin and disk inclination.

Result: The Fe K line is confirmed as a single broad feature from near the innermost stable orbit, yielding a spin of a≈0.98 and disk inclination of θ≈63°, consistent with prior reflection studies and indicating disk-orbit misalignment.

Conclusion: XRISM demonstrates exceptional capability in disentangling spectral features, confirming prior spin measurements, and providing new evidence for accretion disk geometry in X-ray binaries like Cygnus X-1.

Abstract: We present the first high-resolution XRISM/Resolve view of the relativistically broadened Fe K line in Cygnus X-1. The data clearly separate the relativistic broad line from the underlying continuum and from narrow emission and absorption features in the Fe band. The unprecedented spectral resolution in the Fe K band clearly demonstrates that the flux excess can be attributed to a single, broad feature, as opposed to a superposition of previously unresolved narrow features. This broad feature can be best interpreted as emission consistent with an origin near the innermost stable circular orbit around a rapidly rotating black hole. By modeling the shape of the broad line, we find a black hole spin of $a\simeq0.98$ and an inclination of the inner accretion disk of $θ\simeq63^\circ$. The spin is consistent with prior reflection studies, reaffirming the robustness of past spin measurements using the relativistic reflection method. The measured inclination provides reinforcing evidence of a disk-orbit misalignment in Cygnus X-1. These results highlight the unique abilities of XRISM in separating overlapping spectral features and providing constraints on the geometry of accretion in X-ray binaries.

</details>


### [53] [Deep Learning Analysis of Ions Accelerated at Shocks](https://arxiv.org/abs/2511.17363)
*Paxson Swierc,Damiano Caprioli,Luca Orusa,Miha Cernetic*

Main category: astro-ph.HE

TL;DR: The paper explores using deep learning to classify ions accelerated at collisionless shocks in hybrid simulations, achieving over 90% accuracy in predicting particle injection based on magnetic field time series data, with autoencoders reconstructing parameters, to extract physical insights and enable sub-grid models in fluid simulations.


<details>
  <summary>Details</summary>
Motivation: To leverage machine learning for understanding ion acceleration mechanisms in kinetic plasma simulations and pave the way for sub-grid models in fluid approaches.

Method: Ions were classified into thermal, suprathermal, or nonthermal categories based on energy and acceleration regimes. Deep learning models were trained on magnetic field time series data for prediction, while autoencoders reconstructed parameters from encoded representations.

Result: Models achieved >90% accuracy in predicting injected particles. Autoencoder successfully reconstructed time series parameters from encoded data.

Conclusion: Demonstrates potential of machine learning in extracting insights from kinetic plasma simulations and provides foundation for future applications like sub-grid models.

Abstract: We study the application of deep learning techniques to the analysis and classification of ions accelerated at collisionless shocks in hybrid (kinetic ions--fluid electrons) simulations. Ions were classified as thermal, suprathermal, or nonthermal, depending on the energy they achieved and the acceleration regime they fell under. These classifications were used to train deep learning models to predict which particles are injected into the acceleration process with high accuracy (>90%), using only time series of the local magnetic field they experienced during their initial interaction with the shock. An autoencoder architecture was also tested, for which time series of various parameters were reconstructed from encoded representations. This study shows the potential of applying machine learning techniques to extract physical insights from kinetic plasma simulations and sets the groundwork for future applications, including the construction of sub-grid models in fluid approaches.

</details>


### [54] [Discovering periodic and repeating nuclear transients in the XMM-Newton archives](https://arxiv.org/abs/2511.17385)
*Natalie A. Webb,Vincent Foustoul,Robbie Webbe,Matteo Bachetti,Erwan Quintin,Laurent Michel*

Main category: astro-ph.HE

TL;DR: The paper discusses the use of XMM-Newton X-ray catalogues to identify massive black hole phenomena through innovative analysis methods, highlighting discoveries like tidal disruption events, quasi-periodic eruptions, and potential black hole binaries, while introducing tools like STONKS and real-time alert systems for transient follow-up.


<details>
  <summary>Details</summary>
Motivation: To leverage large X-ray datasets for detecting transient and variable phenomena associated with massive black holes, which can reveal insights into tidal disruptions, binary systems, and AGN activity. Traditional methods are insufficient for processing massive archives, necessitating new approaches.

Method: The study utilizes XMM-Newton's EPIC archival data, employs the STONKS pipeline for source detection, conducts periodic variability searches, and implements a near real-time alert system. Examples include analyzing TDEs, quasi-periodic eruptions, and candidate black hole binaries.

Result: 发现了多个与潮汐瓦解事件（TDEs）、准周期喷发现象相关的源，并提出了新的大质量黑洞双星候选体。Furthermore, the STONKS pipeline and real-time alert system have enabled timely follow-up of transients, identifying new fading TDE candidates.

Conclusion: 系统的大规模X射线数据挖掘结合创新算法和实时监控，显著提升了探测黑洞相关变源的能力，为理解极端天体物理过程提供了关键工具，并呼吁持续利用现有档案数据以发现更多天文事件。

Abstract: The regions around massive black holes can show X-ray variability on timescales from seconds to decades. Observing many black holes over different timescales can enhance our chances of detecting variability coming from (partial) tidal disruption events, massive black hole binaries, changing state AGN, blazar activity and much more. X-ray catalogues with hundreds of thousands of detections are treasure troves of such sources, which require innovative methods to identify these black holes. We present the current XMM-Newton catalogues available and describe several examples of tidal disruption events (TDEs) and quasi-periodic eruption sources that have been found whilst mining this data. We describe preliminary work on a search for periodic variables in the XMM-Newton EPIC archival data, with the example of finding new massive black hole binaries. We also describe the STONKS pipeline that is now in the XMM-Newton automatic reduction pipeline and the near real-time alert system that allows the follow-up of new and fading transients. We provide examples of fading sources that are newly identified candidate TDEs.

</details>
