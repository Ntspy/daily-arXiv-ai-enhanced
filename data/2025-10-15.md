<div id=toc></div>

# Table of Contents

- [astro-ph.IM](#astro-ph.IM) [Total: 5]
- [gr-qc](#gr-qc) [Total: 19]
- [hep-ph](#hep-ph) [Total: 22]
- [astro-ph.HE](#astro-ph.HE) [Total: 18]


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [1] [PLATOSpec: a precise spectrograph in support of space missions](https://arxiv.org/abs/2510.11961)
*P. Kabáth,M. Skarka,A. Hatzes,E. Guenther,L. Vanzi,R. Brahm,J. Janík,P. Pintr,P. Gajdoš,J. Lipták,J. Žák,H. M. J. Boffin,L. Antonucci,G. Avila,Z. Balkóová,M. E. Ball,M. Flores,A. Fuentes,J. Fuchs,R. Greimel,A. Gajardo,V. D. Ivanov,J. Köhler,M. Leitzinger,T. Moravčík,J. Nečásek,R. J. U. Neubert,P. Odert,G. Olguin,M. Tala Pinto,M. Roth,L. Řezba,V. Schaffenroth,M. Sigwarth,J. Srba,A. Suárez,P. Škoda,J. Šubjak,J. Václavík,M. Veselý,R. Veselý,M. Vítková,J. U. Winkler,M. Zummer,E. Ždárská*

Main category: astro-ph.IM

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The upcoming space missions that will characterize exoplanets, such as PLATO
and Ariel, will collect huge amounts of data that will need to be complemented
with ground-based observations. The aim of the PLATOSpec project is to perform
science with an echelle spectrograph capable of measuring precise radial
velocities. The main focus of the spectrograph will be to perform the initial
screening and validation of exoplanetary candidates, in addition to study
stellar variability. It will be possible to determine the physical properties
of large exoplanets. The PLATOSpec blue-sensitive spectrograph, with a spectral
range of 380 to 700\,nm and a resolving power of R=70,000, is installed on the
1.5-m telescope at the ESO La Silla Observatory in Chile. Initial results show
that the radial-velocity limit given by the wavelength calibration is about 2-3
m/s. Tests on bright F-K main-sequence standard stars reveal a scatter of about
5 m/s over a few hours. The scatter over a few months is slightly higher. We
demonstrate the capabilities of PLATOSpec on the mass determination of WASP-79
b and the spin-orbit alignment of WASP-62\,b via the Rossiter-McLaughlin
effect. We show its possible usage on variable star research as demonstrated on
the false-positive exoplanetary candidate TIC 238060327, which is proven a
binary star. Investigation of line-profile variations of the roAp star alpha
Cir shows that PLATOSpec can also be used for the surface mapping. Finally, we
present new results on the active star UY Pic in the PLATO southern field. Our
results show that PLATOSpec is a versatile spectrograph with great precision.

</details>


### [2] [Very-Long Baseline Interferometry Imaging with Closure Invariants using Conditional Image Diffusion](https://arxiv.org/abs/2510.12093)
*Samuel Lai,Nithyanandan Thyagarajan,O. Ivy Wong,Foivos Diakogiannis*

Main category: astro-ph.IM

TL;DR: This paper introduces a generative deep learning method for calibrating-free image reconstruction in very-long baseline interferometry (VLBI) using closure invariants. The model achieves high fidelity scores and competitive performance compared to existing methods, offering a simpler and more reproducible solution.


<details>
  <summary>Details</summary>
Motivation: VLBI suffers from sparse aperture coverage and calibration challenges leading to biases and artifacts. Closure invariants provide calibration-independent data but their inverse transformation is ill-posed, necessitating a robust reconstruction method.

Method: A generative deep learning model trained on simple shapes and CIFAR-10 to reconstruct images directly from closure invariants. The approach avoids hand-tuned hyperparameters and focuses on the inverse problem solution.

Result: The model achieved reduced chi-square scores of χ²_CI ≲1 and maximum normalized cross-correlation (NX) of ρ_NX >0.9 for both trained and untrained morphologies. It competes with state-of-the-art algorithms without requiring fine hyperparameter tuning.

Conclusion: The proposed method provides a reliable, calibration-independent imaging solution for sparse VLBI data, enhancing result reliability and simplifying the workflow compared to traditional techniques.

Abstract: Image reconstruction in very-long baseline interferometry operates under
severely sparse aperture coverage with calibration challenges from both the
participating instruments and propagation medium, which introduce the risk of
biases and artefacts. Interferometric closure invariants offers
calibration-independent information on the true source morphology, but the
inverse transformation from closure invariants to the source intensity
distribution is an ill-posed problem. In this work, we present a generative
deep learning approach to tackle the inverse problem of directly reconstructing
images from their observed closure invariants. Trained in a supervised manner
with simple shapes and the CIFAR-10 dataset, the resulting trained model
achieves reduced chi-square data adherence scores of $\chi^2_{\rm CI} \lesssim
1$ and maximum normalised cross-correlation image fidelity scores of $\rho_{\rm
NX} > 0.9$ on tests of both trained and untrained morphologies, where
$\rho_{\rm NX}=1$ denotes a perfect reconstruction. We also adapt our model for
the Next Generation Event Horizon Telescope total intensity analysis challenge.
Our results on quantitative metrics are competitive to other state-of-the-art
image reconstruction algorithms. As an algorithm that does not require finely
hand-tuned hyperparameters, this method offers a relatively simple and
reproducible calibration-independent imaging solution for very-long baseline
interferometry, which ultimately enhances the reliability of sparse VLBI
imaging results.

</details>


### [3] [CCAT: Optical Responsivity, Noise, and Readout Optimization of KIDs for Prime-Cam](https://arxiv.org/abs/2510.12162)
*Eliza Gazda,Quintin Meyers,James R. Burgoyne,Scott Chapman,Steve K. Choi,Cody J. Duell,Anthony I. Huber,Inchara Jagadeesh,David Faulkner Katz,Ben Keller,Lawrence T. Lin,Paul Malachuk,Michael D. Niemack,Darshan A. Patel,Gordon J. Stacey,Benjamin J. Vaughan,Eve M. Vavagiakis,Samantha Walker,Yuhan Wang,Ruixuan,Xie*

Main category: astro-ph.IM

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The Prime-Cam instrument of the Fred Young Submillimeter Telescope (FYST) at
the CCAT Observatory will conduct sensitive millimeter to submillimeter surveys
for a range of astrophysical and cosmological sciences. Prime-Cam will use
kinetic inductance detectors (KIDs) sensitive to multiple frequency bands
spanning 280--850 GHz. With over 100,000 sensors under development, these KID
arrays will soon form the largest submillimeter focal plane ever built. With
fixed microwave tones probing amplitude and phase modulations in the KIDs due
to incoming radiation, challenges arise in determining the optimal readout
settings, especially under varying atmospheric loading. Realizing the science
goals of FYST requires operating the detectors at optimal performance and
determining accurate responsivities, which depend on readout tone placement and
power. To address these challenges, we present laboratory measurements of
sample pixels from the 280 GHz TiN and Al arrays using a blackbody cold load to
simulate observing conditions. These measurements probe detector responsivity
and noise across varying optical loading, tone power, and tone placement,
providing the foundation to guide in situ calibration and operation of the
$>$100,000 KIDs. We characterize detector sensitivity via the Noise Equivalent
Power (NEP) as a function of readout tone power and placement, and measure the
impact of detuning due to varying optical power on the NEP. Our test setup and
methodology will inform the commissioning of Prime-Cam, in situ detector
calibration procedures, the cadence of probe tone resetting, and potential
design refinements for future arrays, supporting FYST's planned first light in
2026.

</details>


### [4] [Encapsulating Textual Contents into a MOC data Structure for Advanced Applications](https://arxiv.org/abs/2510.12213)
*Giuseppe Greco,Thomas Boch,Pierre Fernique,Manon Marchand,Mark Allen,Francois Xavier Pineau,Matthieu Baumann,Marco Molinaro,Roberto De Pietri,Marica Branchesi,Steven Schramm,Gergely Dalya,Elahe Khalouei,Barbara Patricelli,Giulia Stratta*

Main category: astro-ph.IM

TL;DR: The paper introduces Textual and Semantic MOCs to enhance the IVOA's MOC standard, enabling richer annotations, semantic searches, and GenAI integration for advanced astronomical data analysis.


<details>
  <summary>Details</summary>
Motivation: To address the need for enhanced data interoperability and advanced query capabilities in astronomical data by extending MOCs to include textual descriptions and semantic embeddings.

Method: Textual MOCs store annotations directly in sky regions; Semantic MOCs convert text to embeddings stored in vector databases, allowing similarity searches and integration with GenAI.

Result: Textual MOCs improve user engagement through contextual descriptions, while Semantic MOCs enable advanced queries and GenAI interactions, enhancing astronomical data analysis.

Conclusion: The enhancements provide a foundation for integrating AI tools into astronomical research, facilitating more sophisticated spatial, semantic, and visual data operations.

Abstract: Context. The Multi-Order Coverage map (MOC) is a widely adopted standard
promoted by the International Virtual Observatory Alliance (IVOA) to support
data sharing and interoperability within the Virtual Observatory (VO)
ecosystem. This hierarchical data structure efficiently encodes and visualizes
irregularly shaped regions of the sky, enabling applications such as
cross-matching large astronomical catalogs. Aims. This study aims to explore
potential enhancements to the MOC data structure by encapsulating textual
descriptions and semantic embeddings into sky regions. Specifically, we
introduce "Textual MOCs", in which textual content is encapsulated, and
"Semantic MOCs" that transform textual content into semantic embeddings. These
enhancements are designed to enable advanced operations such as similarity
searches and complex queries and to integrate with generative artificial
intelligence (GenAI) tools. Method. We experimented with Textual MOCs by
annotating detailed descriptions directly into the MOC sky regions, enriching
the maps with contextual information suitable for interactive learning tools.
For Semantic MOCs, we converted the textual content into semantic embeddings,
numerical representations capturing textual meanings in multidimensional
spaces, and stored them in high-dimensional vector databases optimized for
efficient retrieval. Results. The implementation of Textual MOCs enhances user
engagement by providing meaningful descriptions within sky regions. Semantic
MOCs enable sophisticated query capabilities, such as similarity-based searches
and context-aware data retrieval. Integration with multimodal generative AI
systems allows for more accurate and contextually relevant interactions
supporting both spatial, semantic and visual operations for advancing
astronomical data analysis capabilities.

</details>


### [5] [Experimental tests of the calibration of high precision differential astrometry for HWO](https://arxiv.org/abs/2510.12270)
*Manon Lizzana,Fabien Malbet,Alain Leger,Fabrice Pancher,Sébastien Soler,Hugo Rousset,Thierry Lepine,Julien Michelot,Yahya Er-Rahmaouy,Youssef Bakka*

Main category: astro-ph.IM

TL;DR: This paper addresses the need for high-precision astrometry in studying exoplanets and dark matter by characterizing the 46Mpx Gigapyx detector, implementing an interferometric testbed, and developing a calibration method for optical distortion, tested via simulation and an optical bench.


<details>
  <summary>Details</summary>
Motivation: The requirement for sub-micro-arcsecond precision in astrometry drives the need for advanced instrumentation like the Habitable Worlds Observatory's detector systems to enable observations of faint sources and minute displacements critical for exoplanet and dark matter research.

Method: The study involves detector characterization of the Gigapyx 46Mpx sensor, creation of an interferometric testbed for pixel position analysis, and a simulation-based optical distortion calibration method validated on an IPAG optical bench.

Result: Successful implementation and testing of the calibration method and testbed, demonstrating feasibility for achieving the precision required for astrometric missions.

Conclusion: The proposed detector and calibration techniques provide viable solutions for next-generation astrometric instruments, advancing capabilities for detecting exoplanets and probing dark matter.

Abstract: Many different scientific applications require sub-micro arcsecond precision
astrometry, including researching rocky exoplanets in the vicinity of the Sun
and studying dark matter. The Habitable Worlds Observatory (HWO) is a promising
candidate to carry an astrometric instrument because it provides a stable,
space-based telescope with a large aperture, which allows faint sources and
small displacements to be observed. This paper presents the characterization of
an appropriate detector for an astrometric instrument: the 46Mpx Gigapyx from
Pyxalis. Moreover it explains the implementation of a testbed enabling
interferometric characterization of pixel positions. Finally, the paper
introduces a method for calibrating the telescope's optical distortion. This
method was implemented in simulation and tested thanks to an optical bench
developed at IPAG in France.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [6] [Floquet resonances and redshift-enhanced acceleration radiation from vibrating atoms in Schwarzschild spacetime](https://arxiv.org/abs/2510.11761)
*Reggie C. Pantig,Ali Övgün,Syed Masood,Li-Gang Wang*

Main category: gr-qc

TL;DR: The study examines acceleration radiation from a two-level Unruh-DeWitt detector oscillating near a Schwarzschild black hole. Using Boulware vacuum and (1+1)-dimensional reduction, they compute an averaged transition rate showing thermal-like spectra with Floquet resonance conditions. Near the horizon, curvature effects amplify radiation, but analysis breaks down close to the horizon, highlighting Boulware vacuum pathologies.


<details>
  <summary>Details</summary>
Motivation: To isolate curvature effects on acceleration radiation by avoiding Hawking thermal background, and to analytically link flat-space detector models to black hole spacetimes. Addressing how spacetime curvature modulates particle emission under acceleration.

Method: First-order time-dependent perturbation theory applied to a radially oscillating detector in Boulware vacuum. Reduced-dimensional (1+1) radial system and periodic averaging. Calculations use Bessel functions to derive Floquet transition rates, considering proximity to the horizon and resonance conditions.

Result: Thermal Bose-Einstein-type emission spectra with Floquet resonance (nΩ > ω₀). Radiation amplification via redshift near horizon (scaling by 1/√f(R₀)). However, spectrum undefined near horizon (R₀ → 2M), illustrating Boulware vacuum pathologies. Results match Minkowski limit at large R₀.

Conclusion: Demonstrates curvature's role in modulating acceleration radiation. Provides a bridge between flat and curved spacetime detector models. Emphasizes analytical tractability while noting limitations (small amplitude,远离视界). Suggests pathway for extensions to full (3+1)D and other vacuum states.

Abstract: We study acceleration radiation from a two-level Unruh-DeWitt detector that
undergoes small-amplitude radial oscillations at fixed mean radius $R_0$
outside a Schwarzschild black hole. The massless scalar field is quantized in
the Boulware vacuum to isolate curvature-modulated acceleration effects without
a thermal Hawking background. Working in a (1+1) radial reduction and using
first-order time-dependent perturbation, we evaluate the period-averaged
transition rate (or the "Floquet" transition rate). The resulting particle
emission spectrum exhibits a thermal Bose-Einstein-type profile with periodic
trajectory yielding a Floquet resonance condition $n\Omega > \omega_0$ and a
closed-form expression for the Floquet transition rate $\overline{P}_n$ which
reduces to the flat Minkowski spacetime result as $R_0\to\infty$. Near the
horizon, $f(R_0)<1$ enhances the effective Bessel argument by
$1/\sqrt{f(R_0)}$, providing a simple analytic demonstration of
curvature/redshift amplification of acceleration radiation. In particular, the
spectrum weighted by Bessel function becomes ill-defined near the black hole
horizon as $R_{0}\rightarrow 2M$, possibly manifesting the well-known
pathological behavior of Boulware vacuum state. We discuss the regime of
validity (small amplitude, $R_0$ away from the horizon) and outline the
extensions to (3+1) dimensions, including density-of-states and greybody
factors, and to alternative vacuum choices. Our results offer an analytically
tractable link between flat-space "vibrating atom" proposals and black-hole
spacetimes.

</details>


### [7] [Cosmology in the Hoyle Narlikar gravity](https://arxiv.org/abs/2510.11762)
*J. K. Singh,Sonal Aggarwal,Shaily,Hamid Shabani*

Main category: gr-qc

TL;DR: The paper investigates late-time cosmic acceleration using the Quintessence model in Hoyle-Narlikar Gravity, incorporating a creation field with non-minimal matter interaction. Results show this model aligns well with observational data, resolves Hubble tension more effectively than ΛCDM, and exhibits stability through phase analysis.


<details>
  <summary>Details</summary>
Motivation: To address the Hubble tension and explore alternatives to ΛCDM, evaluating if Hoyle-Narlikar Gravity with a creation field can better explain cosmic acceleration and observational constraints.

Method: Derived gravitational field equations under spatially flat, homogeneous spacetime; analyzed observational data (Hubble datasets, Pantheon+, Pantheon++BAO) to constrain density parameters; performed w dw phase analysis for dynamical behavior.

Result: The model consistently explains cosmic acceleration, offers tighter Hubble tension constraints, and shows thawing/freezing behavior converging to ΛCDM, confirming stability and observational validity.

Conclusion: Hoyle-Narlikar Gravity with non-minimal matter interaction provides a viable, stable alternative to ΛCDM, better resolving Hubble tension and aligning with observational datasets through its creation field dynamics.

Abstract: In this paper, we study the late time cosmic acceleration of the Quintessence
model within the framework of Hoyle Narlikar Gravity, consisting of a creation
field. Using the Hubble tension as a function of the density parameter for
matter, the density parameter for radiation, and the density parameter for dark
energy in the covariant formulation, we find the gravitational field equations
in the spatially flat, homogeneous, and isotropic spacetime to examine the
dynamical mechanism that leads to cosmic acceleration in the late-time
universe. We analyze the observational constraints on the late-time density
parameters using various recent observational datasets, including the Hubble
datasets, Pantheon+, and joint compilation, Pantheon++BAO. Consequently, it is
explicitly demonstrated that late-time cosmic acceleration can be consistent
with recent observational data in Hoyle Narlikar Gravity with non-minimal
matter interaction. In contrast with other modified theories of gravity, it is
observed that the creation field theory with non-minimal matter interaction
renders more compact constraints on the Hubble tension together with density
parameters, and extensively explains the accelerating expansion of the
universe, which makes it a more plausible option compared to the {\Lambda}CDM
model. Furthermore, the w dw phase analysis confirms alternating thawing and
freezing behaviour of the model, with all trajectories ultimately converging
toward the {\Lambda}CDM point, thereby confirming the model stability and the
observational consistency.

</details>


### [8] [Black Hole Ringdown Amplitudescopy](https://arxiv.org/abs/2510.11782)
*Francesco Crescimbeni,Xisco Jimenez-Forteza,Paolo Pani*

Main category: gr-qc

TL;DR: This paper analyzes how extra field-induced modes in alternative gravity theories affect gravitational-wave signal interpretation, showing they improve bounds on theories like dynamical Chern-Simons and Einstein-scalar-Gauss-Bonnet compared to standard methods.


<details>
  <summary>Details</summary>
Motivation: To evaluate the impact of extra field-induced modes on testing General Relativity in black hole ringdown signals, which are often overlooked in traditional analyses.

Method: Focused on dynamical Chern-Simons and Einstein-scalar-Gauss-Bonnet theories, the authors integrated extra field-induced modes into ringdown modeling to compare constraint strengths against standard spectroscopy.

Result: Including extra modes enhanced constraints on deviations from GR beyond standard quasinormal mode analysis, offering complementary testing avenues.

Conclusion: Accounting for extra field-induced modes is critical for accurate gravitational-wave tests of GR; neglecting them could bias results, necessitating updated ringdown templates.

Abstract: Black hole ringdowns in extensions of General Relativity (GR) generically
exhibit two distinct signatures: (1) theory-dependent shifts in the standard
black-hole quasinormal modes, and (2) additional modes arising from extra
fundamental fields --such as scalar, vector, or tensor degrees of freedom--
that can also contribute to the gravitational-wave signal. As recently argued,
in general both effects are present simultaneously, and accurately modeling
them is essential for robust tests of GR in the ringdown regime. In this work,
we investigate the impact of extra field-induced modes, which are often
neglected in standard ringdown analyses, on the interpretation of
gravitational-wave signals. To provide some concrete examples, we focus on
dynamical Chern-Simons and Einstein-scalar-Gauss-Bonnet theories,
well-motivated extensions of GR, characterized respectively by a parity-odd and
a parity-even coupling between a dynamical scalar field and quadratic curvature
invariants. We show that including extra field-induced modes improves the
bounds on these theories compared to standard spectroscopy and also allows for
equally constraining complementary tests not based on quasinormal mode shifts.
Our analysis highlights the relevance of incorporating extra field-induced
modes in ringdown templates and assesses their potential to either bias or
enhance constraints on GR deviations.

</details>


### [9] [Quasinormal modes from numerical relativity with Bayesian inference](https://arxiv.org/abs/2510.11783)
*Richard Dyer,Christopher J. Moore*

Main category: gr-qc

TL;DR: This paper introduces a flexible Gaussian-process model to quantify numerical uncertainties in numerical relativity waveforms, enabling Bayesian analysis techniques for NR waveforms and efficient sampling of quasinormal mode model posteriors without MCMC.


<details>
  <summary>Details</summary>
Motivation: To address the inevitable numerical uncertainties in NR waveforms, particularly important for studying subdominant/nonlinear effects during merger and ringdown, and to allow established Bayesian methods used in experimental data analysis to be applied to NR waveforms.

Method: Develops a Gaussian-process model for waveform uncertainties across a state-of-the-art NR catalog and an efficient procedure for sampling quasinormal mode model posteriors without MCMC. Uses the Gaussian-process-defined likelihood function for Bayesian analysis.

Result: Demonstrates the method's efficacy by applying it to quasinormal mode analysis of Cauchy-characteristic evolved waveforms, showing improved handling of numerical uncertainties.

Conclusion: The approach successfully integrates Bayesian analysis tools with NR waveform analysis, providing a reliable framework to quantify uncertainties and support studies of gravitational wave physics beyond standard methods.

Abstract: Numerical relativity (NR) enables the study of physics in strong and
dynamical gravitational fields and provides predictions for the
gravitational-wave signals produced by merging black holes. Despite the
impressive accuracy of modern codes, the resulting waveforms inevitably contain
numerical uncertainties. Quantifying these uncertainties is important,
especially for studies probing subdominant or nonlinear effects around the
merger and ringdown. This paper describes a flexible Gaussian-process model for
the numerical uncertainties in all the spherical-harmonic waveform modes across
a state-of-the-art catalog of NR waveforms and a highly efficient procedure for
sampling the posteriors of quasinormal mode models without the need for
expensive Markov chain Monte Carlo. The Gaussian-process model is used to
define a likelihood function which allows many Bayesian data analysis
techniques - already widely used in the analysis of experimental gravitational
wave data - to be applied to NR waveforms as well. The efficacy of this
approach is demonstrated by applying it to the analysis of quasinormal modes in
Cauchy-characteristic evolved waveforms.

</details>


### [10] [False Alarm Rates in Detecting Gravitational Wave Lensing from Astrophysical Coincidences: Insights with Model-Independent Technique GLANCE](https://arxiv.org/abs/2510.11790)
*Aniruddha Chakraborty,Suvodip Mukherjee*

Main category: gr-qc

TL;DR: This paper analyzes the potential false detection of lensed gravitational wave events due to astrophysical uncertainties using the GLANCE pipeline, finding a 0.01% false classification rate for unlensed binary-black hole pairs under specific conditions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of distinguishing lensed gravitational waves from unlensed pairs, which could lead to false positives due to astrophysical uncertainties such as incorrect source classifications.

Method: Simulated population of BBH mergers analyzed via GLANCE pipeline; tested false alarm rates under varying parameters (masses, time delays, magnification).

Result: ≈0.01% of event pairs falsely flagged as lensed at SNR threshold 1.5; false alarms peak at ~1000-day delays. FAR distributions mapped across GW source parameters.

Conclusion: Model-independent techniques like GLANCE help quantify detection reliability for current LIGO and future detectors, aiding in accurate lensed GW identification.

Abstract: The strong lensing gravitational waves (GWs) due to intervening massive
astrophysical systems between the source and an observer are an inevitable
consequence of the general theory of relativity, which can produce multiple GW
events in overlapping sky localization error. However, the confirmed detection
of such a unique astrophysical phenomenon is challenging due to several sources
of contamination, arising from detector noise to astrophysical uncertainties.
Robust model-independent search techniques that can mitigate noise
contamination were developed in the past. In this study, we explore the
astrophysical uncertainty associated with incorrectly classifying a pair of
unlensed GW events as a lensed event, and the associated False Alarm Rate (FAR)
depending on the GW source properties. To understand the effect of unlensed
astrophysical GW sources in producing false lensing detections, we have
performed a model-independent test using the pipeline GLANCE on a simulated
population of merging binary-black holes (BBHs). We find that $\sim$ 0.01\% of
the event pairs can be falsely classified as lensed with a lensing threshold
signal-to-noise ratio of 1.5, appearing at a time delay between the event pairs
of $\sim$ 1000 days or more. We show the FAR distribution for the parameter
space of GW source masses, delay time, and lensing magnification parameter over
which the model-independent technique GLANCE can confidently detect lensed GW
pair with the current LIGO detector sensitivity. In the future, this technique
will be useful for understanding the FAR of the upcoming next-generation GW
detectors, which can observe many more GW sources.

</details>


### [11] [Black hole mergers beyond general relativity: a self-force approach](https://arxiv.org/abs/2510.11793)
*Ayush Roy,Lorenzo Küchler,Adam Pound,Rodrigo Panosso Macedo*

Main category: gr-qc

TL;DR: The paper introduces a new first-principles method based on self-force theory to model black hole mergers in theories beyond general relativity, enabling calculations of merger waveforms with beyond-GR effects in extreme mass ratio inspirals.


<details>
  <summary>Details</summary>
Motivation: Binary black hole mergers' gravitational waves offer a unique regime to test GR and Kerr black holes, but numerical simulations in beyond-GR theories were previously infeasible for full parameter ranges.

Method: Uses self-force theory to model extreme mass ratio inspiral mergers and ringdown in modified gravity theories, allowing modular computation of beyond-GR effects in merger waveforms.

Result: First calculation of self-force effects on merger waveforms in beyond-GR theories, demonstrating an efficient waveform model incorporating these effects.

Conclusion: The approach provides a computationally feasible framework to study modified gravity effects in black hole mergers, advancing tests of GR and black hole properties.

Abstract: Gravitational waves from binary black hole mergers provide a glimpse of
gravitational dynamics in its most extreme observable regime, potentially
enabling precision tests of general relativity (GR) and of the Kerr description
of black holes. However, until recently, numerical simulations of black hole
mergers have not been possible in theories beyond GR. While recent
breakthroughs have overcome that obstacle, simulations covering the full,
interesting range of binary parameters remain unfeasible. Here we present a new
first-principles approach to this problem. We show how self-force theory can be
used to model the merger and ringdown of black holes in a broad class of
gravitational theories, assuming one object is much smaller than the other. We
calculate self-force effects on the merger waveform for the first time, and we
demonstrate how our formulation allows us to modularly compute beyond-GR
effects and readily incorporate them into a fast merger-ringdown waveform
model.

</details>


### [12] [Aspects of holographic complexity and volume of the black holes](https://arxiv.org/abs/2510.11833)
*Suraj Maurya,Sashideep Gutti,Rahul Nigam,Swastik Bhattacharya*

Main category: gr-qc

TL;DR: The article examines complexity growth rates in various black holes using CV/CA dualities, confirming they align with Susskind's conjecture linking them to temperature-entropy products. It also investigates how processes like the Penrose process and Hawking radiation affect these rates, showing non-trivial dependencies, especially under particle accretion where direction of angular momentum influences the outcome. Adjustments for horizon dynamics are recommended when negative complexity changes occur.


<details>
  <summary>Details</summary>
Motivation: To verify Susskind's conjecture about complexity growth rates being proportional to the product of horizon temperature and entropy for different black holes and to understand how physical processes alter these rates.

Method: The study applied complexity-volume (CV) and complexity-action (CA) dualities to analyze complexity growth rates for BTZ, Schwarzschild, Reissner Nordstrom, and Kerr black holes. The effects of the Penrose process, superradiance, particle accretion, and Hawking radiation were explored through calculations of δĊ and considerations of horizon dynamics.

Result: The complexity growth rates confirmed Susskind's conjecture. δĊ increases under Penrose and superradiance processes, varies (increase, zero, decrease) under particle accretion based on particle's angular momentum direction, and requires horizon dynamics adjustments for negative δĊ cases. Hawking radiation's effect was also analyzed.

Conclusion: Complexity growth rates in black holes adhere to Susskind's proposal. Physical processes significantly impact δĊ, necessitating dynamic horizon considerations for accurate measurements, especially when particle accretion leads to negative complexity changes. This work advances understanding of gravitational complexity dynamics.

Abstract: In this article, we study the complexity growth rate for Banados Teitlboim
Zanelli, Schwarzschild, Reissner Nordstrom, and Kerr black holes using
complexity-volume (CV) and complexity-action (CA) dualities and verify that it
is proportional to the product of the horizon temperature and entropy of the
black holes as conjectured by Susskind. Furthermore, we explore the variation
in the complexity growth rate $\delta \dot{\mathcal{C}}$ under various physical
processes, including the Penrose process, superradiance, particle accretion,
and Hawking radiation, and demonstrate that $\delta \dot{\mathcal{C}}$ exhibits
non-trivial behavior. Under the Penrose process and superradiance, $\delta
\dot{\mathcal{C}}$ always increases, and under particle accretion, $\delta
\dot{\mathcal{C}}$ can increase, remain zero, or decrease depending upon the
direction of angular momentum of an infalling particle. For the cases of
particle accretion, where we find $\delta \dot{\mathcal{C}}$ to be negative, we
argue that for a reliable estimate, one has to take into account the
contribution of the horizon dynamics of the perturbed black hole to the growth
of its complexity.

</details>


### [13] [Shift vector symmetry in the Alcubierre warp drive spacetime geometry](https://arxiv.org/abs/2510.11836)
*Osvaldo L. Santos-Pereira,Everton M. C. Abreu,Marcelo B. Ribeiro*

Main category: gr-qc

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This work explores the set of coupled partial differential equations of the
Einstein equations yielding vacuum solutions in the original Alcubierre warp
drive metric with the cosmological constant. It is shown that under an
appropriate ansatz they reveal a Burgers-type equation and a heat-type
equation. These results indicate that the spacetime distortion carrying a mass
particle at superluminal speeds, the Alcubierre warp bubble, may be interpreted
as a geometric analog of a propagating shock front, which suggests a possible
novel theoretical framework to deal with superluminal warp speeds.

</details>


### [14] [Lorentz Covariant Supertranslation Frames for the Angular Momentum Aspect](https://arxiv.org/abs/2510.11849)
*Reza Javadinezhad,Massimo Porrati*

Main category: gr-qc

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this letter, we review the well known ambiguity in defining angular
momentum (and mass dipole) fluxes in general relativity and we reinterpret
recent works that resolve the ambiguity by defining invariant charges. We
resolve the ambiguity by finding the conditions that fix a frame for
supertranslation and for space-time translation. We also present an elementary
method for measuring the angular momentum aspect and work out explicitly the
supertranslation frame-fixing conditions for the metric created by point
particles to first nontrivial order in the Newton constant.

</details>


### [15] [Impact of facility timing and coordination for next-generation gravitational-wave detectors](https://arxiv.org/abs/2510.11861)
*Ssohrab Borhanian,Arianna Renzini,Philippa S. Cole,Costantino Pacilio,Michele Mancarella,Davide Gerosa*

Main category: gr-qc

TL;DR: This paper evaluates how delays in deploying next-gen gravitational wave detectors affect network performance, finding that while sensitivity metrics are unaffected, localization is significantly impacted unless a current-gen detector like LIGO India supports the network.


<details>
  <summary>Details</summary>
Motivation: To assess the scientific implications of delayed detector deployment on network capabilities, ensuring optimal use of future facilities like the Einstein Telescope and Cosmic Explorer.

Method: The study uses Fisher information simulations for binary black holes, binary neutron stars, and primordial black-hole binaries, analyzing the required observation times for key metrics using bootstrapped populations.

Result: Delays minimally affect signal-to-noise ratio but severely impact localization in two-detector networks. Adding LIGO India reduces this impact, enhancing multi-messenger and stochastic background research.

Conclusion: Coordinated deployment timing is critical for localization-based science; integrating existing detectors (e.g., LIGO India) is vital to mitigate delays' negative effects.

Abstract: While the Einstein Telescope and Cosmic Explorer proposals for
next-generation, ground-based detectors promise vastly improved sensitivities
to gravitational-wave signals, only joint observations are expected to enable
the full scientific potential of these facilities, making timing and
coordination between the efforts crucial to avoid missed opportunities. This
study investigates the impact of long-term delays on the scientific
capabilities of next-generation detector networks. We use the Fisher
information formalism to simulate the performance of a set of detector networks
for large, fiducial populations of binary black holes, binary neutron stars,
and primordial black-hole binaries. Bootstrapping the simulated populations, we
map the expected observation times required to reach a number of observations
fulfilling scientific targets for key sensitivity and localization metrics
across various network configurations. We also investigate the sensitivity to
stochastic backgrounds. We find that purely sensitivity-driven metrics such as
the signal-to-noise ratio are not strongly affected by delays between
facilities. This is contrasted by the localization metrics, which are very
sensitive to the number of detectors in the network and, by extension, to
delayed observation campaigns for a detector. Effectively, delays in one
detector behave like network-wide interruptions for the localization metrics
for networks consisting of two next-generation facilities. We examine the
impact of a supporting, current-generation detector such as LIGO India
operating concurrently with next-generation facilities and find such an
addition will greatly mitigate the negative effects of delays for localization
metrics, with important consequences on multi-messenger science and stochastic
searches.

</details>


### [16] [Non-linear causal bulk viscosity in Unified Dark Matter Cosmologies](https://arxiv.org/abs/2510.11900)
*Guillermo Palma,Gabriel Gomez*

Main category: gr-qc

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We propose a bulk viscous unified dark matter scenario based on a nonlinear
extension of the full causal Israel-Stewart theory. This framework allows the
viscous fluid to remain far from equilibrium, an essential feature for a
physically consistent description of viscosity-driven accelerated expansion. We
adopt the standard parametrization for the bulk viscosity, $\xi = \xi_{0}
\rho_{m}^{s}$, treating $s$ as a free parameter, and study the model in a
spatially flat Friedmann-Robertson-Walker background. By reformulating the
cosmological equations as an autonomous dynamical system, we obtain both
asymptotic analytical solutions and a numerical characterization of the phase
space. At early times, the viscous component can mimic a stiff fluid, while at
intermediate epochs it behaves like dark matter. With a suitable choice of
dynamical variables, the system admits three distinct classes of late-time
attractors. Two of them are separated by a basin-boundary saddle point: (i) a
generic quintessence solution for $s = 1/2$, which encompasses a de Sitter-like
behavior when $\xi_{0}$ satisfies a specific relation involving the nonlinear
parameters; (ii) a global exact de Sitter attractor for $s < 1/2$; and (iii) a
phantom-like solution that emerges for $s \ge 1/2$. In contrast to the generic
$s \ne 1/2$ case, the $s = 1/2$ scenario exhibits a qualitatively different
stability structure, allowing de Sitter and phantom attractors to coexist. All
solutions respect entropy production, and cosmic acceleration emerges
independently of $\xi_{0}$, relaxing the strong bounds $\xi_{0} \sim
\mathcal{O}(1)$ required in Eckart-based viscous models.

</details>


### [17] [Information paradox and island of covariant black holes in LQG](https://arxiv.org/abs/2510.11921)
*Yongbin Du,Jia-Rui Sun,Xiangdong Zhang*

Main category: gr-qc

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study information paradox of four dimensional covariant black holes
inspired by loop quantum gravity (LQG) with two well motivated solutions. We
first prepare the spacetime in the Hartle-Hawking state, compute the radiation
entropy and recover a linear growth at late time. When considering the mass
loss and incorporating greybody factors, we show that for Solution~1 the LQG
parameter $\zeta$ leaves temperature and Planckian factor of the spectrum
unchanged but enhances the near-horizon barrier, leading to a faster
evaporation rate as $M$ decreases. This behavior contrasts sharply with
Solution~2, which has slow evaporation rate at small $M$ and admits a
non-singular continuation suggestive of a remnant or a black-to-white-hole
transition. We then apply the island prescription on the eternal background and
find that quantum extremal surfaces exist in solution 1 geometries; $\zeta$
primarily shifts the island boundary and suppresses the late time entropy
growth, preserving unitarity. Our results highlight that covariance-respecting
LQG black hole do not exhibit a universal late time behavior.

</details>


### [18] [Phase space analysis of an exponential model in $f(Q)$ gravity including linear dark-sector interactions](https://arxiv.org/abs/2510.12020)
*Ivan R. Vasquez,A. Oliveros*

Main category: gr-qc

TL;DR: The paper analyzes a cosmological exponential $f(Q)$ gravity model using dynamic systems theory, finding it supports key cosmic epochs and influences late-time acceleration.


<details>
  <summary>Details</summary>
Motivation: To explore the cosmological implications of exponential $f(Q)$ gravity and its ability to explain cosmic evolution through critical points analysis.

Method: Adopts Böhmery's method to transform field equations into autonomous systems; approximates transcendental equations from exponential $f(Q)$, then evaluates critical points' stability. Later incorporates dark energy-matter interaction.

Result: Identifies critical points corresponding to main cosmic eras (radiation, matter, dark energy domination) and shows the model's late-time de Sitter attractor behavior, affected by added interactions.

Conclusion: Exponential $f(Q)$ gravity effectively models cosmic history with stable critical points, offering insights into the universe's accelerating expansion via modified gravity.

Abstract: We present a cosmological analysis of an exponential $f(Q)$ gravity model,
within the dynamical systems formalism. Following the method introduced by
B\"ohmer \textit{et al} [Universe \textbf{9} no.4, 166 (2023)], the modified
Friedmann modified equations are successfully reduced to an autonomous system.
Given the exponential form of $f(Q)$, the equilibrium conditions result in
transcendental equations, which we approximate to identify the critical points.
We therefore perform a general stability analysis of these points in terms of
the model parameters. Finally, we extend the model by including a linear dark
energy-dark matter interaction, where the equilibrium points are found with
their stability properties. The model exhibits the three main domination epochs
in the Universe, as well as a non-trivial impact on the late-time de Sitter
attractor.

</details>


### [19] [Coexistence of Spectrally Stable and Unstable Modes in Black Hole Ringdowns](https://arxiv.org/abs/2510.12135)
*Peng Wang,Tianshu Wu*

Main category: gr-qc

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent studies have shown that a secondary potential barrier, forming a
potential well outside the event horizon, can destabilize the Quasinormal Mode
(QNM) spectrum of black holes. We find that spectral instability may persist
even after the potential well vanishes, giving rise to a distinct family of
spectrally unstable QNMs that differ from the spectrally stable modes localized
near the potential peak and associated with the photon sphere. Nevertheless,
time-domain simulations reveal that early-time ringdown waveforms remain
dominated by stable modes, while unstable modes have only a subdominant
contribution. These results highlight the robustness of black hole
spectroscopy, as the observable ringdown signal is primarily governed by the
most stable QNMs.

</details>


### [20] [The importance of being non-minimally coupled: scalar Hawking radiation from regular black holes](https://arxiv.org/abs/2510.12257)
*Marco Calzà,Massimiliano Rinaldi,Sunny Vagnozzi*

Main category: gr-qc

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In curved space-time, a scalar field $\phi$ is generically expected to couple
to curvature, via a coupling of the form $\xi\phi^2R$. Yet in the study of
Hawking emission from regular black holes (RBHs), where scalar fields are often
introduced as simple probes of the geometry, and the Ricci scalar is
generically non-zero, this non-minimal coupling is almost always ignored. We
revisit this assumption by studying scalar Hawking emission from four
representative RBHs (the Bardeen, Hayward, Simpson-Visser, and
D'Ambrosio-Rovelli space-times), within two benchmark cases: the conformal case
$\xi=1/6$, and a large negative value $\xi=-10^4$ motivated by Higgs inflation.
We compute the graybody factors and emission spectra, showing that the latter
can be either enhanced or suppressed, even by several orders of magnitude. A
crucial role is played by the sign of the term $\xi fR$, with $f(r)=-g_{tt}$ in
Schwarzschild-like coordinates, as it determines whether the non-minimal
coupling suppresses or enhances the geometric potential barrier. For the
D'Ambrosio-Rovelli case with large negative $\xi$, the low-energy emission
spectrum is enhanced by up to five orders of magnitude, since $\xi fR<0$
throughout the space-time, leading to a deep potential well which broadens the
transmissive window. The deviations we find can be particularly relevant in the
case where primordial RBHs are dark matter candidates, given the impact of the
non-minimal coupling on their evaporation history.

</details>


### [21] [Observational Constraints on Chaplygin Gas Models in Non-Minimally Coupled Power Law $f(Q)$ Gravity with Quasars](https://arxiv.org/abs/2510.12472)
*Nakul Aggarwal,Ali Pourmand,Fatimah Shojai,Harish Parthasarathy*

Main category: gr-qc

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In the framework of $f(Q)$ gravity, where gravity emerges from non-metricity
$Q$, we explore the cosmological implications of its non-minimal coupling to
matter. Inspired by the recent success of Chaplygin gas models in explaining
dark energy, we consider a background fluid composed of baryonic matter,
radiation, and a family of Chaplygin gas variants namely Generalized Chaplygin
Gas (GCG), Modified Chaplygin Gas (MCG), and Variable Chaplygin Gas (VCG). We
constrain these models with three recent observational datasets: Observational
Hubble Data (OHD), Baryonic Acoustic Oscillation (BAO) measurements, and
Quasi-Stellar Objects (QSO) data. For the QSO dataset, we propose an analytical
expression for errors in comoving distance to circumvent the reliance on Monte
Carlo simulations. Using kinematic diagnostics such as the deceleration and
jerk parameters and Om diagnostic, we assess deviations of the proposed models
from $\Lambda$CDM. Our joint analysis of the three datasets reveals that the
transition redshift from a decelerated to an accelerated expansion of the
universe for the GCG, MCG and VCG models is $0.620^{+0.018}_{-0.017}$,
$0.537^{+0.017}_{-0.017}$ and $0.470^{+0.012}_{-0.012}$ respectively,
indicating a departure from $\Lambda$CDM.

</details>


### [22] [OCTOPUS: A Versatile, User-Friendly, and Extensible Public Code for General-Relativistic Ray-Tracing in Spherically Symmetric and Static Spacetimes](https://arxiv.org/abs/2510.12585)
*Shiyang Hu,Shijie Tan,Dan Li,Lina Zhang,Chen Deng,Wenfu Cao*

Main category: gr-qc

TL;DR: The paper introduces OCTOPUS, a relativistic ray-tracing algorithm for simulating black hole observables in spherically symmetric spacetimes. It efficiently computes features like black hole shadows and gravitational lensing effects, validated using a Schwarzschild black hole with a Dehnen dark matter halo. The algorithm shows that denser dark matter halos enhance spacetime curvature, affecting observational signatures.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a versatile, efficient computational tool for studying relativistic observables in black hole systems and their correlations with gravitational signals, particularly addressing the need for automated, modular, and adaptable frameworks in astrophysical research.

Method: OCTOPUS uses a Fortran-based, OpenMP-accelerated framework to solve ray-tracing equations in curved spacetimes. It requires only the metric potential and its radial derivatives as input, enabling straightforward implementation for various spacetime models. Validation involved simulating a Schwarzschild black hole with a Dehnen dark matter halo to assess algorithm precision and the impact of dark matter parameters.

Result: The validated algorithm demonstrates high precision and efficiency, showing that increasing dark matter halo density and scale intensifies gravitational effects observable in black hole shadows, redshift distributions, and hot-spot light curves. These effects provide testable signatures for dark matter's role in spacetime curvature.

Conclusion: OCTOPUS offers a robust platform for studying black hole phenomenology and multi-messenger astronomy. The results highlight the importance of dark matter halo properties in modifying observable spacetime features, with future extensions planned for axisymmetric spacetimes to broaden applicability.

Abstract: This paper presents OCTOPUS, a relativistic ray-tracing algorithm developed
within a Fortran-based, OpenMP-accelerated framework, designed for
asymptotically flat, spherically symmetric curved spacetimes. The code
efficiently and accurately computes key relativistic features -- including the
black hole event horizon, photon rings, critical curves, and innermost stable
circular orbits -- and simulates black hole shadows, redshift factor
distributions, accretion disk images, toroidal images, as well as gravitational
lensing, light curves, and gravitational radiation from hot-spots. OCTOPUS
provides an automated, modular solution for qualitative studies of black hole
observables and multi-messenger correlations between electromagnetic and
gravitational signals in curved spacetime. Its implementation requires only the
metric potential and its first-, second-, and third-order radial derivatives as
input, ensuring low user barriers while remaining highly extensible and
adaptable. Using a Schwarzschild black hole surrounded by a Dehnen-type dark
matter halo, we thoroughly validate the algorithm's precision, efficiency, and
functionality, and investigate how dark matter halo parameters affect
observational signatures. Our results demonstrate that increasing the scale and
density of the dark matter halo strengthens the spacetime's gravitational
field, an effect clearly reflected in black hole images and supported by
hot-spot light curve signatures. A future version of OCTOPUS, with expanded
capabilities for axisymmetric spacetimes, is planned for release.

</details>


### [23] [Hessian in the spinfoam models with cosmological constant](https://arxiv.org/abs/2510.12755)
*Wojciech Kamiński,Qiaoyin Pan*

Main category: gr-qc

TL;DR: The paper establishes the non-degeneracy of the Hessian in spinfoam vertex amplitudes for quantum gravity with cosmological constants, confirming applicability of the stationary phase method and avoiding problematic contributions from exceptional configurations.


<details>
  <summary>Details</summary>
Motivation: To prove the non-degeneracy of the Hessian in spinfoam models with a cosmological constant, ensuring the validity of the stationary phase approximation and ruling out unwanted semiclassical contributions.

Method: Reformulated the Hessian non-degeneracy problem using transverse intersection of submanifolds in the phase space of flat SL(2,C) connections, focusing on critical points corresponding to non-degenerate geometric 4-simplices in de Sitter/AdS spaces.

Result: Demonstrated Hessian non-degeneracy for non-degenerate 4-simplices in dS/AdS spacetimes, validated the stationary phase method's applicability, and eliminated dominant contributions from exceptional configurations.

Conclusion: The established criterion confirms the model's consistency with semiclassical gravity and its superiority over the Barrett-Crain model by avoiding problematic contributions, with potential applicability to other spinfoam models after minor adjustments.

Abstract: In this paper, we introduce a general method to prove the non-degeneracy of
the Hessian in the spinfoam vertex amplitude for quantum gravity and apply it
to the spinfoam models with a cosmological constant ($\Lambda$-SF models). By
reformulating the problem in terms of the transverse intersection of some
submanifolds in the phase space of flat ${\rm SL}(2,\mathbb{C})$ connections,
we demonstrate that the Hessian is non-degenerate for critical points
corresponding to non-degenerate, geometric 4-simplices in de Sitter or anti-de
Sitter space. Non-degeneracy of the Hessian is an important necessary condition
for the stationary phase method to be applicable. With a non-degenerate
Hessian, this method not only confirms the connection of the $\Lambda$-SF model
to semiclassical gravity, but also shows that there are no dominant
contributions from exceptional configurations as in the Barrett-Crane model.
Given its general nature, we expect our criterion to be applicable to other
spinfoam models under mild adjustments.

</details>


### [24] [The Gravitational Wave Memory from Binary Neutron Star Mergers](https://arxiv.org/abs/2510.09742)
*Jamie Bamber,Antonios Tsokaros,Milton Ruiz,Stuart L. Shapiro,Marc Favata,Matthew Karlson,Fabrizio Venturi Piñas*

Main category: gr-qc

TL;DR: The study quantifies the effects of neutron star magnetic fields, neutrino emission, and ejected mass on gravitational wave memory in binary neutron star mergers. Key findings include significant contributions (up to 50%) from electromagnetic radiation and magnetic field effects, emphasizing the need to consider these factors for accurate parameter estimation.


<details>
  <summary>Details</summary>
Motivation: To understand how neutron star properties and emission processes influence gravitational wave memory, which has not yet been observed. Accurate modeling is crucial for future parameter estimation from gravitational wave data.

Method: General relativistic magnetohydrodynamic simulations, including neutrinos and various equations of state, were used to analyze memory contributions from magnetic fields, neutrinos, and ejecta in binary neutron star mergers.

Result: Electromagnetic and neutrino contributions can account for up to 50% of memory effects in extreme magnetic fields. Magnetic fields significantly affect gravitational wave luminosity and angular momentum redistribution, altering memory dynamics. Memory duration is extended due to prolonged emission from electromagnetic fields, neutrinos, and ejecta.

Conclusion: Accurate parameter estimation for gravitational wave memory requires incorporating magnetic fields and equations of state. Magnetic fields influence memory through both direct electromagnetic contributions and indirect gravitational wave effects.

Abstract: The gravitational wave signal produced by the merger of two compact objects
includes both an oscillatory transient and a non-oscillatory part, the
so-called memory effect. This produces a permanent displacement of test masses
and has not yet been measured. We use general relativistic magnetohydrodynamic
simulations, including neutrinos, with several representative viable equations
of state, to quantify--for the first time--the effects of the neutron star
magnetic field, neutrino emission, and the ejected mass on the linear and
nonlinear displacement memory in binary neutron star mergers. We find that the
additional contributions due to the emission of electromagnetic radiation,
neutrinos and baryonic ejecta can be ~15% of the total memory for moderate
magnetic fields and up to ~50% for extreme magnetic fields. The memory is most
affected by changes in the equation of state, the binary mass, and the magnetic
field. In particular, for moderate premerger field strengths, the dominant
impact of the electromagnetic field is the change in the gravitational wave
luminosity, and the associated gravitational wave null memory, due to the
unstable growth of the magnetic field and the resulting redistribution of
angular momentum it induces in the remnant. While the direct electromagnetic
contribution to the null memory is additive, the change in the gravitational
wave null memory can--in some cases--result in the total memory being smaller
than that from the corresponding nonmagnetized binary. Furthermore, in contrast
to binary black hole mergers, the growth of the memory in binary neutron star
mergers is extended due to the long emission timescale of electromagnetic
fields, neutrinos, and ejecta. These results necessitate the consideration of
the magnetic field, as well as the equation of state, for accurate parameter
estimation in future analyses of gravitational wave memory data.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [25] [More Effective RS Field Theory](https://arxiv.org/abs/2510.11771)
*Severin Lüst,Michael Nee,Lisa Randall*

Main category: hep-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this paper we derive the effective theory for a stabilized
five-dimensional warped geometry, addressing several outstanding issues in this
derivation. These include allowing for a non-zero 4d cosmological constant,
accounting for constraints from both the UV and IR branes, and determining how
the stabilized theory responds to an energy perturbation on each brane. We show
how a consistent low-energy effective theory from a stable warped solution must
respect a constraint that follows from the higher-dimensional Einstein
equation. Satisfying the constraint requires that the 4d cosmological constant
must be the same everywhere throughout the bulk, which means the stabilizing
fields must adjust to allow for the same 4d curvature everywhere in the extra
dimension. We show explicitly how this works in a 5d model, and how the correct
4d effective potential reproduces this behavior. We find that the cosmological
constant generated from adding energy to one of the branes is unaffected by the
details of the stabilization mechanism at leading order, despite the need for
the stabilizing fields to adjust. In anticipation of a companion paper, we also
briefly discuss how supersymmetry can be realized consistently in the 5d
theory. In particular, we show how the stabilization mechanism remains
consistent with sequestering, even as the supersymmetry-breaking energy is
reflected in the stabilizing field throughout the bulk.

</details>


### [26] [Lepton Flavor Violation and Local Lepton Number](https://arxiv.org/abs/2510.11774)
*Hridoy Debnath,Pavel Fileviez Perez*

Main category: hep-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We investigate the predictions for lepton number violating processes within
the minimal theory of neutrino masses based on the spontaneous breaking of
local lepton number. In this framework, the symmetry is broken at the low
scale, leading to the existence of a viable dark matter candidate. The new
fermions required for anomaly cancellation mediate lepton number violating
processes at the one-loop level. We present a detailed calculation of the most
relevant processes, including $\mu \to e \gamma$, $\mu \to 3 e$, and $\mu \to
e$ conversion in nuclei. The regions of parameter space excluded by current
experimental bounds are identified, and we emphasize the interplay between
collider observables and charged lepton flavor violating signatures as a key
test of this minimal theory of neutrino masses.

</details>


### [27] [BBN Constraints on the Hadronic Annihilation of sub-GeV Dark Matter](https://arxiv.org/abs/2510.11791)
*Afif Omar,Adam Ritz*

Main category: hep-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We investigate the impact of residual annihilation from sub-GeV mass thermal
relic dark matter candidates during big bang nucleosynthesis (BBN). Focusing on
candidates with $p$-wave annihilation channels, we show that the hadronic
injection of pions and kaons beyond freeze-out, and their subsequent
interaction with protons and neutrons prior to the deuterium bottleneck,
provides a sensitivity to annihilation that surpasses that of the CMB and
indirect detection in the galaxy.

</details>


### [28] [On the Speed-up of Wave-like Dark Matter Searches with Entangled Qubits](https://arxiv.org/abs/2510.11795)
*Arushi Bodas,Sohitri Ghosh,Roni Harnik*

Main category: hep-ph

TL;DR: Entangled qubit-based platforms for dark matter detection are analyzed for their search capabilities, showing a quadratic signal scaling with qubit number, and superior scan rates and bandwidth compared to conventional methods, competitive with established experiments for higher mass ranges with achievable qubit coherence and error rates.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to explore the potential of entangled qubit-based systems in improving the sensitivity and efficiency of dark matter searches, particularly addressing challenges faced by traditional approaches like photon-counting cavities. The focus is on enhancing scan rates and maintaining bandwidth advantages while considering realistic error rates and coherence times.

Method: The researchers analyze the bandwidth, scan rate, and error tolerance of entangled qubit protocols, comparing phase-based readout with power-based detection schemes. They derive scaling requirements for coherence times and error rates, and apply their framework specifically to dark photon searches.

Result: Phase-based entangled qubit systems maintain search bandwidth independent of qubit count, outperforming power-based methods. For ~100-qubit systems, competitive results with photon-counting cavities are achievable for dark photon masses above 30-40 µeV, with advantages increasing at higher masses where cavity methods struggle.

Conclusion: Entangled qubit systems present a viable and potentially superior approach for dark matter detection in certain mass ranges, provided technological advancements meet the identified coherence and error rate thresholds. This work establishes a path for leveraging quantum technologies to push the boundaries of dark matter experiments.

Abstract: Qubit-based sensing platforms offer promising new directions for wave-like
dark matter searches. Recent proposals demonstrate that entangled qubits can
achieve quadratic scaling of the signal in the number of qubits. In this work
we expand on these proposals to analyze the bandwidth and scan rate performance
of entangled qubit protocols across different error regimes. We find that the
phase-based readout of entangled protocols preserves the search bandwidth
independent of qubit number, in contrast to power-based detection schemes,
thereby achieving a genuine scan-rate advantage. We derive coherence time and
error rate requirements for qubit systems to realize this advantage. Applying
our analysis to dark photon searches, we find that entangled states of
approximately 100 qubits can become competitive with benchmark photon-counting
cavity experiments for masses $\gtrsim 30{-}40~\mu{\rm eV}$, provided
sufficiently low error rates are achieved. The advantage increases at higher
masses where cavity volume scaling becomes less favorable.

</details>


### [29] [Dynamically generated tilt of isocurvature fluctuations](https://arxiv.org/abs/2510.11803)
*Saarik Kalia*

Main category: hep-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Light scalar fields acquire isocurvature fluctuations during inflation. While
these fluctuations could lead to interesting observable signatures at small
scales, they are strongly constrained on large scales by cosmic microwave
background observations. When the mass of the scalar is much lighter than the
inflationary Hubble scale, $m\ll H_I$, the spectrum of these fluctuations is
flat. Meanwhile, if $m\gg H_I$, the fluctuations are suppressed. A blue-tilted
isocurvature spectrum which exhibits enhanced structure on small scales but
avoids observational constraints on large scales therefore requires a
coincidence of scales $m\sim H_I$ for a free massive scalar. In this Letter, we
show that if a scalar field possesses a nontrivial potential, its inflationary
dynamics naturally cause this condition to be satisfied, and so a blue-tilted
spectrum is generically expected for a large class of potentials. Specifically,
if its potential $V$ exhibits a region which satisfies the slow-roll condition
$V''<3H_I^2$, the scalar condensate will spend most of inflation close to the
boundary of this region, so that its effective mass is typically close to
$H_I$. The resulting blue tilt is inversely proportional to the number of
$e$-folds of inflation prior to horizon crossing. If the scalar is long-lived,
this mechanism leads to an attractor prediction for its relic abundance, which
is insensitive to initial conditions of the scalar. In particular, a scalar
field with quartic self-interactions can achieve the correct abundance to
constitute all of the dark matter for a wide range of masses. We compute the
relationship between the mass and self-coupling of quartic dark matter
predicted by this mechanism.

</details>


### [30] [Modeling the TMD shape function in $J/ψ$ electroproduction](https://arxiv.org/abs/2510.11809)
*Miguel G. Echevarria,Raj Kishore,Samuel F. Romera*

Main category: hep-ph

TL;DR: The paper calculates the next-to-leading order hard function for quarkonium electroproduction using TMD factorization and analyzes the TMD shape function's role in $J/\psi$ electroproduction cross-section predictions at the Electron-Ion Collider.


<details>
  <summary>Details</summary>
Motivation: To improve precision predictions for quarkonium electroproduction in the low-transverse-momentum regime and understand the phenomenological role of the TMD shape function in leptoproduction processes.

Method: The study employs transverse-momentum-dependent (TMD) factorization framework, analyzing the operator-level definition of the TMD shape function. It calculates the NLO hard function and examines the convolution of the unpolarized TMD gluon distribution with the TMD shape function.

Result: Predictions for the unpolarized differential cross-section of $J/\psi$ electroproduction at the future Electron-Ion Collider are provided, focusing on the small transverse momentum region.

Conclusion: The analysis highlights the importance of TMD effects and the convolution structure for precise phenomenological studies of quarkonium production in the low-$p_T$ regime.

Abstract: The next-to-leading order hard function for quarkonium electroproduction is
calculated within the framework of transverse-momentum-dependent (TMD)
factorization in the low-transverse-momentum regime. The structure of the TMD
shape function in quarkonium leptoproduction is analyzed through its
operator-level definition. Particular attention is given to the convolution of
the unpolarized TMD gluon distribution with the TMD shape function, thereby
illustrating the latter's phenomenological role. Building on this framework, we
provide predictions for the unpolarized differential cross-section of $J/\psi$
electroproduction at the future Electron-Ion Collider in the region of small
transverse momentum.

</details>


### [31] [A SHIFT of Perspective: Observing Neutrinos at CMS and ATLAS](https://arxiv.org/abs/2510.11816)
*Alfonso Garcia-Soto,Jeremi Niedziela*

Main category: hep-ph

TL;DR: Analyzes the neutrino physics potential of the proposed SHIFT@LHC experiment using LHC Run-4 simulations, showing feasibility for detecting O(10^4) muon-neutrinos and O(10^3) electron-neutrinos in CMS/ATLAS detectors. Highlights first detection of neutrinos in general-purpose LHC detectors to study hadron production in 5<η<8 range.


<details>
  <summary>Details</summary>
Motivation: Exploit LHC infrastructure to create a novel neutrino source using fixed-target gas collisions, accessing uncharted pseudorapidity regions (5<η<8) beyond current detector coverage. Provides unique data for neutrino/interaction studies relevant to atmospheric neutrino experiments.

Method: Simulated proton-gas collisions, hadron propagation, and neutrino interactions in CMS/ATLAS detectors using 1% of LHC Run-4 luminosity. Estimated detectable neutrino interaction rates across 20 GeV-1 TeV energy spectrum.

Result: Prediction of ~10,000 muon-neutrino and ~1,000 electron-neutrino interactions detectable in current LHC detectors. Demonstrates technical feasibility of first neutrino detection in general-purpose LHC detectors.

Conclusion: SHIFT@LHC offers a new experimental avenue for studying hadron production in 5<η<8 region and neutrino interactions in energy regimes relevant to atmospheric experiments, leveraging existing LHC infrastructure without needing new detectors.

Abstract: We investigate the physics potential of SHIFT@LHC, a proposed gaseous fixed
target installed in the LHC tunnel, as a novel source of detectable neutrinos.
Using simulations of proton-gas collisions, hadron propagation, and neutrino
interactions, we estimate that $O(10^4)$ muon-neutrino and $O(10^3)$
electron-neutrino interactions, spanning energies from 20 GeV to 1 TeV, would
occur in the CMS and ATLAS detectors with 1% of the LHC Run-4 integrated
luminosity. This unique configuration provides access to hadron production in
the pseudorapidity range 5<$\eta$<8, complementary to existing LHC detectors.
If realized, this would mark the first detection of neutrinos in a
general-purpose LHC detector, opening a new avenue to study neutrino production
and interactions in a regime directly relevant to atmospheric neutrino
experiments.

</details>


### [32] [New Tests of Low-Scale Quantum Gravity with Cosmic-Ray Collisions](https://arxiv.org/abs/2510.11879)
*Manuel Ettengruber,Gonzalo Herrera*

Main category: hep-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Cosmic ray collisions at high center of mass energy could enable graviton and
black hole production as expected in theories of low-scale quantum gravity,
such as extra-dimensions, many species, or some versions of string theory. Here
we propose three novel phenomenological tests of these theories. We first
consider the collision of cosmic rays with ambient protons, electrons and
photons in Active Galactic Nuclei (AGN), finding that high-energy neutrino data
from the blazar TXS 0506+056 places a constraint on the fundamental scale of
gravity of $M_f \gtrsim 0.3$ TeV, and future high-energy neutrino data could
raise this bound to $M_f \gtrsim 200$ TeV. We then point out that collisions of
pairs of cosmic rays could occur at a sizable rate in AGN where the accelerated
cosmic rays are not collimated, or on supermassive black hole binaries. This
consideration could potentially let us test unprecedented large fundamental
scales of $M_f \gtrsim 2$ PeV. We further compute the corresponding thermal
neutrino emission arising from the Hawking evaporation of black holes produced
in cosmic ray collisions, finding a spectrum that clearly differs from that
expected in meson decays. Finally, we speculate with an scenario which would
produce high-energy neutrino and gamma-ray emission from regions in the sky
where no multi-wavelength counterparts would be expected, via graviton
propagation from a different brane, which then decays in our Universe.

</details>


### [33] [Spectroscopy of charmonium-like mesons, heavy-light mesons with charm, AdS/QCD, and configurational entropy](https://arxiv.org/abs/2510.11885)
*A. E. Bernardini,W. de Paula,R. da Rocha*

Main category: hep-ph

TL;DR: The paper uses the AdS/QCD model with four quark flavors to analyze charmed meson resonances and charmonium-like states, computing differential configurational entropy to predict masses of heavier mesons and validate experimental data.


<details>
  <summary>Details</summary>
Motivation: To explore properties of heavy-light-flavor mesons and charmonium-like states, predict unobserved charmed meson resonances, and validate theoretical models against PDG data.

Method: AdS/QCD model incorporating four quark flavors, computation of differential configurational entropy, comparison with experimental data from PDG for D^0, D^*, η_c, and χ_{c1} mesons.

Result: Enables prediction of mass spectra for heavier charmed mesons and identification of additional states in PDG through entropy analysis and model fitting.

Conclusion: The AdS/QCD approach with four flavors effectively predicts charmed meson spectra and aligns with experimental data, offering a robust framework for future quarkonium studies.

Abstract: Heavy-light-flavor meson resonances with charm, in the $D^0$ and $D^*$
families, and charmonium-like states, in the $\eta_c$ and $\chi_{c1}$ families,
are explored and discussed in the AdS/QCD model with four quark flavors. The
differential configurational entropy is computed and analyzed for these four
charmed meson families, also combining 4-flavor AdS/QCD to experimental data
for the $D^0$, $D^*$, $\eta_c$, and $\chi_{c1}$ meson families. It makes it
possible to predict the mass spectrum of unexplored heavier charmed meson
resonances and to identify further charmed meson states reported in PDG.

</details>


### [34] [Gravitational Wave Signatures from Lepton Number Breaking Phase Transitions with Flat Potentials](https://arxiv.org/abs/2510.11913)
*Gabriela Barenboim,Yeji Park,Liliana Velasco-Sevilla*

Main category: hep-ph

TL;DR: Thermal inflation models with flaton fields address late-time relic issues post-primordial inflation, diluting unwanted relics without disrupting baryon asymmetry or large-scale structure. Conditions for gravitational wave observability via potential barriers are explored.


<details>
  <summary>Details</summary>
Motivation: To resolve problems caused by late-time relics in Standard Model extensions, such as entropy injection and disruption of Big Bang Nucleosynthesis, while preserving primordial inflation's successes.

Method: Analysis of flaton potentials in thermal inflation models within and without supergravity, focusing on conditions for forming barriers that generate observable gravitational waves when flatons transition to true minima.

Result: Identifies an optimal thermal inflation scale (10^6-10^8 GeV for supergravity; above EW scale otherwise) and conditions for potential barriers enabling detectable gravitational waves.

Conclusion: Thermal inflation is essential for cosmological relic management and gravitational wave signatures offer testable predictions for experimental validation.

Abstract: Extensions of the Standard Model typically contain ``flaton fields" defined
as fields with large vacuum expectation values and almost flat potentials where
scalar self-coupling is small or vanishes at tree level. Such potentials have
been used to drive a secondary inflationary epoch after a primary phase of
inflation, in what are called thermal inflation models. Although the
primordial, high-scale inflationary epoch can solve the horizon and flatness
problems, it does not always resolve difficulties associated with late-time
relics produced in extensions of the Standard Model. These relics typically
decay too late, injecting entropy and energetic particles that spoil successful
predictions like Big Bang Nucleosynthesis. It is here that thermal inflation
plays a crucial role: diluting unwanted relics by many orders of magnitude
without erasing the baryon asymmetry or the large-scale structure set up by the
earlier phase of inflation. The preferred scale for this phenomenon is in the
range $10^6-10^8$ GeV if one considers supergravity, but without it, any scale
above the EW scale is valid. We investigate a typical form of these potentials
and determine what are the conditions for the potentials to develop a barrier
such that when the flatons settle to the true minimum, the associated
Gravitational Waves can be observed.

</details>


### [35] [Jet quenching without energy loss](https://arxiv.org/abs/2510.11914)
*Liliana Apolinário,Chiara Le Roux,Korinna Zapp*

Main category: hep-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The onset of jet quenching, i.e. the suppression of high transverse momentum
particles and jets, is an important question in the context of understanding
the onset of collective behaviour and small collision systems. We investigate a
minimal scenario where a hard parton experiences a single soft re-scattering
that leaves the kinematics unmodified, but the colour exchange leads to a loss
of colour coherence that is observable in the final distribution of fragments.
In particular, the formation time of the first splitting as reconstructed at
hadron level from jets using the formation time clustering algorithm is
sensitive the loss of colour coherence. Moreover, it can distinguish the
coherence loss scenario from a corresponding energy loss scenario, since small
energy loss effects leave the formation time distribution unchanged.

</details>


### [36] [First simultaneous global QCD analysis of kaon and pion parton distributions with lattice QCD constraints](https://arxiv.org/abs/2510.11979)
*P. C. Barry,Chueng-Ryong Ji,W. Melnitchouk,N. Sato,Fernanda Steffens*

Main category: hep-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We perform the first simultaneous global QCD analysis of pion and kaon parton
distribution functions (PDFs), constrained by pion- and kaon-induced Drell-Yan
(DY) and leading neutron electroproduction data, together with lattice QCD data
on pion and kaon PDF moments. The analysis indicates a softer valence $\bar u$
distribution in the $K^-$ than in the $\pi^-$, and a significantly more peaked
valence $s$-quark density in $K^-$ compared with the $\bar u$. The effective
exponent governing the high-$x$ behavior of the PDF is found to be larger for
$\bar u$ in the kaon, $\beta_{\bar u}^{K^-}\!= 1.6(2)$, than in the pion,
$\beta_{\bar u}^{\pi^-}\!= 1.16(4)$, in the range $0.7 \leq x \leq 0.95$. From
the gluon momentum fractions we find the pion's gluon content accounts for
$\approx 1/3$ of the mass budget of the pion at $\mu=2~{\rm GeV}$, but only
$\approx 1/4$ for the kaon.

</details>


### [37] [A Solution to the Hierarchy Problem with Non-Linear Quantum Mechanics](https://arxiv.org/abs/2510.12030)
*David E. Kaplan,Surjeet Rajendran*

Main category: hep-ph

TL;DR: The paper proposes adding a state-dependent term to the Higgs sector to solve the hierarchy problem, allowing the Higgs mass to be naturally light without fine-tuning, even at Planck scale. Some cosmological effects may differ from the Standard Model.


<details>
  <summary>Details</summary>
Motivation: To address the hierarchy problem in the Standard Model where the Higgs boson's mass is unnaturally light compared to the Planck scale without fine-tuning.

Method: Introduce a scalar field with a Higgs-like potential containing an additional term proportional to the expectation value of the squared Higgs field operator.

Result: The Higgs mass can remain parametrically lighter than the cutoff scale without fine-tuning, achieving technical naturalness even at Planck scale. Some cosmological behaviors differ significantly.

Conclusion: The modified Higgs sector offers a natural solution to the hierarchy problem. While simple versions may not be testable at colliders, other versions and cosmological observations could provide distinctions from the Standard Model.

Abstract: We argue that the hierarchy problem of the standard model of particle physics
can be solved by adding a state-dependent term to the Higgs sector. We present
an example of a scalar field with a Higgs-like potential with an additional
term proportional to the expectation value of the squared Higgs field operator.
We show that the mass can be parametrically lighter than the theory's
energy-momentum cutoff without fine tuning. We find the Higgs mass can be
technically natural, even with a Planck-scale cutoff. The simplest version of
the theory may not be distinguishable from the standard model at colliders, but
other versions might. In addition, some aspects of cosmological evolution can
be different in this model, in some cases radically.

</details>


### [38] [Gradient-flowed operator product expansion without IR renormalons](https://arxiv.org/abs/2510.12193)
*Martin Beneke,Hiromasa Takaura*

Main category: hep-ph

TL;DR: The paper proposes using the gradient flow time $1/\sqrt{t}$ as a factorization scale to combine perturbative QCD expansions with OPE power corrections, eliminating infrared renormalon ambiguities. This method improves convergence, reduces uncertainties in the Adler function, and resolves discrepancies in tau lepton decay width analyses.


<details>
  <summary>Details</summary>
Motivation: To address the long-standing issue of combining perturbative QCD with OPE power corrections, which suffer from infrared renormalon ambiguities overlapping in scale with power corrections, thereby improving the predictivity of low $Q^2$ calculations.

Method: Replace traditional factorization scales with gradient flow time $1/\sqrt{t}$; use subtracted perturbative expansions and gradient-flow regularized operator matrix elements in the OPE framework. Applied to the Adler function and gluon condensate example, using lattice data for the gradient-flowed gluon condensate.

Result: Significantly improved convergence of perturbative expansions for the Adler function, reduced theoretical uncertainty at low $Q^2$, and resolution of the fixed-order vs. contour-improved discrepancy in tau lepton decay width calculations.

Conclusion: The gradient flow method provides a robust framework to unify perturbative and non-perturbative QCD effects, enhancing precision in low-energy observables and resolving longstanding theoretical inconsistencies.

Abstract: A long-standing problem concerns the question how to consistently combine
perturbative expansions in QCD with power corrections in the context of the
operator product expansion (OPE), since the former exhibit ambiguities due to
infrared renormalons, which are of the same order as the power corrections. We
propose to use the gradient flow time $1/\sqrt{t}$ as a factorization scale and
to express the OPE in terms of IR renormalon-free subtracted perturbative
expansions and unambiguous matrix elements of gradient-flow regularized local
operators. We show on the example of the Adler function and its leading power
correction from the gluon condensate that this method dramatically improves the
convergence of the perturbative expansion. We employ lattice data on the action
density to estimate the gradient-flowed gluon condensate, and obtain the Adler
function with non-perturbative accuracy and significantly reduced theoretical
uncertainty, enlarging the predictivity at low $Q^2$. When applied to the
hadronic decay width of the tau lepton, the method resolves the long-standing
discrepancy between the fixed-order and contour-improved approach in favour of
the fixed-order treatment.

</details>


### [39] [An improved perturbative QCD study of the decays $B_c^+ \to η_c L^+$](https://arxiv.org/abs/2510.12216)
*Wen-Jing Zhang,Xin Liu*

Main category: hep-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We perform an improved perturbative QCD study of the decays $B_c^+ \to \eta_c
L^+$ ($L$ denotes the light ground state pseudoscalar, vector mesons and the
corresponding $p$-wave scalar, axial-vector, and tensor ones) and predict their
branching ratios (BRs) associated with relative ratios at leading order in the
strong coupling $\alpha_s$. Our results ${\rm BR}(B_c^+ \to \eta_c \pi^+)
=(2.03^{+0.53}_{-0.41}) \times 10^{-3}$ and ${\rm BR}(B_c^+ \to \eta_c
\pi^+)/{\rm BR}(B_c^+ \to J/\psi \pi^+) = 1.74^{+0.66}_{-0.50}$ are consistent
with several available predictions in different approaches within
uncertainties. Inputting the measured $\eta_c \to p\bar p$ and $\eta_c \to
\pi^+\pi^- (\pi^+\pi^-, K^+ K^-, p\bar p)$ BRs with $p$ here being a proton, we
derive the multibody $B_c^+ \to \eta_c (\pi, \rho)^+$ BRs through secondary
decay chains via resonance $\eta_c$ under the narrow-width approximation, which
might facilitate the (near) future tests of $B_c \to \eta_c$ decays. Under the
$q\bar q$ assignment of light scalars, different to $B_c$ decaying into
$J/\psi$ plus a scalar meson and other $B_c^+ \to \eta_c L^+$ modes,
surprisingly small BRs around ${\cal O}(10^{-7}-10^{-9})$ of $\Delta S =0$
modes and highly large ratios near ${\cal O}(10^{2})$ between the BRs of
$\Delta S=1$ and $\Delta S=0$ modes are found in the $B_c$ decays to $\eta_c$
plus light scalars. Many large BRs and interesting ratios presented in this
work could be tested at the LHC experiments, which would help us to examine the
reliability of this improved perturbative QCD formalism for $B_c$-meson decays
and further understand the QCD dynamics in the considered decay modes, as well
as in the related hadrons.

</details>


### [40] [Constraining Heavy Neutral Leptons Coupled to the Tau-Neutrino Flavor at the Large Hadron Collider](https://arxiv.org/abs/2510.12248)
*Edis D. Tireli,Rikke S. Klausen,Oleg Ruchayskiy*

Main category: hep-ph

TL;DR: The paper proposes extending LHC displaced vertex searches to probe heavy neutral leptons (HNLs) predominantly coupling to tau-neutrino flavor by analyzing events without a prompt lepton, using optimized di-lepton mass and DV position cuts to significantly improve sensitivity.


<details>
  <summary>Details</summary>
Motivation: To explore HNLs as candidates for new physics beyond the Standard Model, addressing neutrino mass origins and sensitivity gaps in current searches which require a prompt lepton. This study aims to enhance discovery potential for HNLs coupling to tau-neutrinos.

Method: Monte Carlo simulations of $pp \to W \to \tau N$ with HNL decaying to $\mu^+\mu^-$ or $e^+e^-$, applying ATLAS-style selection criteria. Cuts on di-lepton invariant mass and displaced vertex radial position were optimized to maximize signal sensitivity.

Result: Proposed strategies using Run 2 data improve current HNL bounds by over an order of magnitude, with future runs potentially enhancing sensitivity by up to three orders of magnitude compared to existing limits.

Conclusion: Existing displaced vertex searches can be significantly upgraded by considering prompt-lepton-free events, offering a powerful avenue to probe tau-coupled HNLs and advance beyond-Standard-Model physics.

Abstract: Displaced vertex (DV) signatures at colliders offer a powerful probe of new
long-lived particles beyond the Standard Model. Among the best-motivated
candidates are heavy neutral leptons (HNLs) - heavier counterparts of Standard
Model neutrinos - which can account for the origin of neutrino masses and
potentially produce di-leptonic DV signatures.
  In this study, we demonstrate how existing DV searches at the LHC can be
extended to probe HNLs that couple predominantly to the tau-neutrino flavor.
While current search strategies rely on identifying a prompt lepton alongside a
displaced vertex, we show that analyzing events without a prompt lepton enables
sensitivity to the process $pp \to W \to \tau N$, where the tau decays
hadronically and the HNL subsequently decays to a lepton pair and a neutrino.
  We perform detailed Monte Carlo simulations of this process with HNLs
decaying to $\mu^+\mu^-$ or $e^+e^-$ final states, apply ATLAS-inspired
selection criteria, and optimize signal sensitivity. In particular, we
demonstrate that appropriate cuts in the plane of di-lepton invariant mass and
DV radial position significantly enhance signal visibility. We propose several
such optimized strategies and show that even with Run 2 data
$139~\text{fb}^{-1}$ , existing bounds can be improved by more than an order of
magnitude. Future high-luminosity runs may strengthen sensitivity by up to
three orders of magnitude compared to current limits.

</details>


### [41] [Search for the $D^{*}\bar{D}^{*}$ Molecular State $X_{2}(4013)$ in $K^{-}p$ and $pp$ Collisions](https://arxiv.org/abs/2510.12292)
*Min Yuan,Yin Huang*

Main category: hep-ph

TL;DR: The study investigates the production mechanisms of the hypothetical spin-2 particle $X_2(4013)$ in specific hadronic reactions, predicting its detectability at experiments like AMBER@CERN and LHCb with cross-sections reaching picobarn levels. Initial-state interactions significantly enhance production rates.


<details>
  <summary>Details</summary>
Motivation: Driven by the interpretation of $X(3872)$ as a $D\bar{D}^*$ molecular state, heavy-quark spin symmetry suggests the existence of a spin-2 partner $X_2(4013)$ ($J^{PC}=2^{++}$) as a $D^*\bar{D}^*$ molecule, whose experimental confirmation is still lacking.

Method: The production of $X_2(4013)$ was modeled in $K^-p \to \Lambda_c^+ D_s^- X_2$ and $pp \to \Lambda_c^+\Lambda_c^+ X_2$ reactions using an effective Lagrangian approach with $t$-channel $D/\bar{D}^*$ meson exchanges, incorporating initial-state interactions via Pomeron/Reggeon exchanges.

Result: Calculated total cross sections reach picobarn levels, enabled by ISI which boost results by ~10x. Angular distributions differ between reactions: forward-peaked in $K^-p$ vs. central-angle dip in $pp$.

Conclusion: The study offers a theoretical framework for experimental detection of $X_2(4013)$ and underscores the critical role of ISI in analyzing high-energy particle production processes.

Abstract: Motivated by the interpretation of $X(3872)$ as a $D\bar{D}^{*}$ molecular
state, heavy-quark spin symmetry predicts a spin-2 partner, $X_{2}(4013)$,
which can be regarded as a $D^{*}\bar{D}^{*}$ molecule with quantum numbers
$J^{PC} = 2^{++}$. Its experimental confirmation, however, remains elusive. In
this work, we investigate the production mechanisms of $X_{2}(4013)$ in the
reactions $K^{-}p \to \Lambda_{c}^{+} D_{s}^{-} X_{2}(4013)$ and $pp \to
\Lambda_{c}^{+}\Lambda_{c}^{+} X_{2}(4013)$ within an effective Lagrangian
framework. The production processes are modeled via $t$-channel $D/\bar{D}^{*}$
meson exchanges, while initial-state interactions (ISI) mediated by Pomeron and
Reggeon exchanges are also taken into account. Our calculations indicate that
the total cross sections can reach the pb level, suggesting that $X_{2}(4013)$
may be accessible at current and future experiments such as AMBER@CERN and
LHCb. Inclusion of ISI enhances the cross sections by nearly one order of
magnitude. The differential distributions show distinct angular behaviors for
the two reactions: the $K^{-}p$ reaction exhibits a forward-peaked
distribution, whereas the $pp$ reaction shows a dip near central angles. This
study provides a quantitative theoretical benchmark for future experimental
searches of $X_2(4013)$ and highlights the importance of initial-state
interactions (ISI) in high-energy particle investigations.

</details>


### [42] [Constraining neutrino electromagnetic properties with recent low-energy electron recoil data at dark matter direct detection experiments](https://arxiv.org/abs/2510.12449)
*M. Demirci,H. I. Sezer,M. F. Mustamin,A. B. Balantekin*

Main category: hep-ph

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Neutrinos that are elastically scattered off atomic electrons provide a
unique opportunity to investigate the Standard Model (SM) and beyond SM
physics. In this work, we explore the new physics effects of neutrino
electromagnetic properties through elastic neutrino-electron scattering using
solar neutrinos at the low energy range of PandaX-4T and XENONnT experiments.
The properties of interest include the neutrino magnetic moment, millicharge,
and charge radius, all of which are natural consequences of non-zero neutrino
masses. We investigate their effects by incorporating each property into the
Standard Model (SM) framework, given the measured the solar neutrino flux. By
analyzing the latest Run 0 and Run 1 datasets from the PandaX-4T experiment,
together with recent results from XENONnT, we derive new constraints on each
electromagnetic property of neutrino. We present both flavor-independent
results, obtained using a common parameter for all three neutrino flavors, and
flavor-dependent results, derived by marginalizing over the three neutrino
flavor components. Bounds we obtained are comparable or improved compared to
those reported in the previous studies.

</details>


### [43] [Initial-state geometry and multiplicity distributions in pPb collisions](https://arxiv.org/abs/2510.12561)
*R. Terra,A. V. Giannini,F. S. Navarra*

Main category: hep-ph

TL;DR: This paper investigates the initial geometric shape of protons in proton-nucleus collisions at the LHC, focusing on the baryon junction configuration (Y-shaped gluon strings linking three quarks). Using a Monte Carlo event generator with k_T-factorization and KLN gluon distributions, the study finds that hard-sphere and Gaussian proton models fail to describe pPb collision data, while the baryon junction model succeeds when incorporating intrinsic fluctuations in the saturation scale.


<details>
  <summary>Details</summary>
Motivation: To confirm the existence of the baryon junction configuration by testing its ability to describe proton-nucleus collision data, as prior studies in pp collisions were inconclusive and further validation is needed before electron-ion collider experiments.

Method: Monte Carlo simulations using k_T-factorization framework with CGC and KLN gluon distributions; comparing multiplicity distributions from three proton geometries (hard-sphere, Gaussian, baryon junction) against pPb data.

Result: Hard-sphere and Gaussian models are incompatible with data. Baryon junction model matches data only when accounting for saturation scale fluctuations.

Conclusion: The baryon junction configuration is viable for describing pPb collisions when including saturation scale fluctuations, supporting its role in proton structure but requiring further confirmation from future experiments.

Abstract: This work investigates the possibility of accessing the initial geometric
shape of the proton in proton-nucleus collisions at the LHC, in particular the
configuration in which the proton is made of three quarks linked by a Y-shape
gluon string, called baryon junction. This initial state spatial configuration
has been used in the past to describe data on baryon rapidity distributions,
diffractive $J/\psi$ production and multiplicity distributions in pp
collisions. In spite of its success in explaining the data, the evidence of the
baryon junction still needs confirmation. Further studies will be undertaken at
the electron-ion collider. In this work we study multiplicity distributions
measured in pPb collisions. Different initial state geometries are used as
input in a Monte Carlo event generator which implements the $k_T$-factorization
formalism of the CGC with KLN unintegrated gluon distributions. The results
indicate that the hard-sphere and Gaussian proton configurations are
incompatible with the data. In contrast, the baryon junction configuration can
describe the data provided that intrinsic fluctuations of the saturation scale
are included.

</details>


### [44] [Complementarity of gravitational wave analyses and di-Higgs production in the exploration of the Electroweak Phase Transition dynamics in the RxSM](https://arxiv.org/abs/2510.12569)
*Johannes Braathen,Sven Heinemeyer,Carlos Pulido Boatella,Alain Verduras Schaeidt*

Main category: hep-ph

TL;DR: The RxSM model can produce a strong first-order electroweak phase transition (SFOEWPT) via single- or two-step processes. Different experimental probes (gravitational waves, di-Higgs collider searches) are needed to distinguish scenarios where the phase transition is driven by the Higgs doublet (showing collider signals) versus the singlet field (producing strong GW signals).


<details>
  <summary>Details</summary>
Motivation: To explore the RxSM's capability to generate an SFOEWPT and identify experimental signatures distinguishing different phase transition mechanisms, thereby revealing the true Higgs potential and validating beyond-Standard Model physics.

Method: Analyzed thermal histories of RxSM, computed gravitational wave signals and di-Higgs production with one-loop corrections. Compared parameter space regions producing detectable signals for Higgs-direction vs. singlet-driven transitions.

Result: Higgs-driven transitions lead to measurable collider signals (trilinear couplings) but narrow GW regions. Singlet-driven cases produce SM-like Higgs but strong GW signals, accessible for large parameter spaces. Complementary experiments are critical for full characterization.

Conclusion: RxSM requires dual approaches (collider/detectors) to uncover phase transition dynamics and Higgs potential shape. The singlet field's role offers new observational opportunities distinct from Higgs interactions.

Abstract: The real singlet extension of the Standard Model (SM), RxSM, is one of the
simplest Beyond-the-Standard Model (BSM) theories that can accommodate a strong
first-order electroweak phase transition (SFOEWPT). We survey the possible
thermal histories of the early Universe in the RxSM, and find that a SFOEWPT
can occur in this model as single- or two-step phase transitions. We
investigate complementary approaches to probe such scenarios experimentally:
either via searches for a stochastic background of gravitational waves (GWs) or
via searches for di-Higgs production processes at future collider experiments:
the HL-LHC, or a possible high-energy $e^+e^-$ collider. For these analyses we
consistently include one-loop corrections to the trilinear Higgs couplings. We
find that entirely different phenomenological signals are possible, depending
on how the SFOEWPT occurs. In scenarios where such a transition is driven by
the Higgs doublet direction in field space, BSM deviations in properties of the
detected Higgs boson, particularly in the trilinear scalar coupling, typically
lead to observable signals at colliders, while the regions of parameter space
with detectable GW signals are very narrow. On the other hand, if the SFOEWPT
is triggered by the singlet field direction, the detected Higgs boson is very
SM-like and no signs of BSM physics would appear in di-Higgs production
processes. However, strong GW signals could be produced for significant parts
of the RxSM parameter space with singlet-driven SFOEWPT. This work highlights
the crucial importance of exploiting complementary experimental directions to
determine the dynamics of the electroweak phase transition and access the shape
of the Higgs potential realised in Nature.

</details>


### [45] [High-energy photons from Gamma-Ray Bursts, but no neutrinos](https://arxiv.org/abs/2510.12634)
*A. De Rújula*

Main category: hep-ph

TL;DR: The paper discusses the success of the Cannon-Ball model in explaining high-energy gamma rays from bright Gamma-Ray Bursts (GRBs) and addresses the challenges in detecting their associated neutrinos.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the capability of the Cannon-Ball (CB) model in accounting for TeV gamma-ray observations in some GRBs and to explain why their neutrinos are so hard to detect.

Method: Analysis within the established framework of the CB model, examining the production mechanisms of high-energy gamma rays and the theoretical predictions for neutrino generation and detection challenges.

Result: The CB model effectively explains the origin and properties of observed TeV gamma rays from bright GRBs and highlights the extreme observational difficulties posed for detecting their corresponding neutrinos due to their lower interaction cross-sections and predicted low flux levels.

Conclusion: The CB model remains robust in explaining both the observed phenomena and the observational challenges, reinforcing its status as a valid theoretical framework for GRB studies while underscoring the need for advanced detection technologies to observe neutrinos.

Abstract: The Cannon-Ball model of Gamma-Ray Bursts and their afterglows--described in
the text and in innumerable previous occasions--is extremely successful and
predictive. In a few intrinsically bright GRBs, gamma-rays with energies in the
TeV range have been observed. The CB model, I argue, has no difficulty in
describing the origin and approximate properties of these high-energy gamma
rays and the extreme difficulty of observing their accompanying neutrinos.

</details>


### [46] [Dark Matter Boosted by Terrestrial Collisions](https://arxiv.org/abs/2510.12791)
*Zamiul Alam,Christopher V. Cappiello,Francesc Ferrer*

Main category: hep-ph

TL;DR: Inelastic dark matter (IDM) models, which require upscattering before interaction, enhance detector sensitivity by considering excitation in Earth and downscattering in detectors, extending XENON100/XENON1T limits to larger mass splittings.


<details>
  <summary>Details</summary>
Motivation: To address IDM's consistency with direct detection experiments' limits by exploring processes that improve sensitivity despite energy thresholds.

Method: Model excitation of IDM in Earth followed by downscattering in detectors, analyzing XENON100 and XENON1T data to assess extended mass splitting constraints.

Result: Existing detector sensitivities are significantly improved; XENON data now constrain larger mass splittings than previously thought.

Conclusion: IDM models' viability is bolstered by Earth-mediated upscattering, showing existing detectors can probe broader parameter spaces without experimental upgrades.

Abstract: Inelastic dark matter (IDM) models feature an energy threshold for scattering
with Standard Model particles, which enables their consistency with the
increasingly stringent limits placed by direct detection experiments. In a
typical construction, elastic scattering is absent at tree level, and a lighter
dark matter state must first upscatter into a heavier state in order to
interact with the nuclei in the detector. We model the excitation of IDM in the
Earth followed by its downscattering inside a detector, and we show that
considering this process markedly enhances the sensitivity of existing
detectors. In particular, current limits based on XENON100 and XENON1T data can
be extended to significantly larger mass splittings.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [47] [MESA-QUEST: Tracing the formation of direct collapse black hole seeds via quasi-stars](https://arxiv.org/abs/2510.11772)
*Andrew D. Santarelli,Claire B. Campbell,Ebraheem Farag,Earl P. Bellinger,Priyamvada Natarajan,Matthew E. Caplan*

Main category: astro-ph.HE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The origin of the first supermassive black holes (SMBHs) observed at
redshifts $z\geq 9$ remains one of the most challenging open questions in
astrophysics. Their rapid emergence suggests that massive ``heavy seeds'' must
have formed early, possibly through the direct collapse of pristine gas clouds
in the first galaxies. We present MESA-QUEST, a new framework built upon the
Modules for Experiments in Stellar Astrophysics (MESA) code, designed to model
the structure and evolution of quasi-stars -- massive, radiation-supported
envelopes hosting accreting black holes at their cores -- believed to be the
progenitors of direct-collapse black hole (DCBH) seeds. Our implementation
introduces flexible boundary conditions representing both Bondi accretion and
saturated-convection regimes, and explores the impact of several stellar wind
and mass-loss prescriptions, including Reimers, Dutch, and super-Eddington
radiation-driven winds. We find that quasi-stars can grow central black holes
to $\geq 10^3\,M_{\odot}$ under favorable conditions, with saturated-convection
models yielding BH-to-total mass ratios up to 0.55$M_*$ -- five times higher
than Bondi-limited cases. However, strong radiation-driven winds can
dramatically curtail growth, potentially quenching heavy-seed formation unless
balanced by sustained envelope accretion. Our results delineate the physical
limits under which quasi-stars can remain stable and produce heavy seeds
capable of evolving into the earliest SMBHs detected by JWST and Chandra.
Future extensions will incorporate rotation, magnetic fields, and GR-radiation
hydrodynamics to refine accretion physics and constrain the viability of the
quasi-star pathway for early SMBH formation.

</details>


### [48] [Violent mergers revisited: The origin of the fastest stars in the Galaxy](https://arxiv.org/abs/2510.11781)
*Rüdiger Pakmor,Ken J. Shen,Aakash Bhat,Abinaya Swaruba Rajamuthukumar,Christine E. Collins,Cillian O'Donnell,Evan B. Bauer,Fionntan P. Callan,Friedrich K. Röpke,Joshua M. Pollin,Kate Maguire,Lindsey A. Kwok,Ravi Seth,Stefan Taubenberger,Stephen Justham*

Main category: astro-ph.HE

TL;DR: Violent mergers of carbon-oxygen white dwarfs can explain the origin of hyper-velocity white dwarfs and suggest a larger population of mergers that produce normal Type Ia supernovae through quadruple detonation, leaving no remnant.


<details>
  <summary>Details</summary>
Motivation: To reassess the contribution of violent mergers to Type Ia supernovae and hyper-velocity white dwarfs, addressing previous shortcomings in detonation modeling.

Method: Simulated violent merger events with improved detonation physics, tracking core survival, ejected material, and post-merger outcomes.

Result: The merger produces a ~0.16 solar mass hyper-velocity CO white dwarf at 2800 km/s and ejects ~0.005 solar masses of helium, carbon, and oxygen at high velocities. The surviving core indicates such mergers could explain fast-moving WDs.

Conclusion: Violent mergers not only account for hyper-velocity white dwarfs but imply a larger population of slightly less massive primary WD mergers that can produce normal Type Ia supernovae via quadruple detonation without remnant cores.

Abstract: Binary systems of two carbon-oxygen white dwarfs are one of the most
promising candidates for the progenitor systems of Type Ia supernovae.
  Violent mergers, where the primary white dwarf ignites when the secondary
white dwarf smashes onto it while being disrupted on its last orbit, were the
first proposed double degenerate merger scenario that ignites dynamically.
  However, violent mergers likely contribute only a few per cent to the total
Type Ia supernova rate and do not yield normal Type Ia supernova light curves.
  Here we revisit the scenario, simulating a violent merger with better
methods, and in particular a more accurate treatment of the detonation.
  We find good agreement with previous simulations, with one critical
difference. The secondary white dwarf, being disrupted and accelerated towards
the primary white dwarf, and impacted by its explosion, does not fully burn.
Its core survives as a bound object.
  The explosion leaves behind a $0.16\,\mathrm{M_\odot}$ carbon-oxygen white
dwarf travelling $2800\,\mathrm{km/s}$, making it an excellent (and so far the
only) candidate to explain the origin of the fastest observed hyper-velocity
white dwarfs.
  We also show that before the explosion, $5\times10^{-3}\,\mathrm{M_\odot}$ of
material consisting predominantly of helium, carbon, and oxygen has already
been ejected at velocities above $1000\,\mathrm{km/s}$.
  Finally, we argue that if a violent merger made D6-1 and D6-3, and violent
mergers require the most massive primary white dwarfs in binaries of two
carbon-oxygen white dwarfs, there has to be a much larger population of white
dwarf mergers with slightly lower-mass primary white dwarfs. Because of its
size, this population can essentially only give rise to normal Type Ia
supernovae, likely exploding via the quadruple detonation channel and leaving
no bound object behind.

</details>


### [49] [Energy calibration of LHAASO-KM2A using the cosmic ray Moon shadow](https://arxiv.org/abs/2510.12649)
*Ali Raza,Huihai He,Faisal Akram,The LHAASO Collaboration*

Main category: astro-ph.HE

TL;DR: The study uses lunar shadow data from LHAASO/KM2A to calibrate cosmic-ray detector response between 20-260 TeV, showing strong agreement with simulations and determining an energy-scale parameter within tight confidence intervals.


<details>
  <summary>Details</summary>
Motivation: To calibrate the KM2A detector's energy response and validate its Monte Carlo simulations through precise measurement of cosmic-ray variations caused by the Moon's shadow.

Method: Analysed 3.5 years of KM2A data to measure the westward, rigidity-dependent shift of the Moon's shadow, comparing observed detector responses with Monte Carlo simulations.

Result: Achieved excellent agreement between measured and simulated detector responses; determined best-fit parameter epsilon=0.015±0.08 (95% CI [-14%,+17%]), confirming high simulation accuracy.

Conclusion: The KM2A-MC simulation accurately models detector behavior in 20-260 TeV range, ensuring reliable energy calibration for future cosmic-ray studies.

Abstract: We present a precise measurement of the westward, rigidity-dependent shift of
the Moon's shadow using three and a half years of cosmic-ray data collected by
the Kilometer Square Array (KM2A) of the Large High Altitude Air Shower
Observatory (LHAASO). These measurements enable us to calibrate the detector
energy response in the range 20-260 TeV, with results showing excellent
agreement with the response derived from Monte Carlo (MC) simulations of the
KM2A detector. We also measure a best-fit parameter $\epsilon = 0.015 \pm
0.08$, corresponding to a 95% confidence interval of [-14%, +17%] for the
energy-scale estimation. This result establishes the exceptional accuracy of
the KM2A-MC in simulating the detector's response within this energy range.

</details>


### [50] [Eruptive mass loss less than a year before the explosion of superluminous supernovae. II. A systematic search for pre-explosion eruptions with VLT/X-shooter](https://arxiv.org/abs/2510.11799)
*A. Gkini,C. Fransson,R. Lunnan,S. Schulze,J. Sollerman K. Tsalapatas,N. Sarin,M. Nicholl,C. Angus,U. Burgaz,S. J. Brennan,T. -W. Chen,A. Gal-Yam,A. Gangopadhyay,Y. Hu,M. M. Kasliwal,R. R. Laher,P. J. Pessi,B. Rusholme,E. Russeil,A. Singh,C. Skoglund,R. Smith,B. van Baal,S. L. West,L. Yan*

Main category: astro-ph.HE

TL;DR: This study uses X-shooter observations of 21 hydrogen-poor superluminous supernovae to search for circumstellar material (CSM) via broad Mg II absorption features. Five SLSNe-I show evidence of CSM shells, including two new discoveries. These shells were ejected 2-3 months prior to explosion, suggesting late-stage mass loss. No strong correlations with SN properties are found except a marginal link between light curve decline rate and shell velocity.


<details>
  <summary>Details</summary>
Motivation: The goal is to understand the progenitor environments of SLSNe-I by detecting CSM through Mg II absorption features, which indicate pre-explosion mass ejection events.

Method: High-quality X-shooter spectra of 21 SLSNe-I at z=0.13-0.95 were analyzed. The region around 2800A was modeled to detect broad blueshifted Mg II absorption indicative of CSM. Statistical constraints were applied for non-detections.

Result: Five SLSNe-I (including two new: DES15S2nr and DES16C3ggu) showed CSM shells. The shells' ejection times (2-3 months prior) align with late-stage mass loss. No significant correlations with SN properties except marginal velocity-decline rate link. Detection feasibility analysis suggests findings aren't due to observational bias.

Conclusion: Detection of CSM shells isn't due to selection effects but points to a subclass of SLSNe-I experiencing late-stage shell ejections before explosion, affecting their environments and energetics.

Abstract: We present X-shooter observations of a sample of 21 hydrogen-poor
superluminous supernovae (SLSNe-I), spanning a redshift range of z=0.13-0.95,
aimed at searching for shells of circumstellar material (CSM). Specifically, we
focus on identifying broad Mg II absorption features that are blueshifted by
several thousand kilometers per second and have previously been interpreted as
arising from resonance line scattering of the SLSN continuum by rapidly
expanding CSM ejected shortly before explosion. Utilizing high-quality spectra,
we model the region around 2800A to characterize the Mg II line profiles,
enabling us to either confirm their presence or place constraints on undetected
CSM shells. We identify five objects in our sample that show broad Mg II
absorption features consistent with the presence of CSM. While SN2018ibb,
SN2020xga and SN2022xgc have been previously reported, we identify previously
undiscovered CSM shells in DES15S2nr and DES16C3ggu. These shells were likely
expelled approximately two and three months, respectively, before the explosion
of their associated SNe, timescales consistent with late-stage mass-loss
episodes. We do not find any correlations between the shell properties and the
SN properties, except for a marginal correlation between the light curve
decline time scale and the shell velocities. We further demonstrate that CSM
configurations similar to the majority of the detected shells would have been
observable in spectra with signal-to-noise >8 per resolution element, and that
the lines from a shell are in general detectable except in the cases where the
shell is either very geometrically and/or optically thin. Therefore, we
conclude that the detection of CSM shells is not a selection effect, but may
instead point to the existence of a subclass of SLSNe-I undergoing late-stage
shell ejections shortly before explosion.

</details>


### [51] [Ultraviolet radiation and neutrinos: two messengers from CCSNe in the CSM scenario](https://arxiv.org/abs/2510.11916)
*Silvia Gagliardini,Simone Dall'Osso,Dafne Guetta,Angela Zegarelli,Silvia Celli,Antonio Capone,Irene Di Palma*

Main category: astro-ph.HE

TL;DR: This paper investigates the impact of massive star mass loss and circumstellar medium (CSM) interaction on supernova (SN) emissions and explores detection capabilities of future UV missions (e.g., ULTRASAT) and neutrino detectors (e.g., IceCube, KM3NeT) for such events.


<details>
  <summary>Details</summary>
Motivation: Understanding how pre-supernova mass loss shapes SN environments and their observable signatures, especially in UV/optical bands. Recent transient observations (SN 2023ixf, SN 2024ggi, FBOTs) suggest dense CSM influences emission properties and necessitates improved detection strategies.

Method: Modeling SN-CSM shock interactions to calculate UV detection horizons for upcoming missions and neutrino signal strengths for IceCube/KM3NeT. Analyzing expected event rates and multi-messenger observational prospects.

Result: Future UV telescopes like ULTRASAT could detect SN-CSM interactions up to hundreds of Mpc, while neutrino detectors might observe rare but energetic events. Multi-messenger studies combining UV and neutrino data will enhance understanding of CSM dynamics and progenitor systems.

Conclusion: Integrating advanced UV and neutrino observations will revolutionize insights into massive star evolution, CSM environments, and extreme astrophysical phenomena like FBOTs, emphasizing the need for coordinated multi-messenger strategies.

Abstract: Massive stars (>8 $M_{\odot}$) often undergo intense mass loss through winds
or eruptive events in the final stages of their evolution, leading to the
formation of a dense circumstellar medium (CSM). This material, expelled months
to years before core collapse, shapes the pre-explosion environment and
influences the early supernova (SN) emission. In particular, the interaction of
the SN ejecta with the dense CSM can power an extended emission into the
UV/optical bands, as seen in a growing fraction of type II SN. Recent events
such as SN 2023ixf and SN 2024ggi confirm the relevance of dense environments
and highlight the value of UV observations. Moreover, Fast Blue Optical
Transients (FBOTs) may represent extreme cases of this interaction, possibly
linked to more compact/massive CSM. In this work, we model the SN-CSM shock
interaction in order to (i) estimate the maximum detection horizons and
expected rates for future UV missions like ULTRASAT, and (ii) to estimate the
intensity and expected rate of potential neutrino signals detectable by IceCube
and KM3NeT. We then discuss the prospects for multi-messenger observations of
such events in the near future.

</details>


### [52] [Rotation of Polarization Angle in Gamma-Ray Burst Prompt Phase. III. The Influence of the Magnetic Field Orientation](https://arxiv.org/abs/2510.11971)
*Xing-Yao Wang,Jia-Sheng Li,Mi-Xiang Lan*

Main category: astro-ph.HE

TL;DR: The paper examines how magnetic field orientation affects polarization angle (PA) rotation in a reconnection model, finding that PA rotations are largely independent of field orientation except for specific angles where rotational diversity decreases.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand the sensitivity of polarization angle rotation to magnetic field configurations, extending prior research by exploring parameter variations under different field orientations.

Method: A parametric magnetic reconnection model with an aligned large-scale magnetic field is used. The impact of field orientation angles (δ) between 0° and 90° on PA rotation spectra and parameter space is analyzed.

Result: PA rotation outcomes remain consistent across most orientations except near δ=0°/90°, where parameter spaces allowing >10° ΔPA shrink. Complementary orientations produce identical ΔPA values.

Conclusion: Magnetic field orientation significantly influences PA rotation scope, with δ biases reducing rotational diversity. Key findings underscore the importance of orientation in interpreting observational PA data.

Abstract: Polarization is very sensitive to the configuration of the magnetic field in
the radiation region. In addition to polarization curve and polarization
spectrum, studies of polarization angle (PA) rotation spectrum is also crucial.
In this paper, we use a simple parametric magnetic reconnection model with a
large-scale aligned magnetic field in the radiation region to study the effects
of field orientation on the PA rotations. Under different field orientations,
variations of the PA rotation with parameters and the PA rotation spectra are
studied. We find that the conclusions obtained in our previous works are almost
independent of the field orientations. The area of the parameter space with
$\Delta$PA $>10^\circ$ will shrink as the value of field orientation ($\delta$)
increases for $0^\circ<\delta<90^\circ$. The $\Delta$PA values would be the
same for two complementary field orientations. For two particular magnetic
field orientations ($\delta=0^\circ$ and $90^\circ$), the $\Delta$PA would also
only be $0^\circ$ or $90^\circ$ within the burst duration.

</details>


### [53] [Impact of Cosmic Ray Acceleration on the Early Evolution of Bow Shocks around Massive Runaway Stars](https://arxiv.org/abs/2510.11988)
*Keito Watanabe,Stefanie Walch,Tim-Eric Rathjen,Jonathan Mackey,Pierre Nürnberger,Philipp Girichidis*

Main category: astro-ph.HE

TL;DR: This paper investigates how cosmic ray (CR) acceleration affects the morphology and non-thermal emission of bow shocks created by massive runaway stars interacting with the interstellar medium. Using 3D simulations with variable CR diffusion rates, they show CR diffusion strongly influences shock structure and emission levels, aligning with observational data when efficient forward shock acceleration is assumed.


<details>
  <summary>Details</summary>
Motivation: Recent observations of gamma-ray and radio synchrotron emissions from bow shocks highlight their role as particle accelerators. The study aims to quantitatively explore how CR acceleration dynamics influence bow shock evolution and emission properties under varying physical conditions.

Method: 3D ideal cosmic ray magnetohydrodynamic simulations using FLASH code with Eulerian grids. Parameters include variable CR diffusion coefficients, star velocities, and ISM-like environments. A gradient-based shock detection algorithm dynamically injects CRs. Post-processing uses a simplified spectral model to compute gamma-ray and radio synchrotron emission limits.

Result: Varying CR diffusion coefficients significantly alter bow shock morphology and non-thermal emission luminosities. Efficient CR injection at forward shocks aligns simulation results with observed emission levels, demonstrating the critical balance between injection efficiency and diffusion effects.

Conclusion: CR acceleration with differing diffusion rates can substantially shape wind-driven bow shock structures and their non-thermal emissions, contingent on efficient particle acceleration at the forward shock. This underscores the importance of CR dynamics in astrophysical shock studies.

Abstract: Bow shocks generated from the interaction of winds from massive runaway stars
with the interstellar medium have been shown to be prominent particle
accelerators through recent $\gamma$-ray and radio synchrotron observations.
Here, we study particle acceleration from bow shocks by conducting 3D ideal
cosmic ray magnetohydrodynamic simulations in the advection-diffusion limit. We
use the Eulerian grid-based code FLASH, where stellar winds are injected
through tabulated wind velocities and mass loss rates. We implement a
gradient-based shock detection algorithm to resolve the shocked regions where
the CRs are injected dynamically. Simulations are performed for different
values of the CR diffusion coefficient and star velocities within an ISM-like
environment up to 180 kyr to showcase the impact of dynamical CR injection on
the early evolution of the wind-driven bow shock. With a simplified spectral
model in post-processing, we calculate the expected upper limits of
$\gamma$-ray and synchrotron emission and compare with those from current
observations. We observe that variations of CR diffusion rates can strongly
dictate the morphology of the bow shock and the overall $\gamma$-ray and radio
synchrotron luminosity due to the balance between the CR injection efficiency
and diffusion. Our results yield qualitatively comparable results with current
observations, primarily attributed to the high-energy protons and electrons
contributing to non-thermal emission from efficient acceleration at the forward
shock through the approximations and assumptions in the injection algorithm. We
conclude that CR acceleration, with varying CR diffusion rates, may
substantially affect the morphology of wind-driven bow shocks and their
non-thermal emission, if there is efficient particle acceleration in the
forward shock. [abridged]

</details>


### [54] [Fe XVIII-XXIV K beta Inner-shell Absorption Lines in the X-ray Spectra of Neutron Star and Black Hole Binaries with XRISM](https://arxiv.org/abs/2510.12317)
*Masahiro Tsujimoto,Daiki Miura,Hiroya Yamaguchi,Ehud Behar,Chris Done,Maria Diaz Trigo,Chamani M. Gunasekera,Peter A. M. van Hoof,Stefano Bianchi,Maryam Dehghanian,Gary J. Ferland*

Main category: astro-ph.HE

TL;DR: The paper demonstrates the necessity to expand atomic databases by calculating and implementing Fe K beta lines in spectral analysis using XRISM data, achieving good agreement with observations.


<details>
  <summary>Details</summary>
Motivation: The existing atomic databases lack information on Fe K beta lines of mildly ionized iron ions, which were newly detected by XRISM/Resolve. This limitation hinders accurate interpretation of high-resolution astrophysical spectra.

Method: The authors performed atomic structure calculations using the FAC code to compute Fe K alpha and K beta lines. They validated K alpha results against experimental data and existing calculations. They then integrated the K beta lines into the cloudy radiative transfer code and compared synthetic spectra with XRISM observations.

Result: The synthetic spectra generated with the new K beta lines showed reasonable agreement with observed data from XRISM, validating the expanded atomic database approach.

Conclusion: The study underscores the critical need to update atomic databases to include previously missing spectral features like Fe K beta lines for accurate analysis of high-resolution X-ray spectra from astrophysical sources.

Abstract: The advent of the X-ray microcalorimeter spectrometer Resolve onboard the
XRISM space telescope opened a new era for high-resolution X-ray spectroscopy
of astrophysical plasmas. Many spectral features were newly detected, including
the K alpha and K beta inner-shell transition lines of mildly ionized (F- to
Li-like) Fe at 6-8 keV in the spectra of X-ray binaries and active galactic
nuclei. The widely used atomic databases contain information on the K alpha but
not K beta lines of these ions. We conducted the atomic structure calculation
using FAC to derive the Fe K alpha and K beta lines and verified the result
against ground experiments and other calculations of the Fe K alpha lines. We
then implemented the Fe K beta lines in a radiative transfer code (cloudy) and
compared the synthesized and observed spectra with XRISM. A reasonably good
agreement was obtained between the observation and the ab initio calculations.
This exemplifies the need to expand the atomic databases to interpret
astrophysical spectra.

</details>


### [55] [Lanthanide Impact on the Infra-Red Spectra of Nebular Phase Kilonovae](https://arxiv.org/abs/2510.12413)
*Quentin Pognan,Kyohei Kawaguchi,Shinya Wanajo,Sho Fujibayashi,Anders Jerkstrand*

Main category: astro-ph.HE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Nebular phase kilonovae (KNe) have significant infra-red (IR) emission
thought to be mostly forbidden emission lines from rapid neutron capture
(r-process) species in neutron star merger ejecta. Lanthanide elements in
particular have complex atomic structures with many IR transitions. Using
non-local thermodynamic equilibrium (NLTE) radiative transfer simulations, we
explore the impact of lanthanides on the IR spectra of KNe in the nebular
phase, exploring a parameter space of ejecta mass and lanthanide fraction. We
find that lanthanide impact is greater at higher densities, corresponding to
earlier epochs and greater ejecta masses. The wavelengths most affected are
found to be $\lambda \lesssim 4~\mu$m, with the species Ce\,\textsc{iii} and Nd
\textsc{ii} being the most important contributors to spectral formation. We
also find significant emission from species proposed in observations, notably
Te\,\textsc{iii} at 2.1 $\mu$m, and Se\,\textsc{iii} at 4.5 and 5.7 $\mu$m,
while W\,\textsc{iii} is subdominant at 4.5 $\mu$m. The Te\,\textsc{iii}
feature at 2.1 $\mu$m is always blended, particularly with Zr\,\textsc{ii},
Ce\,\textsc{iii}, and Nd\,\textsc{ii}. We do not reproduce the smooth
blackbody-like continua observed in AT2023vfi. Based on our results, we argue
that line opacity alone is likely insufficient to produce optically thick
continua in the nebular phase, even in the case of lanthanide/actinide-rich
ejecta, as our models are optically thin in the IR at these epochs. Given that
lanthanide contributions are dominant below 4 $\mu$m, we suggest that NIR
observations best probe these elements, while MIR spectroscopy with
\textit{JWST} can reliably probe non-lanthanide emission even in relatively
lanthanide-rich cases.

</details>


### [56] [Optically thick winds of very massive stars suppress intermediate-mass black hole formation](https://arxiv.org/abs/2510.12465)
*Stefano Torniamenti,Michela Mapelli,Lumen Boco,Filippo Simonato,Giuliano Iorio,Erika Korb*

Main category: astro-ph.HE

TL;DR: The paper examines how metallicity affects the formation of IMBHs from very massive stars, finding that higher metallicity suppresses IMBH formation due to increased mass loss from optically thick winds, narrowing the possible progenitor metallicity for GW2311's primary BH.


<details>
  <summary>Details</summary>
Motivation: IMBHs bridge stellar-mass and supermassive black holes. Gravitational wave observations have detected IMBHs in the 100-300 solar mass range, necessitating improved understanding of their formation mechanisms from VMSs. Existing models underestimate mass loss at higher metallicities, so the study aims to refine this by including optically thick winds.

Method: New VMS models were calculated considering the transition between optically thin and thick winds, evaluating mass loss effects on IMBH formation and black hole mass function across metallicities Z=1e-4 to 0.02.

Result: Optically thick winds significantly reduce IMBH formation through VMS collapse at Z>0.001, which is ten times lower metallicity than previous estimates. For GW2311's primary BH, progenitor metallicity must be Z<0.002 if formed via direct VMS collapse.

Conclusion: Metallicity plays a critical role in IMBH formation via VMS collapse. Future gravitational wave detections can constrain cosmic chemical evolution and stellar models by comparing observed IMBH populations with these refined metallicity-dependent predictions.

Abstract: Intermediate-mass black holes (IMBHs) are the link between stellar-mass and
supermassive black holes. Gravitational waves have started unveiling a
population of IMBHs in the $\sim 100-300 \, \mathrm{M_{\odot}}$ range. Here, we
investigate the formation of IMBHs from very massive stars (VMSs, $>100\,{}
\mathrm{M_{\odot}}$). We calculate new VMS models that account for the
transition from optically thin to optically thick winds, and study how this
enhanced mass loss affects IMBH formation and the black hole mass function at
intermediate and high metallicity ($Z=10^{-4}-0.02$). We show that optically
thick winds suppress the formation of IMBHs from direct VMS collapse at
metallicities $Z>0.001$, one order of magnitude lower than predicted by
previous models. Our models indicate that the stellar progenitors of GW231123
must have had a metallicity $Z<0.002$, if the primary black hole formed via
direct VMS collapse.

</details>


### [57] [DIPLODOCUS II: Implementation of transport equations and test cases relevant to micro-scale physics of jetted astrophysical sources](https://arxiv.org/abs/2510.12505)
*Christopher N. Everett,Marc Klinger-Plaisier,Garret Cotter*

Main category: astro-ph.HE

TL;DR: The paper introduces the second installment of the DIPLODOCUS framework, presenting its numerical implementation in Diplodocus.jl, a Julia-based code package. It includes a novel Monte-Carlo sampling method for computing anisotropic collision integrals and validates the package through test cases focused on micro-scale phenomena critical to modeling astrophysical jets from sources like Active Galactic Nuclei and X-Ray Binaries.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a robust numerical tool (Diplodocus.jl) capable of accurately modeling the transport of particle distributions across seven-dimensional phase space, particularly addressing complex interactions like anisotropic collisions and external forces relevant to astrophysical jet simulations.

Method: The framework uses a novel Monte-Carlo sampling technique for pre-computing anisotropic collision integrals, integrated within the Julia package Diplodocus.jl. It builds on the analytic methods outlined in the preceding paper, focusing on numerical implementation and validation through specific test cases.

Result: The implementation successfully demonstrates the package's capability to handle binary collisions, emissive interactions, and external forces. Test results validate its applicability for modeling micro-scale processes in jetted astrophysical systems.

Conclusion: Diplodocus.jl provides a versatile and accurate numerical tool for simulating multi-dimensional particle transport in astrophysical contexts, laying groundwork for future studies of jet dynamics in systems like Active Galactic Nuclei and X-Ray Binaries.

Abstract: DIPLODOCUS (Distribution-In-PLateaux methODOlogy for the CompUtation of
transport equationS) is a novel framework being developed for the general
transport of particle distribution functions through the seven dimensions of
phase space, including forcing terms and interactions between particles.
Following Paper I, which details the background analytic framework, this second
paper provides an overview of the numerical implementation in the form of the
code package Diplodocus.jl, written in Julia, including the description of a
novel Monte-Carlo sampling technique for the pre-computation of anisotropic
collision integrals. In addition to the discussion of numerical implementation,
a selection of test cases are presented to examine the package's capabilities.
These test cases focus on micro-scale physical effects: binary collisions,
emissive interactions and external forces that are relevant to the modelling of
jetted astrophysical sources, such as Active Galactic Nuclei and X-Ray
Binaries.

</details>


### [58] [The double neutron star PSR J1946+2052 I. Masses and tests of general relativity](https://arxiv.org/abs/2510.12506)
*Lingqi Meng,Paulo C. C. Freire,Kevin Stovall,Norbert Wex,Xueli Miao,Weiwei Zhu,Michael Kramer,James M. Cordes,Huanchen Hu,Jinchen Jiang,Emilie Parent,Lijing Shao,Ingrid H. Stairs,Mengyao Xue,Adam Brazier,Fernando Camilo,David J. Champion,Shami Chatterjee,Fronefield Crawford,Ziyao Fang,Qiuyang Fu,Yanjun Guo,Jason W. T. Hessels,Maura MacLaughlin,Chenchen Miao,Jiarui Niu,Ziwei Wu,Jumei Yao,Mao Yuan,Youlin Yue,Chengmin Zhang*

Main category: astro-ph.HE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We conducted high-precision timing of PSR J1946+2052 to determine the masses
of the two neutron stars in the system, test general relativity (GR) and
assessed the system's potential for future measurement of the moment of inertia
of the pulsar. We analysed seven years of timing data from the Arecibo 305-m
radio telescope, the Green Bank Telescope (GBT), and the Five-hundred-meter
Aperture Spherical radio Telescope (FAST). The data processing accounted for
dispersion measure variations and relativistic spin precession-induced profile
evolution. We employed both DDFWHE and DDGR binary models to measure the spin
parameters, kinematic parameters and orbital parameters. The timing campaign
has resulted in the precise measurement of five post-Keplerian parameters,
which yield very precise masses for the system and three tests of general
relativity. One of these is the second most precise test of the radiative
properties of gravity to date: the intrinsic orbital decay, $\dot{P}_{\rm
b,int}=-1.8288(16)\times10^{-12}\rm\,s\,s^{-1}$, represents $1.00005(91)$ of
the GR prediction, indicating that the theory has passed this stringent test.
The other two tests, of the Shapiro delay parameters, have precisions of 6\%
and 5\% respectively; this is caused by the moderate orbital inclination of the
system, $\sim 74^{\circ}$; the measurements of the Shapiro delay parameters
also agree with the GR predictions. Additionally, we analysed the higher-order
contributions of $\dot{\omega}$, including the Lense-Thirring contribution.
Both the second post-Newtonian and the Lense-Thirring contributions are larger
than the current uncertainty of $\dot{\omega}$
($\delta\dot{\omega}=4\times10^{-4}\,\rm deg\,yr^{-1}$), leading to the
higher-order correction for the total mass.

</details>


### [59] [Gamma-rays from Wolf-Rayet stellar winds](https://arxiv.org/abs/2510.12510)
*A. Inventar,G. Peron,S. Recchia,S. Gabici*

Main category: astro-ph.HE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Gamma-ray observations of young star clusters have recently provided evidence
for particle acceleration occurring at stellar wind termination shocks, fueled
by the mechanical energy of stellar winds from massive stars. In this work, we
explore the possibility that the wind from a single powerful star, whether
isolated or part of a cluster, can alone provide sufficient energy to generate
gamma-ray emission detectable by current instruments. This scenario is
particularly relevant given that a significant fraction of Wolf-Rayet (WR)
stars are not found within clusters. To investigate this, we compiled a large
sample of WR stars and ranked them based on their wind luminosity divided by
the square of their distance, a proxy for their potential gamma-ray flux. We
then searched for spatial coincidences between the most promising candidates
and cataloged gamma-ray sources. This analysis leads us to propose associations
between the stars WR14, WR110, WR111, and WR114 and several unidentified
gamma-ray sources. These results suggest that WR stellar winds could represent
a distinct and previously unrecognized population of gamma-ray emitters.

</details>


### [60] [Modeling gamma-ray signatures of particle acceleration in stellar clusters from GeV to PeV](https://arxiv.org/abs/2510.12562)
*A. Inventar,S. Gabici,E. Peretti*

Main category: astro-ph.HE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Young massive stellar clusters (YMSCs) have recently regained interest as
PeVatron candidates, potentially accounting for the cosmic-ray (CR) knee as
alternatives to isolated supernova remnants (SNRs). LHAASO's unique capability
to detect photons above 0.1 PeV, hence tracing multi-PeV CRs, can provide
critical constraints on galactic acceleration models when combined with
H.E.S.S. and Fermi-LAT data. We investigate the transport of particles from
YMSCs acceleration sites, namely wind termination shocks (WTS) or embedded
SNRs, to nearby dense molecular clouds where proton-proton interactions produce
high-energy gamma rays. We determine the necessary conditions, such as the
distance between the acceleration site and the target, or the cluster's power
and age, for detectable gamma-ray excesses and identify viable systems through
parameter space exploration. By comparing with observations, we can constrain
key physical parameters including WTS efficiency, diffusion coefficient and
injection slope. Our analysis also examines whether some of LHAASO's
unidentified sources might correspond to such cluster-cloud systems.

</details>


### [61] [JWST and Keck Observations of the Off-Nuclear TDE AT 2024tvd: A Massive Nuclear Star Cluster and Minor-Merger Origin for its Black Hole](https://arxiv.org/abs/2510.12572)
*Kishore C. Patra,Ryan J. Foley,Nicholas Earl,Kyle W. Davis,Enrico Ramirez-Ruiz,V. Ashley Villar,Sebastian Gomez,K. Decker French,Kirsty Taggart,Prasiddha Arunachalam,Phillip Macias,Ravjit Kaur,Samaporn Tinyanont*

Main category: astro-ph.HE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present JWST/NIRSpec and NIRCam observations of the first optically
selected off-nuclear tidal disruption event (TDE), AT 2024tvd, along with
Keck/KCWI integral field unit spectroscopy. The spectra show broad H and He
emission lines that are characteristic of a TDE. Stellar kinematics show smooth
host-galaxy morphology and ordered bulge rotation, with no evidence of
disturbances in velocity, dispersion, age or metallicity space. We construct
the first quasi-simultaneous spectral-energy distribution (SED) from X-rays to
infrared for a TDE and decompose it into three components: the TDE accretion
flow, an unresolved nuclear star cluster (NSC), and heated dust emission. The
accretion component implies a black hole mass of $\log(M_\bullet/M_\odot) =
5.50\pm 0.04$, an instantaneous super-Eddington accretion rate of $\log
(\dot{M}/M_{\odot} yr^{-1}) = -1.22 \pm 0.04$, and an outer disk photosphere
radius of $\log(r_{out}/r_{g}) = 3.8 \pm 0.1$. The dust emission is well
described by a blackbody with $T_{dust} = 873\pm 15$ K and peak luminosity
$\log (L_{dust}/erg$ $s^{-1}) = 40.80\pm 0.01$, consistent with a dust echo
near the sublimation radius. The SED is best fit when including additional
stellar emission above the galaxy background at the TDE location, corresponding
to $\log(M_{\star}/M_\odot) = 7.97^{+0.16}_{-0.26}$, which we interpret as a
massive NSC or an ultra-compact dwarf galaxy. These results support a
minor-merger origin for the MBH responsible for the TDE over scenarios
involving gravitational recoil or dynamical ejection from the nucleus.

</details>


### [62] [The Influence of the Accretion Disc Structure on X-ray Spectral States in Symbiotic Binaries](https://arxiv.org/abs/2510.12654)
*Jesús A. Toalá,Diego A. Vasquez-Torres*

Main category: astro-ph.HE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Symbiotic stars are binary systems where a white dwarf (WD) accretes material
from the wind of an evolved, late-type companion. X-ray-emitting symbiotic
systems are classified into $\alpha$, $\beta$, $\delta$, and $\beta/\delta$
types, attributed to distinct physical mechanisms such as thermonuclear
burning, wind interactions, and accretion-driven boundary layers. We present
synthetic X-ray spectra derived from hydrodynamics simulations using the
PHANTOM code, coupled with radiative-transfer calculations from SKIRT. We
reproduce all X-ray spectral types by exploring different density structure of
the accretion disc, the viewing angle, the plasma temperature of the boundary
layer, and/or the presence of extended emission. The synthetic X-ray spectra
consist of both absorbed and reflected components. In systems with massive,
high-column density discs and viewing angles close to edge-on, the reflected
continuum can dominate the X-ray emission. This effect is less pronounced in
systems with low-mass, lower-column density discs. We explore i) systems going
from $\delta$ to $\beta$ states, ii) $\delta$-types that become $\beta/\delta$
sources, iii) the variability of the three Fe emission lines in the 6.0-7.0
energy range, and iv) the possible physical processes behind the $\alpha$
sources. The observations from iconic symbiotic systems are discussed in line
of the present models. Our framework offers predictive power for future X-ray
monitoring and provides a path toward connecting accretion disc physics with
observed spectral states in symbiotic binaries with accreting WDs.

</details>


### [63] [Tycho supernova exploded inside a planetary nebula (SNIP)](https://arxiv.org/abs/2510.12674)
*Noam Soker*

Main category: astro-ph.HE

TL;DR: The paper presents evidence that the type Ia supernova remnant Tycho is an SNIP (Supernova Ia Inside Planetary Nebula), based on observed 'ear' structures similar to known SNIPs. It concludes that ~70-90% of normal SNe Ia are SNIPs, implying a revised model for their delay time distribution involving younger stellar populations in quiescent galaxies.


<details>
  <summary>Details</summary>
Motivation: To determine the nature of Tycho's SNR and understand the prevalence of SNIPs among SNe Ia, which challenges existing models of their progenitor systems and galactic environments.

Method: Analysis of Tycho's observational data, comparing its 'ear' structures to other SNRs confirmed as SNIPs (Kepler, G299.2-2.9, G1.9+0.3). Also evaluates explosion timing constraints favoring the core-degenerate scenario over other progenitor models.

Result: Tycho is classified as an SNIP, with implications that most normal SNe Ia originate in planetary nebulae, requiring broader stellar population considerations in their delay time distributions.

Conclusion: SNIPs constitute the majority of normal SNe Ia, necessitating revisions to standard models to account for younger stellar populations in galaxies with low star formation rates.

Abstract: I examine recent observations of the type Ia supernova remnant (SNR Ia) Tycho
and conclude that Tycho is an SN Ia inside a planetary nebula (SNIP). The
observations reveal two opposite protrusions, termed ears, projected on the
main shell of Tycho. The pair of ear structures resembles that of the SNRs Ia
Kepler, SNR G299-2.9, and SNR G1.9+0.3, which earlier studies considered as
SNIPs. The requirement that the explosion occurs within hundreds of thousands
of years after the formation of the planetary nebula (by the second star to
evolve) makes the core-degenerate scenario the most likely for Tycho. Several
other possible scenarios lead to an SNIP, but they are unlikely for Tycho. The
identification of Tycho as an SNIP leads to two general conclusions. (1) The
fraction of SNIPs among normal SNe Ia is very large, ~70-90%. Namely, the vast
majority of normal SNe Ia are SNIPs. (2) To accommodate the large fraction of
SNIPs, the delay time distribution of normal SNe Ia includes not only the
stellar evolution timescale (as usually assumed), but also includes pockets of
younger stellar populations in galaxies without ongoing star formation; the
SNIPs come from the younger stellar populations in galaxies.

</details>


### [64] [Mapping the Perseus Galaxy Cluster with XRISM: Gas Kinematic Features and their Implications for Turbulence](https://arxiv.org/abs/2510.12782)
*Congyao Zhang,Irina Zhuravleva,Annie Heinrich,Elena Bellomi,Nhut Truong,John ZuHone,Eugene Churazov,Megan E. Eckart,Yutaka Fujita,Julie Hlavacek-Larrondo,Yuto Ichinohe,Maxim Markevitch,Kyoko Matsushita,François Mernier,Eric D. Miller,Koji Mori,Hiroshi Nakajima,Anna Ogorzalek,Frederick S. Porter,Ayşegül Tümer,Shutaro Ueda,Norbert Werner*

Main category: astro-ph.HE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this paper, we present extended gas kinematic maps of the Perseus cluster
by combining five new XRISM/Resolve pointings observed in 2025 with four
Performance Verification datasets from 2024, totaling 745 ks net exposure. To
date, Perseus remains the only cluster that has been extensively mapped out to
~0.7$r_{2500}$ by XRISM/Resolve, while simultaneously offering sufficient
spatial resolution to resolve gaseous substructures driven by mergers and AGN
feedback. Our observations cover multiple radial directions and a broad
dynamical range, enabling us to characterize the intracluster medium kinematics
up to the scale of ~500 kpc. In the measurements, we detect high velocity
dispersions ($\simeq$300 km/s) in the eastern region of the cluster,
corresponding to a nonthermal pressure fraction of $\simeq$7-13%. The velocity
field outside the AGN-dominant region can be effectively described by a single,
large-scale kinematic driver based on the velocity structure function, which
statistically favors an energy injection scale of at least a few hundred kpc.
The estimated turbulent dissipation energy is comparable to the gravitational
potential energy released by a recent merger, implying a significant role of
turbulent cascade in the merger energy conversion. In the bulk velocity field,
we observe a dipole-like pattern along the east-west direction with an
amplitude of $\simeq\pm$200-300 km/s, indicating rotational motions induced by
the recent merger event. This feature constrains the viewing direction to
~30$^\circ$-50$^\circ$ relative to the normal of the merger plane. Our
hydrodynamic simulations suggest that Perseus has experienced at least two
energetic mergers since redshift z~1, the latest associated with the radio
galaxy IC310. This study showcases exciting scientific opportunities for future
missions with high-resolution spectroscopic capabilities (e.g., HUBS, LEM, and
NewAthena).

</details>
