<div id=toc></div>

# Table of Contents

- [gr-qc](#gr-qc) [Total: 10]
- [astro-ph.IM](#astro-ph.IM) [Total: 7]
- [astro-ph.HE](#astro-ph.HE) [Total: 15]
- [hep-ph](#hep-ph) [Total: 19]


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [1] [Unscreening of f(R) gravity near the galactic center black hole: Testability through pericenter shift below S0-2's orbit](https://arxiv.org/abs/2602.06151)
*Debojit Paul,Sanjeev Kalita,Abhijit Talukdar*

Main category: gr-qc

TL;DR: This paper examines the effects of f(R) gravity near the Galactic Centre's supermassive black hole, comparing it to General Relativity. It finds that deviations due to f(R) gravity are detectable in compact orbits, particularly when considering unscreened scalaron fields, and evaluates observational constraints using PPN parameters and future telescopes.


<details>
  <summary>Details</summary>
Motivation: The Galactic Centre's strong gravitational potential (100x solar system) provides a new testing ground for modified gravity theories like f(R). Solar system tests are insufficient, so studying the GC's environment can reveal deviations from GR.

Method: The study uses both model-dependent (R² gravity) and model-independent approaches. For the former, pericentre shift differences between GR and f(R) are analyzed for orbits like S0-2 (a=1000 au). Screening effects of scalarons (ψ) are studied using PPN parameters. Model-independent analysis derives f(R) parameters from PPN constraints.

Result: f(R) effects are significant for compact orbits. Heavier scalarons (10^-16 eV) are screened, but lighter ones (10^-22/10^-19 eV) remain unscreened in S0-2/S4716 orbits. These unscreened scalarons could be detectable with Extremely Large Telescopes.

Conclusion: The GC black hole's environment offers a stringent test for f(R) gravity. Astrometric observations from ELTs can constrain or detect modified gravity effects, highlighting the importance of upcoming telescopic capabilities in validating alternative theories.

Abstract: General Relativity (GR) has been tested extensively in the solar system and is being tested in the new environment of the Galactic Centre (GC) black hole where the dimensionless gravitational potential ($GM/c^2r$) is 100 times stronger than the one encountered in solar system. Therefore, the neighbourhood of the GC black hole is a naive opportunity to test modified theories of gravity. In this work, effect of $f(R)$ gravity near the black hole is studied. The difference of pericentre shift between GR and $f(R)$ gravity is studied for compact orbits having semi-major axis equal to and below $a=1000$ au (S0-2 like orbits). In a model dependent approach, we choose $f(R) \propto R^2$ (power law gravity) model which is cosmologically motivated and study the deviation in orbital pericentre shift for both zero spin and non-zero spin of the black hole. It is found that effect of $f(R)$ gravity becomes prominent for compact orbits. In model independent approach to $f(R)$ gravity with the generic scalaron fields ($ψ=f'(R)$), we extract the parameters of $f(R)$ gravity from the current bounds on Parametrised Post Newtonian (PPN) parameters ($γ, β$) near the GC black hole. The screening of $f(R)$ gravity is also investigated for these bounds on PPN parameters. It has been found that sufficiently massive scalarons ($10^{-16}$ eV) are completely screened but light and intermediate mass scalarons ($10^{-22}$ eV and $10^{-19}$ eV) are unscreened towards S0-2 like orbits as well as in the orbit of the newly discovered short period star S4716 ($a=407$ au). The possibility of detection of the $f(R)$ gravity effects due to these unscreened scalarons is forecasted with existing and upcoming astrometric capabilities of Extremely Large Telescopes (ELTs).

</details>


### [2] [Combining the Generalized and Extended Uncertainty Principles](https://arxiv.org/abs/2602.06189)
*Bernard Carr,Jonas Mureika*

Main category: gr-qc

TL;DR: The paper explores combined Generalized and Extended Uncertainty Principles (GEUP and EGUP), analyzing their implications in different energy regimes, including effects of negative parameters and connections to black holes and elementary particles.


<details>
  <summary>Details</summary>
Motivation: To investigate how combining GUP and EUP modifications of the Heisenberg Uncertainty Principle affects quantum mechanics at Planck scales, particularly examining their behavior in various regions of position-momentum space and potential links to black hole physics.

Method: The authors analyze two combined models (GEUP and EGUP), examining their mathematical formulations across the ($Δx$, $Δp$) plane. They consider both positive and negative parameter values, and explore the models' implications for black hole thermodynamics and their correspondence with elementary particle properties.

Result: GEUP predicts a new type of strong-gravity black hole, linking black hole physics to elementary particles. EGUP exhibits momentum-position duality with unusual features. Both models show that negative parameter values yield distinct physical behaviors, and all exhibit connections between uncertainty principles and black hole phenomena.

Conclusion: Combining GUP and EUP provides deeper insights into quantum gravity effects at Planck scales, particularly through their relationship with black holes. The findings suggest novel scenarios where quantum uncertainty principles directly influence black hole formation and particle-like behavior of spacetime structures.

Abstract: The Generalized Uncertainty Principle (GUP) and Extended Uncertainty Principle (EUP) are modifications to the Heisenberg Uncertainly Principle (HUP), expected to apply as the energy approaches the Planck scale. Here we consider a possible combination of these modifications (GEUP) and analyse the implications in various regions of the ($Δx$, $Δp$) plane. We also consider an alternative combination (EGUP) which exhibits duality between $Δp$ and $Δx$, showing that this has some unusual features. The parameters which describe these models are usually assumed to be positive but we extend our analysis to include negative values. All these proposals entail a link between black holes and the various types of Uncertainty Principle. In particular, the GEUP predicts a new kind of strong-gravity black hole and this implies an interesting link between black holes and elementary particles.

</details>


### [3] [The canonical energy-momentum currents in cosmology](https://arxiv.org/abs/2602.06509)
*Tomi S. Koivisto*

Main category: gr-qc

TL;DR: Proposes a parallel theory of relativity that allows conserved energy-momentum currents without Killing symmetries, uniting Einstein's gravity formulations and defining covariant charges uniquely, with applications in cosmology via Kodama vector and Misner-Sharp mass.


<details>
  <summary>Details</summary>
Motivation: To address limitations in conventional relativity by eliminating reliance on Killing symmetries for energy-momentum conservation and to unify Einstein's geometric and field equation approaches to gravity.

Method: Introduces the reference frame as an independent variational field, imposes a condition that it carries no energy, derives covariant charges, and applies the framework to isotropic/homogeneous cosmology linking to Kodama vector and Misner-Sharp mass.

Result: Successfully derives conserved currents without Killing vectors, unifies Einstein's formulations, and reproduces known mass expressions in cosmological contexts, validating the theory's consistency.

Conclusion: Demonstrates a novel relativistic framework with broader applicability than traditional approaches, offering new insights into gravitational energy quantification and cosmological dynamics.

Abstract: The parallel theory of relativity predicts conserved energy-momentum currents for an arbitrary metric, without invoking Killing symmetries. By treating the reference frame as an independent variational field and requiring it to carry no energy, the theory naturally unifies Einstein's two formulations of gravity and yields uniquely defined covariant charges. In isotropic and homogeneous cosmology, the canonical time direction selected by the reference frame coincides with the Kodama vector, and the associated Noether energy reproduces the Misner-Sharp mass.

</details>


### [4] [Continued fraction method for high overtone quasinormal modes in effective potentials with discontinuity](https://arxiv.org/abs/2602.06536)
*Guan-Ru Li,Jodin C. Morey,Wei-Liang Qian,Ramin G. Daghigh,Michael D. Green,Kai Lin,Rui-Hong Yue*

Main category: gr-qc

TL;DR: The study extends Leaver's method to compute quasinormal modes (QNMs) in black holes with discontinuous potentials, focusing on high overtones. The method uses expanded wavefunction at the discontinuity and junction conditions, achieving high precision for up to 2000 modes. Results show deformed high overtones with universal asymptotic features matching modified Pöschl-Teller potentials, suggesting potential observational impacts on gravitational waves.


<details>
  <summary>Details</summary>
Motivation: To address spectral instability引起的的波形大幅变形对QNMs的影响 and accurately compute high overtones in discontinuous potentials, moving beyond the limitations of traditional methods.

Method: Extended Leaver's continued fraction method by expanding wavefunctions at discontinuity points and incorporating Israel-Lanczos-Sen junction conditions, eliminating wavefunction convergence constraints.

Result: Computed QNMs up to 2000 modes with high precision. Low modes matched matrix/Prony method results; high overtones showed significant deformation due to echoes from discontinuity. Asymptotic spectra display universal features independent of discontinuity specifics.

Conclusion: High overtone deformations reveal universal asymptotic behavior, suggesting possible observational signatures in gravitational waves despite discontinuity variations. This implies collective overtone effects might be detectable.

Abstract: In this study, we extend Leaver's continued fraction method to evaluate black hole quasinormal modes (QNMs) in systems where the effective potential exhibits a discontinuity. Besides the low-lying modes, we particularly focus on high overtones, which are physically pertinent due to the substantial deformation of the QNM spectrum triggered by spectral instability. In our algorithm, we expand the wavefunction at the point of discontinuity, instead of the black hole horizon, and incorporate the Israel-Lanczos-Sen junction conditions. %As the wavefunction convergence condition becomes irrelevant, our proposed algorithm generalizes the original method by expanding the wavefunctions at the point of discontinuity, and the associated difficulty is mitigated by rectifying the recurrence relations between the expansion coefficients to incorporate the Israel-Lanczos-Sen junction conditions. We apply this algorithm to compute the QNMs of the modified Regge-Wheeler potential up to $2000$ modes with high precision. For the low-lying modes, the numerical results show excellent agreement with those obtained using the matrix and Prony methods. The high overtones are significantly deformed, owing to the presence of echoes due to the discontinuity. This deformation in the asymptotic QNM spectrum reveals universal features that are largely independent of the specific form of the discontinuity in the potential, seemingly coinciding with those observed in the modified Pöschl-Teller effective potential. We speculate on whether the collective effect of the high overtones has an observational impact on gravitational wave signals.

</details>


### [5] [Quantum Effective Dynamics and Stability of Vacuum in Anti-de Sitter Spacetimes](https://arxiv.org/abs/2602.06583)
*Shi-Yuan Li,Chengwu Liu*

Main category: gr-qc

TL;DR: The paper examines the canonical quantization of scalar and Maxwell fields in anti-de Sitter space, focusing on vacuum stability. For scalar fields with ξ > 5/48, ghost states are needed to ensure a non-negative Hamiltonian. Maxwell fields naturally yield a gauge-invariant Hamiltonian without such issues, and both fields' energy tensors are renormalized for a stable, symmetric vacuum. Semi-classical back-reactions are also analyzed.


<details>
  <summary>Details</summary>
Motivation: To ensure the stability of the quantum vacuum in AdS spacetime by resolving issues in quantization, particularly for non-minimally coupled scalar fields requiring ghost states beyond a critical ξ value.

Method: Canonical quantization of scalar (with ξRφ² term) and Maxwell fields in AdS; analysis of Hamiltonian self-adjointness and vacuum stability. Ghost state assignment for scalar fields with ξ>5/48 and gauge-fixing (e.g., temporal gauge) for Maxwell fields. Operator-level renormalization of energy tensors.

Result: Scalar fields require ghost states to achieve a non-negative Hamiltonian when ξ>5/48; Maxwell fields exhibit naturally stable vacuums. Renormalized energy tensors ensure maximally symmetric vacuums. Back-reactions from excited states via semi-classical Einstein equations are consistent with vacuum stability.

Conclusion: Non-minimal coupling in scalar fields demands careful quantization to avoid instabilities, while Maxwell fields remain robust. Renormalization and proper gauge handling ensure stable vacuums in AdS backgrounds, with ghost states providing a viable quantization framework for challenging cases.

Abstract: We investigate the details of the canonical quantization of effective quantum field theories in anti-de Sitter spacetime, emphasizing the stability of the quantum vacuum. We take the scalar and Maxwell fields as examples. For the non-minimally coupled massless real scalar field with ξRφ^2 term in the Lagrangian (mass can be introduced by shift of ξ), only when ξ\le 5/48, the quantized Hamiltonian is spontaneously non-negative and the vacuum is well defined. For ξ> 5/48, one has to assign the negative energy spectrum as that of the ghost particles, introducing anti-commutation relations to make the corresponding part of the Hamiltonian trivial, ensuring the Hamiltonian non-negative and the vacuum (and the Hilbert space) well defined. This method of ghost states is applicable once the proper radial boundary conditions guarantee the Hamiltonian self-adjoint. The resulting dynamics can be compared with those resulting from the positive self-adjoint extensions when the latter is available for ξ\le 9/48. For the Maxwell fields, the gauge invariant canonical energy momentum tensor straightforwardly leads to the gauge invariant non-negative Hamiltonian (well-defined vacuum). Hence the redundant gauge degree of freedom is irrelevant, and the 2-dimensional dynamical degrees of freedom are quantized in a concrete, e.g., temporal gauge. The energy momentum tensors for both quantized fields are renormalized to be finite at operator level, which renders the stable vacuum maximally symmetric. The back-reactions to the background spacetime by excited states via the semi-classical Einstein equations are also discussed.

</details>


### [6] [Gravastar on the brane with a timelike extra dimension](https://arxiv.org/abs/2602.06691)
*Shounak Ghosh,Rikpratik Sengupta,Kazuharu Bamba*

Main category: gr-qc

TL;DR: We construct a gravastar in the Shtanov-Sahni braneworld scenario, showing it can avoid singularities through braneworld dynamics and Weyl corrections, offering a black hole alternative.


<details>
  <summary>Details</summary>
Motivation: Classical black holes form singularities. The goal is to explore if SS braneworld dynamics can regularize geometry and prevent singularities without thin-shell approximations.

Method: Solve modified Einstein equations on the SS brane to get interior/shell/exterior solutions. Model core as Bose-Einstein condensate, shell as stiff matter. Analyze mass, entropy, etc., ensuring junction conditions.

Result: Gravastar exhibits suppressed/negative effective mass from repulsive condensate and stable equilibrium. Pressure anisotropy and stability arise from bulk Weyl corrections, not ad hoc assumptions.

Conclusion: SS gravastar is a viable compact object alternative to black holes, demonstrating geometric singularity avoidance via braneworld effects without idealized models.

Abstract: We construct a gravastar configuration within the Shtanov-Sahni (SS) braneworld scenario, characterized by a timelike extra dimension and negative brane tension. Unlike classical black holes, which inevitably culminate in central curvature singularities, our model demonstrates that the SS braneworld dynamics naturally regularize the interior geometry and prevent singularity formation. By solving the modified Einstein field equations induced on the brane, we obtain explicit interior, shell, and exterior solutions without invoking the idealized thin-shell approximation. The gravastar core is modeled as a Bose--Einstein condensate, while the intermediate shell consists of ultra-dense stiff matter. Bulk Weyl corrections induce anisotropic effective pressures on the brane, a feature that emerges intrinsically in this scenario and supports stability. We analyze the active gravitational mass, energy, entropy, and proper thickness of the shell, and establish the junction conditions at the interfaces. Our analysis reveals that the SS gravastar exhibits suppressed or even negative effective mass, reflecting the repulsive nature of the interior condensate, and admits stable equilibrium solutions consistent with energy conditions. This highlights the SS braneworld gravastar as a physically viable compact object and a compelling alternative to black holes. A key novelty of our construction is that the stabilizing pressure anisotropy and suppressed effective gravitational mass arise dynamically from higher-dimensional Weyl corrections, rather than being imposed through ad hoc matter sources or thin-shell idealizations. This provides the first fully analytic realization of a finite-thickness, stable gravastar in the Shtanov-Sahni braneworld, highlighting a genuinely geometric mechanism for singularity avoidance in compact objects.

</details>


### [7] [Accelerating TTL noise post-processing via combined coefficients and alternative TDI configuration](https://arxiv.org/abs/2602.06731)
*Xue Wang,Gang Wang*

Main category: gr-qc

TL;DR: The paper addresses tilt-to-length (TTL) noise in space-based gravitational-wave detectors like LISA by proposing a modified parameter set and using PD4L interferometry to accelerate parameter estimation, improving efficiency by factors of 10 and 18 for linear and quadratic models respectively.


<details>
  <summary>Details</summary>
Motivation: To reduce computational costs of parameter estimation for TTL noise models caused by spacecraft angular jitter, which currently require many parameters (24 for linear, 60 for quadratic), slowing down calibration.

Method: 1. Apply a linear transformation to original angular coupling coefficients to decorrelate TTL parameters, reducing interdependencies. 2. Use PD4L (second-gen TDI) instead of Michelson configuration for parameter fitting. Demonstrated through simulation showing accelerated convergence.

Result: Modified parameters and PD4L achieved 10x faster convergence for linear models and 18x for quadratic models, maintaining calibration accuracy without extra hardware.

Conclusion: The proposed methods significantly enhance computational efficiency in TTL noise calibration, critical for future space-based gravitational-wave missions.

Abstract: Tilt-to-length (TTL) noise induced by angular jitter of spacecraft and test masses can affect the sensitivity of space-based gravitational-wave detectors such as LISA, Taiji, and TianQin. Such angular jitter can be measured using the differential wavefront sensing technique, enabling the modeling and subtraction of TTL noise from the data. However, owing to the multiple degrees of freedom of the detector constellation, a linear TTL model requires at least 24 parameters, while a higher-fidelity quadratic model involves up to 60 coefficients, rendering parameter estimation computationally expensive. To accelerate parameter determination, we propose a modified parameter set obtained via a linear transformation of the original angular coupling coefficients, which effectively reduces correlations among TTL noise components. In addition, we perform parameter fitting using an alternative second-generation time-delay interferometry configuration, PD4L, rather than the fiducial Michelson configuration. These two improvements enhance the convergence speed of the fitting procedure by a factor of approximately 10 for the linear model and approximately 18 for the quadratic model. The proposed approach can therefore substantially improve the efficiency of TTL noise calibration in space-based gravitational-wave detectors.

</details>


### [8] [Timelike Entanglement Entropy of Hawking Radiation](https://arxiv.org/abs/2602.06833)
*Yahya Ladghami,Francisco S. N. Lobo,Taoufik Ouali*

Main category: gr-qc

TL;DR: The paper introduces timelike entanglement entropy of Hawking radiation as a new tool to study the black hole information paradox. It shows that this measure exhibits periodic or quasi-periodic behavior in various black hole spacetimes, correlates with black hole properties like charge and rotation, and offers a unitary information recovery mechanism without requiring islands or firewalls.


<details>
  <summary>Details</summary>
Motivation: To address the black hole information paradox by proposing a novel approach that avoids solutions with firewall or island assumptions, while preserving unitarity and horizon smoothness.

Method: The authors analytically continue black hole spacetimes to Euclidean signature to define timelike entanglement entropy correlations. They compute timelike Page times and analyze the behavior in Schwarzschild, Reissner-Nordström, higher-dimensional static/rotating, and braneworld black holes.

Result: Timelike entanglement entropy shows periodic/quasi-periodic oscillations with recurrence times dependent on surface gravity, charge, rotation, and dimensionality. Extremal black holes exhibit frozen thermal oscillations modulated by rotation. The approach consistently recovers information through radiation alone.

Conclusion: Timelike entanglement provides a physically intuitive, unitary mechanism for information recovery across diverse black hole types, offering a robust alternative to existing paradigms in quantum gravitational studies.

Abstract: We introduce the concept of timelike entanglement entropy of Hawking radiation as a novel probe of the black hole information paradox. By analytically continuing black hole spacetimes to Euclidean signature, we define timelike correlations that reveal a sequence of timelike Page times at which the entanglement entropy equals the Bekenstein-Hawking entropy. Applying this framework to Schwarzschild, Reissner-Nordström, static higher-dimensional and braneworld solutions, four-dimensional Kerr, and higher-dimensional rotating Myers--Perry black holes, we demonstrate that timelike entanglement exhibits periodic or quasi-periodic behavior, with the recurrence times sensitive to surface gravity, charge, rotation, and spacetime dimensionality. Extremal and near-extremal black holes display effectively frozen thermal oscillations with persistent rotational modulation, reflecting their near-horizon geometries. Unlike conventional approaches based on islands or firewalls, our framework encodes information entirely in the Hawking radiation, preserving unitarity while avoiding violations of horizon smoothness. These results establish timelike entanglement as a robust and physically transparent mechanism for information recovery and provide a versatile tool for exploring quantum gravitational dynamics across a wide range of black hole spacetimes.

</details>


### [9] [Chaos as a Possible Probe for Scalar Hair in Horndeski Gravity](https://arxiv.org/abs/2602.06848)
*Yang Yu,Ruo-Ting Chen,Shulan Li,Dan Zhang,Jian-Pin Wu*

Main category: gr-qc

TL;DR: This study introduces chaotic dynamics as a new method to detect black hole scalar hair in Horndeski gravity by analyzing the motion of spinning test particles, finding that scalar hair reduces orbital chaos and enhances gravitational wave coherence, providing a novel probe for testing gravity in strong-field regimes.


<details>
  <summary>Details</summary>
Motivation: To develop a sensitive probe for detecting black hole scalar hair, which challenges the general relativity no-hair theorem, beyond conventional methods.

Method: Investigated the motion of spinning test particles in static, spherically symmetric hairy black hole spacetimes under Horndeski gravity. Analyzed indicators like precession regularity, Lyapunov exponents, Poincare sections, and gravitational wave polarization correlations.

Result: Increasing scalar hair reduces orbital chaos (lower Lyapunov exponents, regular precession, smaller Poincare sections) and strengthens correlation between gravitational wave polarizations, restoring phase coherence.

Conclusion: Chaotic observables and gravitational wave signatures together form a sensitive probe for detecting scalar hair, offering a new approach to test strong-field gravity theories.

Abstract: The detection of black hole scalar hair, a possible deviation from general relativity's "no-hair" theorem, requires sensitive probes beyond conventional methods. This study proposes chaotic dynamics as a novel indicator for scalar hair in Horndeski gravity. We investigate the motion of a spinning test particle in a static, spherically symmetric hairy black hole spacetime. Our results show that increasing scalar hair systematically suppresses orbital chaos, as evidenced by regularized precession, reduced Lyapunov exponents, and contracted Poincare sections. Furthermore, scalar hair enhances the correlation between the two gravitational wave polarization modes, restoring phase coherence. These findings demonstrate that chaotic observables and gravitational wave signatures can jointly serve as sensitive probes for black hole hair, offering a complementary approach to testing gravity in strong-field regimes.

</details>


### [10] [The N-Body 2PN Hamiltonian and Numerical Integration of the Equations of Motion](https://arxiv.org/abs/2602.06961)
*Felix M. Heinze,Gerhard Schäfer,Bernd Brügmann*

Main category: gr-qc

TL;DR: The paper presents an analytic expression for the general N-body 2PN Hamiltonian in the ADM gauge, addressing previously unresolved integral terms and validating it numerically. It also explores efficient numerical integration methods for N-body systems at 2PN order.


<details>
  <summary>Details</summary>
Motivation: The motivation is to extend the known 2PN Hamiltonian beyond three bodies, enabling more accurate gravitational dynamics calculations for arbitrary N-body systems, and to provide computationally feasible methods for its evaluation.

Method: The authors derived an analytic form for the N-body 2PN Hamiltonian except for one integral term, developed numerical methods to evaluate all integrals to machine precision, and tested various integration techniques for computational efficiency.

Result: They successfully computed the N-body 2PN Hamiltonian numerically, validated against analytical results where possible, and demonstrated feasible equation-of-motion integration using different methods with efficiency optimization strategies.

Conclusion: The work enables precise gravitational dynamics simulations for any number of bodies at 2PN accuracy and paves the way for future research in high-precision relativistic N-body systems.

Abstract: To date, the second-order post-Newtonian (2PN) Hamiltonian has been known in closed analytic form only for systems of up to three point masses. In this paper, we present an analytic expression for the general $N$-body 2PN Hamiltonian in the ADM gauge up to a single integral term that, to our knowledge, has no known closed-form analytic solution. We show that the integrals appearing in the 2PN Hamiltonian can be evaluated numerically to machine precision, allowing for cross-validation against analytical results and enabling the full numerical computation of the $N$-body 2PN Hamiltonian. Furthermore, we demonstrate the practical feasibility of the numerical integration of the equations of motion for $N$ bodies at 2PN order using different methods and discuss several strategies for improving computational efficiency.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [11] [Correlation Calibration: A Hybrid Calibration Technique for Radio Interferometric Arrays](https://arxiv.org/abs/2602.06109)
*Robert Pascua,Jonathan Sievers,Adrian Liu*

Main category: astro-ph.IM

TL;DR: Introduces CorrCal, a covariance-based calibration method for radio interferometric arrays that combines sky-based and redundant calibration, offering robustness against modeling errors and high computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Calibrating drift-scanning arrays is challenging due to the inability to track calibration sources, leading to reliance on accurate sky and instrument models. Small calibration errors can overwhelm the cosmological 21 cm signal, necessitating a robust calibration strategy.

Method: CorrCal is a hybrid calibration approach that integrates traditional sky-based calibration with redundant calibration, using a covariance-based framework to handle model uncertainties efficiently.

Result: CorrCal demonstrates unbiased calibration errors below thermal noise thresholds across various modeling errors and processes 1,000-element arrays in under a second on a single laptop core.

Conclusion: CorrCal is a computationally efficient and robust tool for calibrating 21 cm cosmology data, suitable for current and future experiments like next-generation radio interferometers.

Abstract: Calibrating out per-antenna signal chain effects is an essential step in analyzing radio interferometric data. For drift-scanning arrays, robustly calibrating the data is especially challenging due to the lack of the ability to track a calibration source. Consequently, calibration strategies for drift-scanning arrays are limited by our knowledge of the radio sky at large, as well as the direction-dependent instrument response. In the context of 21 cm cosmology, where small calibration errors can conspire to overwhelm the cosmological signal, it is therefore crucially important to develop calibration strategies that are capable of accurately calibrating the data in the presence of sky or instrument modeling errors. In this paper we present CorrCal, a covariance-based calibration strategy for redundant radio interferometric arrays. CorrCal is a hybrid calibration strategy that leverages the strengths of traditional sky-based calibration and redundant calibration in a computationally efficient framework that is fairly insensitive to modeling errors. We find that the calibration errors from CorrCal are unbiased and far below typical thermal noise thresholds across a wide range of modeling error scenarios. We show that CorrCal is computationally efficient: our implementation is capable of evaluating the likelihood and its gradient in less than a second for 1,000-element class arrays using just a single laptop core. Given CorrCal's computational efficiency and robustness to modeling errors, we anticipate that it will serve as a useful tool in the analysis of radio interferometric data from current and next-generation experiments targeting the cosmological 21 cm signal.

</details>


### [12] [A Demonstration of a Neural Network as a Bridge Between Galaxy Simulations and Surveys](https://arxiv.org/abs/2602.06492)
*E. Elson*

Main category: astro-ph.IM

TL;DR: The paper shows that stellar masses of galaxies in the GAMA survey can be accurately predicted using a simple neural network trained on synthetic data from the SHARK model, requiring only absolute magnitudes and color indices. This approach outperforms expectations, showing that complex deep learning isn't necessary for robust mass estimation.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of deriving accurate stellar masses for galaxies using minimal observational data and demonstrate that simple machine learning models, trained on simulations, can effectively transfer to real observations without requiring deep architectures.

Method: A single hidden layer neural network trained on synthetic galaxies from the SHARK semi-analytic model. The model uses only absolute magnitudes and color indices as inputs to predict stellar masses, which are then validated against GAMA's SED-derived masses.

Result: The model achieves a scatter of ~0.131 dex across 3.5 dex in stellar mass, closely matching GAMA's measurements. It successfully predicts masses for 17,006 galaxies without SED-derived masses, providing error estimates through photometric uncertainty propagation.

Conclusion: Simple machine learning models trained on simulations can efficiently and accurately estimate stellar masses, offering a computationally lightweight alternative to complex models. This method establishes a transparent and effective framework for transfer learning in galaxy evolution studies.

Abstract: This paper demonstrates that the stellar masses of galaxies in the Galaxy and Mass Assembly (GAMA) survey, originally derived via stellar population synthesis modelling, can be accurately predicted using only their absolute magnitudes and colour indices. A central contribution of this work is the demonstration that this long-standing inference problem can be solved using an exceptionally simple machine-learning model: a fully connected, feed-forward artificial neural network with a single hidden layer. The network is trained exclusively on synthetic galaxies generated by the SHARK semi-analytic model and is shown to transfer effectively to real observations. Across nearly 3.5 dex in stellar mass, the predicted values closely track the GAMA SED-derived masses, with a typical scatter of ~0.131 dex. These results demonstrate that complex deep-learning architectures are not a prerequisite for robust stellar mass estimation, and that simulation-trained, lightweight machine-learning models can capture the dominant physical information encoded in broad-band photometry. The method is further applied to 17,006 GAMA galaxies lacking SED-derived masses, with photometric uncertainties propagated through the network to provide corresponding error estimates on the inferred stellar masses. Overall, this work establishes a computationally efficient and conceptually transparent pathway for simulation-to-observation transfer learning in galaxy evolution studies.

</details>


### [13] [Joint optical-digital design strategy for adaptive optics systems: application to wavelength selection for satellite imaging](https://arxiv.org/abs/2602.06537)
*Florian Cheyssial,Laurent Mugnier,Cyril Petit*

Main category: astro-ph.IM

TL;DR: The paper proposes a co-design strategy for adaptive optics and digital restoration in imaging systems, optimizing wavelength distribution and using multiple wavelength channels to enhance robustness against turbulence and source variations.


<details>
  <summary>Details</summary>
Motivation: Traditional sequential design of adaptive optics and digital systems leads to suboptimal performance. Co-design can better integrate adaptive optics and restoration algorithms, improving overall system efficiency and robustness.

Method: Derive a criterion considering source properties and system performance for joint optimization. Apply this to optimize wavelength distribution between wavefront sensor and camera. Explore multi-wavelength channels for turbulence and source variation robustness.

Result: Demonstrated improved system performance through co-design, particularly in optimizing wavelength allocation and multi-channel usage. Shows potential for future system-wide parameter optimization.

Conclusion: Joint optimization of adaptive optics and digital systems enhances imaging quality. Future work includes expanding co-design to all system parameters for even better performance adaptability.

Abstract: Adaptive optics can be used to mitigate the effects of atmospheric turbulence on imaging systems, but the correction is only partial, and deconvolution is often required to improve the resolution. This results in entire optical/digital systems, which are traditionally designed sequentially, i.e. , the adaptive optics system is optimised first, and the restoration algorithms are designed a second time. Studies on optical/digital systems have shown that jointly optimizing the whole system is a better alternative. We propose to extend these co-design strategies to the design of an adaptive optics-assisted imaging system. We derive a simple criterion that takes into account the source properties and the entire optical/ digital system performance. To illustrate its interest, we use it to optimize the wavelength distribution between the wavefront sensor and the imaging camera. In addition, we explore the potential of using multiple imaging channels operating at different wavelengths as a means of making an imaging system robust to turbulence strength and source magnitude variations. Later, any parameter of the optical/digital system, if not the entire system itself, could be optimized this way.

</details>


### [14] [SF2A Environmental Transition Commission: Summary of the 2025 workshop](https://arxiv.org/abs/2602.06564)
*Faustine Cantalloube,Jack Berat,Naïs Fargette,Pierre Larue,N. Pourré,Sarah E Anderson,Julien Milli,Jean-François Gonzalez,Olivier Berné,Didier Barret,Angèle Mouinié,Clarisse Picard,Karine Dassas,Jürgen Knödlseder,David Redon*

Main category: astro-ph.IM

TL;DR: The SF2A 2025 conference included a special session on environmental transition, focusing on institutional initiatives, early-career researchers' roles, research infrastructure, and geopolitical factors influencing astronomical research, concluding with a collaborative discussion to gather diverse perspectives.


<details>
  <summary>Details</summary>
Motivation: To address the necessity of sustainable practices and environmental considerations within the French astronomy community, ensuring that research activities align with broader societal goals of environmental stewardship while adapting to geopolitical challenges.

Method: The session analyzed four key areas: institutional actions, early-career researcher challenges, infrastructure and tools, and geopolitical conditions. Discussions included presentations and a round-table to engage participants in sharing insights and strategies.

Result: Identified key strategies and challenges in implementing environmental transitions within astronomy, emphasizing the need for structured institutional support, tailored programs for early-career scientists, and consideration of geopolitical dynamics to maintain research viability.

Conclusion: Collaborative efforts across institutions, tailored support for early-career researchers, and proactive adaptation to geopolitical changes are critical for fostering sustainable and resilient astronomical research in France.

Abstract: During its annual conference in 2025, the French Society of Astronomy \& Astrophysics (SF2A) hosted, for the fifth time, a special session dedicated to discussing the environmental transition within the French A\&A research community. During the 2025 workshop, the goal was to review four contemporary topics within the context of environmental transition actions and discussions: (1) institutional actions, (2) the early-career researchers singularity, (3) research infrastructures and tools, and (4) the geopolitical conditions under which A\&A research remains possible. The workshop concluded with a round-table discussion that brought together the various speakers so that every participant could express their ideas.

</details>


### [15] [Mobile neutron monitor for latitude cosmic ray monitoring](https://arxiv.org/abs/2602.06589)
*Kobelev P. G.,Maurchev E. A.,Yanke V. G*

Main category: astro-ph.IM

TL;DR: The paper describes the development and testing of a modernized neutron monitor designed for marine expeditionary studies, demonstrating its effectiveness in cosmic ray measurements under challenging conditions.


<details>
  <summary>Details</summary>
Motivation: To enable high-precision continuous monitoring of galactic cosmic rays in expeditionary settings with limited maintenance access, requiring compact and stable equipment.

Method: Upgraded CHM-15 boron thermal neutron counter integrated with a microprocessor-based data acquisition system, using modern experimental nuclear physics methods.

Result: Test trials showed comparable performance to standard neutron monitors, with enhanced portability and operational stability for field use.

Conclusion: The mobile neutron detector is viable for expeditionary cosmic ray studies, offering reliable data under conditions where maintenance is difficult.

Abstract: Neutron monitors are a standard tool for high-precision continuous monitoring of galactic cosmic ray flux variations arising from variations in heliospheric conditions and solar activity for space weather applications. These measurements form the basis for solving the inverse problem of determining the cosmic ray anisotropy vector beyond the magnetosphere. To support such studies, periodic latitude measurements are necessary to determine the coupling functions of primary and secondary cosmic rays variations. The aim of this work is to develop and characterize a modernized standard neutron monitor based on a CHM-15 boron thermal neutron counter and a data acquisition system designed for marine expeditionary studies of cosmic ray variations. Modern nuclear physics experimental methods and the principles of microprocessor-based data acquisition systems were used to solve this problem. The results of test trials and of continuous monitoring showed that the characteristics of the upgraded and standard neutron monitor are similar, and the ease of use, compactness, and stability allow us to conclude that the mobile neutron detector can be used in expeditionary conditions with limited access for maintenance personnel.

</details>


### [16] [Introducing a new generation Adaptive Optics simulation framework: from PASSATA to SPECULA](https://arxiv.org/abs/2602.06688)
*Fabio Rossi,Alfio Puglisi,Guido Agapito*

Main category: astro-ph.IM

TL;DR: The paper introduces SPECULA, a new Python-based AO simulation framework designed for scalable, efficient end-to-end numerical simulations across CPU, GPU, and HPC environments. It enables hybrid setups combining simulations with real hardware and demonstrates applications in system design and lab experiments.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing AO simulation tools like PASSATA by creating a more flexible, computationally efficient, and scalable framework capable of handling complex systems and integrating with both supercomputing resources and laboratory hardware.

Method: SPECULA uses an object-oriented architecture modeling physical components as interconnected processing objects. It leverages Python with CuPy and CUDA streams for GPU acceleration, supports distributed computing via HPC clusters (tested on Leonardo), and includes hybrid simulation capabilities to interface with real laboratory equipment.

Result: Successful deployment of SPECULA across multiple platforms (CPU/GPU/HPC), validated through experiments on Leonardo supercomputer and hybrid simulations in Arcetri's labs. Examples showcase its efficiency and versatility in AO system prototyping and real-time testing.

Conclusion: SPECULA provides a robust, adaptable platform for AO system development, enabling efficient simulation at scale and seamless integration with experimental setups. Future work focuses on enhancing modularity, adding new AO component models, and expanding support for real-time hardware interfaces.

Abstract: Numerical end-to-end simulation in Adaptive Optics (AO) is a key tool in the development of complex systems, from the initial design to the commissioning phase. Based on our previous experience with PASSATA, we decided to develop a new AO simulation framework in Python language, naming it SPECULA (short for: Scalable Parallel Execution of Computations Upscaling Large Adaptive optics simulations). Following an object-oriented approach, the physical entities are modeled as processing objects connected to each other to exchange data objects. A simulation is run by providing its description instead of writing and executing a specific script. The Python language and its library flexibility allowed us to write one single code that can be run on CPU and GPU platforms. We put a strong focus on computational efficiency, relying on CuPy and its interface to access the CUDA-stream mechanism. Moreover, SPECULA is capable of distributed computations over multiple processing nodes, making it suitable to run in an HPC environment, as tested on the Italian supercomputer Leonardo. SPECULA can also be used in laboratory environment to implement a hybrid simulation, allowing us to interface simulated and concrete objects: this feature was demonstrated in the Adaptive Optics laboratories at Arcetri Observatory. In this paper, we describe the main characteristics of SPECULA, show some relevant examples of its use, and finally draw our goals for the future.

</details>


### [17] [Antenna for the detection of electromagnetic audio-band disturbances on-board LISA](https://arxiv.org/abs/2602.06878)
*D. Serrano,A. Pérez-Ortega,D. Roma-Dollase,J. Salvans-Tort,J. Ho-Zhang,J. Ramos-Castro,M. Nofrarias*

Main category: astro-ph.IM

TL;DR: LISA Pathfinder's coils, used as magnetometers, surpass LISA mission requirements by an order of magnitude in the audio frequency band, achieving magnetic noise levels as low as 0.1 pT/√Hz at frequencies above 1 kHz.


<details>
  <summary>Details</summary>
Motivation: The LISA mission requires monitoring of high-frequency magnetic signals from onboard electronics to maintain instrument sensitivity below millihertz frequencies, which are affected by magnetic forces. Current magnetometers may not meet these demands.

Method: Characterization of LISA Pathfinder's coils operating as magnetic sensors in the audio frequency band. Performance metrics were measured and compared to LISA mission requirements.

Result: LPF coils demonstrated superior performance: 1.45 pT/√Hz at 50 Hz, 0.17 pT/√Hz at 500 Hz, and 0.1 pT/√Hz above 1 kHz, exceeding LISA's specifications by tenfold at lower audio-band frequencies.

Conclusion: The LPF coil magnetometers are capable of meeting and exceeding the magnetic monitoring needs for the LISA mission, especially in high-frequency ranges, ensuring better sensitivity protection against magnetic interference.

Abstract: The LISA mission will be the first observatory to detect gravitational waves from space within the millihertz frequency band. Magnetic forces have an important impact on the instrument's sensitivity below the millihertz. Hence, monitoring the magnetic environment within each of the LISA spacecrafts is of utmost importance. In this Letter we present the characterization of the coils that were used in LISA Pathfinder (LPF) when operating as magnetic sensors in the audio frequency band. The necessity of implementing this type of magnetometer is presented in order to monitor high frequency magnetic signals from the electronics on-board. We show that the LPF coils have a performance one order of magnitude better than the current requirements set by the LISA mission at the low end of the audio-band frequency. The LPF coils are able to measure a magnetic noise level of 1.45 $\rm pT/\sqrt{\rm{Hz}}$ at 50 Hz and 0.17 $\rm pT/\sqrt{\rm{Hz}}$ at 500 Hz. Additionally, the LPF coils can reach a magnetic noise floor of 0.1 $\rm pT/\sqrt{\rm{Hz}}$ at frequencies above 1 kHz.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [18] [Searching for dark matter signals with high energy astrophysical neutrinos in IceCube](https://arxiv.org/abs/2602.06121)
*Khushboo Dixit,Gopolang Mohlabeng,Soebur Razzaque*

Main category: astro-ph.HE

TL;DR: The study uses IceCube neutrino data from four AGN to set the tightest constraints on DM-neutrino interaction cross-sections, finding upper limits of 8×10^-39 cm² (energy-independent) and 1×10^-39 cm² (energy-dependent) at 90% CL.


<details>
  <summary>Details</summary>
Motivation: To probe DM-neutrino interactions using high-energy neutrinos from AGN with enhanced DM densities, leveraging IceCube's detection of neutrino sources like TXS 0506+056 and others.

Method: Statistical analysis of individual AGN neutrino data and a stacked analysis of all four sources, focusing on DM spikes around SMBHs and adiabatic black hole growth scenarios.

Result: The stacked analysis yielded the strongest constraints: σ₀ ≤ 8×10⁻³⁹ cm² (energy-independent) and σ₀ ≤ 1×10⁻³⁹ cm² (energy-dependent) at 90% confidence level.

Conclusion: These results provide the most stringent limits to date on DM-neutrino scattering cross-sections, offering insights into particle physics beyond the Standard Model via astrophysical DM signatures.

Abstract: High-energy neutrinos provide a potentially powerful and distinctive probe for dark matter (DM) - neutrino interactions, particularly in environments with enhanced DM densities, such as the DM spikes predicted to form around supermassive black holes (SMBHs) at the center of active galactic nuclei (AGN). Recent observations by the IceCube Neutrino Observatory, which identified four AGN, namely TXS 0506+056, NGC 1068, PKS 1424+240, and NGC 4151 as neutrino sources, provide a unique opportunity to search for signatures of these interactions. In this study, we use IceCube data to derive the most stringent constraints to date on both the energy-dependent and energy-independent DM-neutrino scattering cross-sections. We perform a statistical analysis using data from individual sources as well as a combined (stacked) analysis of all four sources. Our strongest limits arise from the stacking analysis, yielding an upper bound of $σ_{0} \le 8\times 10^{-39}$ cm$^2$ for an energy-independent cross-section and $σ_{0} \le 10^{-39}$ cm$^2$ for a linearly energy-dependent cross-section, both at 90\% confidence level, particularly in scenarios involving the adiabatic growth of black holes.

</details>


### [19] [Ne and Fe abundances in the ISM: Archival Study of Fe-L and Ne-K edges in Chandra and XMM-Newton](https://arxiv.org/abs/2602.06132)
*D. L. Moutard,L. R. Corrales,I. Psaradaki,E. Temple,M. Shi*

Main category: astro-ph.HE

TL;DR: The paper measures interstellar iron and neon abundances using X-ray absorption features from LMXBs, finding Fe depleted by 90% and tight constraints on abundance ratios consistent with literature.


<details>
  <summary>Details</summary>
Motivation: To precisely determine elemental abundances in the interstellar medium by studying X-ray absorption features, addressing uncertainties from depletion and scattering.

Method: Analyzed soft X-ray spectra from XMM-Newton and Chandra observations of low-mass X-ray binaries (LMXBs) to calculate column densities for Fe and Ne, accounting for depletion and scattering effects.

Result: Found [Fe/Ne] = -0.523±0.025, [Fe/H] = 7.482±0.016, and [Ne/H] = 8.012±0.022 with 5% and 1-7% uncertainties from depletion and scattering respectively, confirming consistency with protosolar abundance references.

Conclusion: The study provides the tightest observational constraints on interstellar Fe and Ne abundances, highlighting the importance of account ing for grain depletion and scattering in such measurements.

Abstract: The abundance of elements in the interstellar medium (ISM) is a key facet for many fields of astrophysical study. In the soft X-ray spectra, absorption by interstellar gas can result in deep absorption features that affect continuum measurements. In this paper, we focus on measuring the abundance of interstellar iron and neon from the column densities observed in soft spectra from XMM-Newton and Chandra for various low mass X-ray binaries (LMXBs), which allows for a direct probe of elemental abundances. As a noble gas, neon will not deplete into solid form, thus providing a benchmark with abundances determined via UV spectroscopy. We find that, when assuming Fe is 90\% depleted into grains, [Fe/Ne]$ = -0.523\pm0.025$, [Fe/H]$ + 12 = 7.482\pm0.016$, and [Ne/H]$ + 12 = 8.012\pm0.022$, which are the tightest observational constraints on these abundances to date, while being consistent with literature which uses protosolar abundances. We also test how depletion into solid grains and scattering affect the results. The choice of depletion fraction can affect the abundance measurement by roughly $5\%$, and that the inclusion of a scattering component can affect abundance measurements by $\sim1-7\%$.

</details>


### [20] [How Neutron Star Observations Point Towards Exotic Matter: Existing Explanations and a Prospective Proposal](https://arxiv.org/abs/2602.06170)
*Mauro Mariani,Ignacio F. Ranea-Sandoval*

Main category: astro-ph.HE

TL;DR: The paper proposes slow stable hybrid star configurations with a novel quark matter equation of state parametrization to address tensions between neutron star observations and theoretical models. It finds that exotic matter in cores is favored, but uncertainties remain due to model assumptions and parameter tuning.


<details>
  <summary>Details</summary>
Motivation: To reconcile discrepancies between observational data from multi-messenger astronomy and theoretical equations of state for neutron star cores, particularly addressing the need for models that satisfy all current constraints.

Method: Proposes slow stable hybrid star configurations with an abrupt first-order phase transition and a new non-CSS quark matter equation of state. Analyzes these models alongside existing theoretical approaches.

Result: Modern observations support exotic matter in neutron star cores. The proposed slow hybrid star scenario meets constraints but requires careful parameter tuning. Persistent theoretical tensions and uncertainties remain unresolved.

Conclusion: While hybrid stars with exotic cores align with observations, unresolved issues like phase transition assumptions and parameter sensitivity indicate the need for improved microphysics understanding and observational data.

Abstract: Multi-messenger astronomical observations of neutron stars, together with more precise calculations and constraints coming from dense matter microphysics, are generating tension with regard to equations of state models used to describe neutron star cores. Assuming an abrupt first-order phase transition with a slow conversion speed between phases, we propose different slow stable hybrid star configurations aiming to reconcile all current constraints simultaneously; within this framework, we also introduce a novel non-CSS parametrization to the quark matter equation of state and discuss its strengths and limitations. We analyze our model results in conjunction with a review of other relevant theoretical possibilities existing in the literature. We found that modern neutron star observations seem to favor the existence of some type of exotic matter in the neutron star cores; in particular, our slow stable hybrid star scenario remains a proposal capable of satisfying these constraints. However, due both to the existing skepticism regarding some of the adopted hypotheses in most extreme neutron star measurements and to the precise adjustment needed for the equation-of-state parameters, significant tension and open questions remain.

</details>


### [21] [Hypersoft X-ray Sources: A New Class of Luminous Cosmic Emitters](https://arxiv.org/abs/2602.06192)
*Mustafa Muhibullah,Jimmy A. Irwin,R. Di Stefano*

Main category: astro-ph.HE

TL;DR: The paper introduces hypersoft X-ray sources, a new class of luminous extragalactic objects detected below 0.3 keV, which are likely accreting compact objects such as white dwarfs, neutron stars, or black holes. These sources were previously missed due to observational limitations, emit most of their energy in EUV, and may significantly contribute to galactic ionization.


<details>
  <summary>Details</summary>
Motivation: The motivation is to discover and characterize novel astrophysical sources that emit predominantly in the EUV and soft X-ray regime, a spectral range traditionally overlooked by astronomical surveys. This addresses the observational challenges in detecting cooler thermal sources and expands our understanding of galactic X-ray binaries and their roles.

Method: Analysis of X-ray data to identify point-like sources with spectra peaking below 0.3 keV, calculating photon ratio (0.15-0.3 keV vs 0.3-1.0 keV) >8 to distinguish these sources. Spectral modeling suggests their luminosity peaks in EUV with bolometric corrections. They are proposed as accreting compact objects including potential Type Ia supernova progenitors.

Result: Discovery of hypersoft X-ray sources in galaxies, characterized by extreme softness ratios and luminosities above 1E38 erg/s in the 0.15-0.3 keV band. These were undetected in past surveys due to their EUV peak, making them major sources of ionizing photons in galaxies.

Conclusion: Hypersoft sources represent a new population of high-energy emitters critical for galactic ecology. Their identification highlights the need for improved EUV detection capabilities and suggests their importance in galactic evolution through ionization processes.

Abstract: X-ray binaries, powered by black holes, neutron stars, or white dwarfs accreting matter from a companion star, are among the brightest beacons in galaxies, outshining the Sun by a factor of millions. Most emit primarily above 0.3 keV in X-rays, but cooler thermal sources peaking in the extreme ultraviolet (EUV) would be much more difficult to detect due to astronomy's critical blind spot in EUV. Here, we report the discovery of a remarkable new class of luminous, point-like, non-nuclear X-ray objects in galaxies-hypersoft X-ray sources -- that have been missed by all previous surveys to date. Detected primarily or exclusively below 0.3 keV, with 0.15-0.3 keV to 0.3-1.0 keV photon ratios >8, the most luminous examples radiate >1E38 erg/s in the narrow X-ray band, with spectral models indicating even greater bolometric luminosities, largely emitted in the EUV. They rank among the most energetic sources in galaxies, yet their EUV-peaking spectra evaded earlier detections. We propose that hypersoft sources are X-ray binaries spanning multiple physical classes, including accreting white dwarfs or post-nova systems-potential Type Ia supernova progenitors-and systems hosting accreting black holes. Beyond their elusive nature, they may play crucial role in ionizing gas within galaxies.

</details>


### [22] [An Archival Optical Counterpart Search for Extragalactic Fast X-Ray Transients Discovered by Einstein Probe](https://arxiv.org/abs/2602.06321)
*Run-Duo Liang,Wen-Xiong Li,Liang-Duan Liu,Ken Smith,Stephen Smartt,Niu Li,Arne Rau,Ling-Zhi Wang,Armin Rest,Ezequiel Treister,Jia-Sheng Huang,Franz Bauer,Jennifer Chacon,Ning-Chen Sun,Qin-Yu Wu,Seán Brennan,Matt Nicholl,Ting-Wan Chen,Amar Aryan,Sheng Yang,Albert K. H. Kong,Sofia Rest,Qinan Wang,James Gillanders,Dong-Yue Li,An Li,Jun Yang,Qing-Chang Zhao,Hui Sun,Yun-Fei Xu,Zhi-Xing Ling,Thomas J. L. de Boer,Chien-Cheng Lin,Thomas B. Lowe,Ken C. Chambers,Eugene A. Magnier,J. Quirola-Vásquez,Xiaofeng Wang,Jing-Wei Hu,Yong Chen,Chen Zhang,Dong-Hua Zhao,He-Yang Liu,Hua-Qing Cheng,Chen-Zhou Cui,Shu-Mei Jia,Cheng-Kui Li,Ju Guan,Mao-Hai Huang,Hao-Wei Peng,Samaporn Tinyanont,Yuan Liu,Wei-Min Yuan*

Main category: astro-ph.HE

TL;DR: The Einstein Probe (EP) detects an extragalactic fast X-ray transient (eFXT), EP240506a, associated with a core-collapse supernova (AT 2024ofs).光学 follow-up observations and rate estimates highlight EP's role in linking high-energy transients to supernovae and stress the need for rapid follow-up.


<details>
  <summary>Details</summary>
Motivation: To classify eFXTs and determine their physical origins by identifying optical counterparts, which is hindered by insufficient timely follow-up observations.

Method: Systematic search of optical survey data (ZTF, TNS) for eFXT counterparts detected by EP; spectroscopy of host galaxy (VLT) for redshift; analysis of archival and multiwavelength data to characterize the transient; detectability simulations to estimate event rates.

Result: EP240506a linked to supernova AT 2024ofs with z=0.120; luminosity/light-curve matches core-collapse supernova; estimated local event rate density 8.8−3.9+21.2 yr⁻¹ Gpc⁻³; completeness-corrected rate 36–78 yr⁻¹ Gpc⁻³ for EP-detected X-ray transients linked to supernovae.

Conclusion: EP's capability to discover eFXTs connected to core-collapse supernovae is demonstrated, emphasizing the necessity for rapid follow-up observations to fully utilize EP's potential.

Abstract: Extragalactic fast X-ray transients (eFXTs) represent a rapidly growing class of high-energy phenomena, whose physical origins remain poorly understood. With its wide-field, sensitive all-sky monitoring, the Einstein Probe (EP) has greatly increased the discovery rate of eFXTs. The search and identification of the optical counterparts of eFXT are vital for understanding their classification and constraining their physical origin. Yet, a considerable fraction of eFXTs still lack secure classifications due to the absence of timely follow-up observations. We carry out a systematic search of publicly available optical survey data and transient databases (including the Zwicky Transient Facility, ZTF, and the Transient Name Server, TNS) for optical counterparts to eFXT candidates detected by EP. In this paper, we describe our ongoing program and report the first results. Specifically, we identified the eFXT EP240506a to be associated with a UV/optical counterpart, AT 2024ofs. Spectroscopy of its host galaxy with VLT yields a redshift of $z = 0.120 \pm 0.002$. By combining archival survey data with early-time multiwavelength observations, we find that the luminosity and light-curve evolution of AT~2024ofs are consistent with a core-collapse supernova origin. From detectability simulations, we estimate a local event rate density $ρ_{0}=8.8^{+21.2}_{-3.9}\ \mathrm{yr^{-1}\, Gpc^{-3}}$ for EP240506a-like events, and completeness-corrected rate of about $36$--$78\ \mathrm{yr^{-1}\ Gpc^{-3}}$ for EP-detected X-ray transients associated with supernovae. Our results demonstrate the potential of EP to uncover prompt high-energy emission from core-collapse supernovae and underscore the critical importance of timely follow-up of future eFXT events.

</details>


### [23] [Primordial Black Hole signatures from femtolensing and spectral fringe of Gamma Ray Bursts](https://arxiv.org/abs/2602.06407)
*Chang-Yu Dai,Po-Yan Tseng*

Main category: astro-ph.HE

TL;DR: The study investigates femtolensing effects of gamma-ray bursts (GRBs) to constrain primordial black holes (PBHs) as dark matter candidates. Using Swift XRT data, spectral analyses show moderate evidence for PBH-induced fringes in some GRBs but lack robust constraints unless GRB sizes are under 5×10^7 meters for 5×10^-15 solar mass PBHs.


<details>
  <summary>Details</summary>
Motivation: To constrain the existence of PBHs as dark matter by analyzing femtolensing effects in GRB spectra, addressing the gap for PBH masses below 10^-13 solar masses.

Method: Applied wave optics formalism to GRB data from Swift XRT, statistically compared PBH lensing effects with BAND model spectra, analyzed spectral fringes indicative of femtolensing.

Result: A few GRBs showed spectral fringes suggestive of PBH lensing with improved goodness of fit, but most fittings did not significantly improve, leading to upper bounds on PBH abundance. Robust constraints require GRB sizes <5×10^7 m for specific PBH masses.

Conclusion: Femtolensing analysis provides moderate evidence for PBHs but cannot robustly constrain their abundance without tighter GRB size limits. Further observations are needed to refine constraints on PBHs as dark matter candidates.

Abstract: Femtolensing of gamma ray bursts (GRBs) are vastly studied to constrain the primordial black hole (PBHs) lighter than $10^{-13}$ solar mass and may close the window for PBH dark matter. In this case, wave optics formalism is required and carefully implemented in our analysis. Incorporating the GRB observational data from Swift XRT, we perform the statistic analysis of PBH lensing, comparing with null hypothesis where BAND model is used to parametrize the GRB spectrum. We found few GRB data manifest the spectral fringe which characterize the feature of femtolensing by PBHs, and the analysis shows moderate statistical preference in terms of goodness of fit. Conversely, since most of the fitting to GRB spectral data do not improved with PBH lensing, we utilize to obtain upper bound on the PBH fractional abundance with respect to dark matter. However, the robust constraint cannot be achieved, unless the size of GRBs are smaller than $5\times10^{7}$ m for PBH mass around $5\times10^{-15}$ solar mass.

</details>


### [24] [Super-knee cosmic rays from interacting supernovae](https://arxiv.org/abs/2602.06410)
*Nick Ekanger,Shigeo S. Kimura,Kazumi Kashiyama*

Main category: astro-ph.HE

TL;DR: The study investigates how massive stars' late-phase mass loss creates a dense circumstellar medium (CSM), which interacts with supernova ejecta post-collapse. This interaction accelerates nuclei to very high energies via diffusive shock acceleration, including magnetic field amplification and composition-dependent effects. The model shows that Type IIn supernovae could dominantly contribute to the super-knee cosmic-ray flux (10¹⁵–10¹⁷ eV) and aligns with LHAASO data, explaining the heavy-ion composition observed at these energies.


<details>
  <summary>Details</summary>
Motivation: To understand the origin of cosmic rays beyond the knee (few×10¹⁵ eV) and explain their heavy composition using observations like LHAASO, by linking it to supernova-CSM interactions and particle acceleration mechanisms.

Method: Developed a model incorporating CSM interactions, diffusive shock acceleration, non-resonant streaming instability-induced magnetic field amplification, heavy-ion abundance enhancement, and composition-dependent acceleration. Applied this to various supernova subclasses.

Result: Type IIn supernovae can account for a dominant portion of the observed super-knee cosmic-ray flux from ~few×10¹⁵ to few×10¹⁷ eV, consistent with LHAASO detections. The model also naturally explains the trend of increasingly heavy nuclei dominating at higher energies in this range.

Conclusion: Type IIn supernovae are plausible primary sources of the super-knee cosmic-rays, with their unique CSM environment and acceleration mechanisms sufficiently explaining both the energy spectrum and compositional changes observed at these extreme energies.

Abstract: There is increasing evidence that, in the very late phase of stellar evolution before core collapse, massive stars have winds with large mass loss rates that give rise to a dense circumstellar medium (CSM) surrounding the progenitor star. After core collapse, a shock wave forms when the supernova ejecta interacts with this CSM. In such an interaction, the nuclei in the CSM can undergo diffusive shock acceleration and reach very high energies. We consider such a model, which includes magnetic field amplification from the non-resonant streaming instability, enhancement to the abundance of heavy-ions, and composition-dependent acceleration. Applying this to several supernova subclasses, we find that IIn supernovae can supply a dominant fraction of the observed super-knee cosmic-ray (CR) flux from $\sim{\rm few}\times10^{15}\,{\rm eV}$ to $\sim{\rm few}\times10^{17}\,{\rm eV}$ and is consistent with recent LHAASO measurements above the CR knee. This systematic model also explains the increasingly heavy nuclear composition in this energy range.

</details>


### [25] [Studies on the spin and magnetic inclination evolution of magnetars Swift J1834.9-0846 under wind braking](https://arxiv.org/abs/2602.06615)
*Biaopeng Li,Zhifu Gao,Wenqi Ma,Weifeng Zhang,Quan Cheng,L. C. Garcia de Andrade*

Main category: astro-ph.HE

TL;DR: The paper proposes a unified spin-evolution model for the magnetar Swift J1834.9-0846 to explain its anomalous braking index and insufficient rotational energy loss. By integrating magnetic dipole radiation, gravitational waves, and wind braking, the model shows wind braking contributes 17-51% of the torque. The birth period is unconstrained, and a toroidal magnetic field is favored. Early gravitational-wave signals could be detectable by future observatories but require stable rotation, which is unlikely for young magnetars.


<details>
  <summary>Details</summary>
Motivation: To address the magnetar's deviation from canonical n=3 braking index and insufficient rotational energy to power X-ray emission, requiring a new model beyond traditional magnetic dipole radiation.

Method: A unified model combining magnetic dipole radiation, gravitational wave emission, and wind braking. Bayesian inference constrains parameters like wind braking coefficient (κ), precession cycles (ξ), and birth period. Analysis also assesses gravitational-wave detectability using aLIGO and ET projections.

Result: Wind braking contributes significantly (17-51%) to spin-down torque with κ between 13-37. Birth period is unconstrained/prior-dependent. Toroidal magnetic field configuration matches observed low braking index. Early gravitational-wave signals could marginally reach detectability thresholds for next-gen observatories under optimistic rotation stability assumptions.

Conclusion: The model provides a self-consistent explanation linking magnetar spin-down to internal magnetic structure and multi-messenger observations. Highlights importance of wind braking and toroidal fields, and identifies observational limits on magnetar birth properties and gravitational-wave detection prospects.

Abstract: The magnetar Swift J1834.9-0846 presents a significant challenge to neutron star spin-down models. It exhibits two key anomalies: an insufficient rotational energy loss rate to power its observed X-ray luminosity, and a braking index of $ = 1.08\pm 0.04$, which starkly contradicts the canonical magnetic dipole value of $n=3$. To explain these anomalies, we develop a unified spin-evolution model that self-consistently integrates magnetic dipole radiation, gravitational wave emission, and wind braking. Within this framework, we constrain the wind braking parameter to $κ\in [13, 37]$ from the nebular properties, finding it contributes substantially (17%-51%) to the current spin-down torque. Bayesian inference reveals that the birth period is poorly constrained by present data and is prior-dependent, indicating a millisecond birth is allowed but not required. Furthermore, we constrain the number of precession cycles to $ξ\sim 10^{4}$--$10^{5}$, and our analysis favors a toroidally-dominated internal magnetic field configuration as the most self-consistent explanation for the low braking index. Finally, we assess the continuous gravitational-wave detectability. The present-day signal is undetectable. However, the early-time signal might have reached the projected sensitivity of next-generation gravitational-wave observatories, such as the Advanced Laser Interferometer Gravitational-Wave Observatory (aLIGO) and the Einstein Telescope (ET), although a confident detection would require exceptionally stable rotation, an assumption considered highly optimistic for a young magnetar. This work establishes a unified framework that links magnetar spin-down with their interior physics and multi-messenger observables, providing a physically consistent interpretation for Swift J1834.9-0846 and a new tool for understanding similar extreme neutron stars.

</details>


### [26] [Rotational Behaviour of Exotic Compact Objects](https://arxiv.org/abs/2602.06660)
*Zakary Buras-Stubbs,Ilídio Lopes*

Main category: astro-ph.HE

TL;DR: The paper explores exotic compact objects made of self-interacting asymmetric fermionic dark matter with a repulsive Yukawa potential, comparing their structural, tidal, and rotational properties to neutron stars. It uses simulations under Hartle-Thorne formalism and discusses their distinguishability via gravitational waves.


<details>
  <summary>Details</summary>
Motivation: To understand the role of dark compact systems in gravitational wave astronomy, particularly addressing challenges in determining mass and radius from events without electromagnetic counterparts.

Method: Constructed models of solar mass dark matter systems using 1 GeV and 10 GeV fermionic dark matter. Analyzed static and rotating systems via Hartle-Thorne formalism to second order, comparing tidal deformability and rotational properties with SLy4 neutron stars.

Result: Highlighted differences in tidal deformability and rotational characteristics between dark objects and neutron stars, suggesting observational distinctions in gravitational wave signals for current/future detectors.

Conclusion: Dark compact objects exhibit distinct gravitational wave signatures that could be detectable, aiding in distinguishing them from neutron stars and informing dark matter models through gravitational wave astronomy.

Abstract: We construct exotic compact objects composed entirely of self-interacting asymmetric fermionic dark matter governed by a repulsive Yukawa potential with massive dark interaction boson. By considering the structural, tidal, and rotational properties of solar mass self-gravitating dark matter systems, and contrasting them against purely baryonic neutron stars, described by the well understood SLy4 equation of state, we hope to shed some light on the place of dark compact systems in the context of gravitational wave astronomy, specifically due to the difficulty parsing mass and radius data from events with no electromagnetic counterpart. Here we consider systems composed of 1 GeV and 10 GeV dark matter. Relevant compact objects are then analysed and simulated as both static bodies, and rotating systems governed by the Hartle-Thorne formalism to second order. Here within we highlight the differences in key tidal and rotational properties encoded in gravitational wave signals, and analyse how dark objects may mimic or distinguish themselves to current and future gravitational wave observatories.

</details>


### [27] [Sensipy: simulate gamma-ray observations of transient astrophysical sources](https://arxiv.org/abs/2602.06783)
*Jarred G. Green,Barbara Patricelli,Antonio Stamerra,Monica Seglar-Arroyo*

Main category: astro-ph.HE

TL;DR: Sensipy is an open-source Python toolkit designed to simulate observations of transient astrophysical sources in high- and very-high-energy gamma-ray ranges, aiding astronomers in optimizing observation strategies and assessing detectability under different emission models.


<details>
  <summary>Details</summary>
Motivation: The short-lived nature of explosive astrophysical events requires efficient observation scheduling to capture their energy emissions effectively. Sensipy helps test theoretical models and maximize telescope time use.

Method: Sensipy provides tools to simulate observations, estimate detection rates, and offer real-time scheduling insights by considering various theoretical emission mechanisms and assumptions.

Result: Enables astronomers to assess source detectability, justify observation feasibility, estimate annual detection rates for source classes, and optimize multi-messenger observational campaigns.

Conclusion: This toolkit improves the ability to plan and execute efficient observations of transient gamma-ray sources, bridging theoretical predictions with observational capabilities through simulation-based analysis.

Abstract: We present sensipy, an open-source Python toolkit for simulating observations of transient astrophysical sources, particularly in the high-energy (HE, keV-GeV) and very-high-energy (VHE, GeV-TeV) gamma-ray ranges. The most explosive events in our universe are often short-lived, emitting the bulk of their energy in a relatively narrow time window. Due to often rapidly fading emission profiles, understanding how and when to observe these sources is crucial both to test theoretical predictions and efficiently optimize available telescope time. The information extracted from the tools included in sensipy can be used to help astronomers investigate the detectability of sources considering various theoretical assumptions about their emission processes and mechanisms. This information can further help to justify the feasibility of proposed observations, estimate detection rates (events/year) for various classes of sources, and provide scheduling insight in realtime during gamma-ray and multi-messenger observational campaigns.

</details>


### [28] [Forecasting Supermassive Black Hole Binary Gravitational Wave Probes: Prospects for Future Pulsar Timing Array and Space-Borne Detectors](https://arxiv.org/abs/2602.06789)
*Katsunori Kusakabe,Yoshiyuki Inoue,Daisuke Toyouchi,Keitaro Takahashi*

Main category: astro-ph.HE

TL;DR: The paper evaluates the detection potential of future gravitational wave observatories (LISA, Taiji, TianQin, SKA-PTA) for supermassive black hole binaries using AGN data, predicting event rates and observing timescales.


<details>
  <summary>Details</summary>
Motivation: To assess how future GW detectors can improve SMBHB detection rates and properties characterization across different frequency bands.

Method: Framework using AGN fractions, X-ray luminosity functions, and SNR ≥5 thresholds; analyzes space-borne detectors and SKA-PTA configurations.

Result: Space-based detectors expect 1-20 annual events; SKA-PTA could detect first individual SMBHB GWs in years and reach 100-1000 sources in a decade.

Conclusion: Future observatories will revolutionize SMBHB studies, enabling population analysis and multi-band GW astronomy.

Abstract: We present a comprehensive framework for predicting the detection prospects of supermassive black hole binaries (SMBHBs) by future gravitational wave (GW) observatories, examining both space-borne detectors (LISA, Taiji, TianQin) and next-generation pulsar timing array (PTA) combined with the Square Kilometre Array (SKA-PTA). Leveraging dual active galactic nucleus (AGN) fractions and AGN X-ray luminosity functions, we systematically evaluate the detectable SMBHB populations with a detection threshold of signal-to-noise ratio $\geq 5$ for each GW observatory. Our analysis reveals that space-borne detectors are expected to identify approximately $\sim 1 \text{--} 2$ to $\sim 20$ events per year, depending on the SMBHB orbital evolution prescriptions. On the other hand, SKA-PTA demonstrates the potential to reach the first GW detection from individual SMBHBs within a few years of observation and achieve detectable GW source counts of $10^2 \text{--} 10^3$ after about 10 years, depending on PTA configurations. These facilities will significantly improve SMBHB detectability and enable characterization of their properties across different frequency bands.

</details>


### [29] [Merger Driven or Internal Evolution? A New Morphological Study of Tidal Disruption Event Host Galaxies](https://arxiv.org/abs/2602.06839)
*Janet N. Y. Chang,Connor Bottrell,Lixin Dai,Rudrani Kar Chowdhury,Meng Gu,Renbin Yan,Leonardio Ferreira,Sara L. Ellison,Scott Wilkinson,Thomas de Boer*

Main category: astro-ph.HE

TL;DR: The study of 14 TDE host galaxies using advanced imaging and machine learning found they are more centrally concentrated but lack recent merger signs. Instead, bar/ring structures are common, suggesting secular evolution via bars, especially for green valley galaxies.


<details>
  <summary>Details</summary>
Motivation: To investigate whether TDE host galaxies' properties are due to recent mergers or other processes, given their central concentration and green valley associations.

Method: Analyzed r-band images from SDSS, DECaLS, UNIONS; used traditional morphology methods and ML-driven merger diagnostics on 14 TDE hosts vs control galaxies.

Result: TDE hosts 16% more concentrated than controls but no merger activity detected. 1.5-2.5x higher likelihood of bar/ring structures, up to 3x in green valley galaxies.

Conclusion: Secular evolution driven by bars, not mergers, explains TDE hosts' properties and high TDE rates in green valley galaxies.

Abstract: Host galaxies of tidal disruption events (TDEs) show enhanced central stellar concentration and are preferentially found in post-starburst and green valley populations. This connection has led to the proposal that TDE host galaxies likely have gone through recent mergers. We conduct a new morphological study of 14 TDE host galaxies, using the r-band images from Sloan Digital Sky Survey (SDSS), Dark Energy Camera Legacy Survey (DECaLS), and Ultraviolet Near-Infrared Optical Northern Survey (UNIONS), with the images from the latter two surveys having much higher depth and resolution than SDSS. We examine galaxy structures using conventional methods and also apply diagnostics of merger activity from machine learning models. Consistent with previous studies, our results show that TDE host galaxies are ~16% more centrally concentrated when compared to non-TDE-host controls. However, surprisingly, TDE hosts lack any indication of recent merger activity from both morphological analysis and our machine learning merger classifier. Instead, our results reveal that TDE host galaxies are approximately 1.5 to 2.5 times more likely to have bar-like or ring-like structures than their controls. This enhancement is even more prominent for TDEs in the green valley, with the factor reaching almost 3. Based on these results, we propose that bar-driven secular evolution, instead of merger, likely dominates the recent evolution of TDE hosts found in the green valley, which can simultaneously explain their distinctive nuclear properties and enhanced TDE rates.

</details>


### [30] [Optical spectral characterization of OP 313. Constraining the contribution of thermal and non-thermal optical emission](https://arxiv.org/abs/2602.06919)
*J. Otero-Santos,M. Nievas Rosillo,J. A. Acosta-Pulido,R. Clavero*

Main category: astro-ph.HE

TL;DR: The discovery of quasar OP 313 in very-high-energy gamma rays prompted an analysis of its emission components. New spectroscopic data from NOT and TNG telescopes revealed stable Mg II line luminosities, indicating constant thermal components (broad line region, accretion disk, dusty torus) and a BH mass of ~10^8.36 solar masses. The accretion rate and high Compton dominance suggest an FSRQ-like nature rather than a changing-look quasar.


<details>
  <summary>Details</summary>
Motivation: To characterize the external photon fields (accretion disk, broad line region, dusty torus) during high-activity states of quasar OP 313, investigate variability and blurring effects on emission lines, and address the lack of updated measurements needed for interpreting gamma-ray production linked to accretion processes.

Method: New spectroscopic observations using NOT and TNG telescopes were compared with archival SDSS data. Mg II emission line flux and equivalent width were measured to assess continuum variability. Luminosities of thermal components (BLR, disk, torus) were derived from stable Mg II line parameters, and BH mass was estimated via Mg II and C III] lines.

Result: Stable Mg II luminosity implies constant thermal components: log(L_BLR)=44.91, log(L_disk)=45.91, log(L_torus)=44.70. BH mass ~8.36 solar masses. Accretion rate λ=0.23 suggests FSRQ-like Compton dominance, contradicting previous 'changing-look' classification.

Conclusion: Observations favor a radio-loud FSRQ nature for OP 313 over changing-look behavior. Constant thermal luminosities and high Compton dominance indicate accretion processes aligned with blazar properties, emphasizing the need for multi-wavelength studies to fully understand gamma-ray production mechanisms.

Abstract: The quasar OP 313 was discovered in December 2023 in very-high-energy $γ$ rays above 100 GeV, enabling for the first time a complete characterization of its emission. However, the lack of updated measurements of its accretion disk, broad line region and dusty torus hampers a detailed interpretation of the role of accretion in the observed $γ$-ray production. We intend to characterize, during high-activity states, the external photon fields contributing to the IR-to-UV emission$-$namely dusty torus, broad line region and accretion disk$-$and investigate potential variability and blurring effects on the broad emission lines. We present new spectroscopic observations of OP 313 with the NOT and TNG telescopes to characterize its optical spectrum and variability with respect to archival observations from SDSS. We measure the luminosity of different broad emission lines, characterizing the broad line region, accretion disk and dusty torus. We measure the Mg II emission line, with an average flux of $F_{\mathrm{Mg \ II}} = (0.85 \pm 0.11)\times 10^{-14}$ erg cm$^{-2}$ s$^{-1}$. Its equivalent width and luminosity are consistent with a constant line with a variable non-thermal continuum. From the stable Mg II line we derive a constant luminosity of the thermal components, $\log(L_{\mathrm{BLR}} \ \mathrm{[erg \ s^{-1}]}) = 44.91 \pm 0.19$, $\log(L_{\mathrm{disk}} \ \mathrm{[erg \ s^{-1}]}) = 45.91 \pm 0.19$ and $\log(L_{\mathrm{torus}} \ \mathrm{[erg \ s^{-1}]}) = 44.70 \pm 0.16$, and estimated a BH mass of $\log(M_{BH}/M_{\odot})=8.36 \pm 0.18$, in line with with that derived from the C III] line. These characteristics and the indicator of the accretion rate from the disk/Eddington luminosity ratio $λ=L_{AD}/L_{Edd} = 0.23 \pm 0.10$, along with a high Compton dominance, favour a FSRQ-like nature, contrary to the argued changing-look nature.

</details>


### [31] [Mapping plasma properties of Cassiopeia A with XRISM/Resolve: a Bayesian analysis via UltraSPEX](https://arxiv.org/abs/2602.06952)
*Manan Agarwal,Jacco Vink,Liyi Gu,Paul P. Plucinsky,Aya Bamba,Toshiki Sato,Daniele Rogantini,Yuken Ohshiro*

Main category: astro-ph.HE

TL;DR: The study provides the first microcalorimeter-based plasma parameter maps of Cassiopeia A (Cas A) using XRISM/Resolve data, revealing insights into its ejecta structure, abundance ratios, and kinematics. Key findings include higher Ni/Fe ratios in the NE jet, velocity differences between iron-group and intermediate-mass elements, and evidence for ejecta clumping and nonthermal radiation dominance in certain regions.


<details>
  <summary>Details</summary>
Motivation: To map the physical conditions of shocked plasma in young SNRs, specifically Cas A, to understand explosion mechanisms, ejecta structure, and asymmetries.

Method: 采用了超过350千秒的XRISM/Resolve高光谱分辨率观测数据，将Cas A划分为1'×1'区域，拟合光谱为两组纯金属抛射物成分（中等质量元素和铁族元素）加上非热同步辐射。使用UltraSPEX贝叶斯框架，结合SPEX等离子体代码和UltraNest嵌套采样算法，进行参数后验分布和退化探索。

Result: 发现了增强的Ar/Si和Ca/Si丰度比在硅富集喷流基部，东北喷流基部的Ni/Fe质量比为0.08±0.015。铁族元素的多普勒速度和展宽高于中等质量元素，最大差异约为800和1200 km/s。电离时间和电子温度呈反相关，可能由抛射物的聚块结构（铁族 overdensity ~10，中等质量元素高达~100）和较低的逆向激波速度解释。非热辐射在4-6 keV占47%以上，在西部区域主导且光谱硬化。

Conclusion: 这些发现表明Cas A的不对称喷射和内部结构复杂性，支持模型中的高密度聚块和非热辐射分布，需进一步结合数值模拟和观测研究爆炸机制。

Abstract: Mapping the physical conditions of the shocked plasma of young supernova remnants (SNR) is crucial for understanding their explosion mechanisms, ejecta structure, and large-scale asymmetries. Using $>350$ ks of XRISM/Resolve high spectral resolution observations of Cassiopeia A (Cas A), the youngest known Galactic core-collapse SNR, we present the first microcalorimeter-based plasma parameter maps of any SNR. We tessellate Cas A into $1'\times1'$ regions and fit the broadband spectra as thermal emission from two pure-metal ejecta components -- corresponding to intermediate-mass elements (IMEs) and iron-group elements (IGEs) -- plus nonthermal synchrotron radiation. For robust inference, we introduce UltraSPEX, a Bayesian framework that couples the SPEX plasma code with the UltraNest nested-sampling algorithm, yielding full posterior distributions and exploration of parameter degeneracies. Key findings include enhanced Ar/Si and Ca/Si abundance ratios near the base of the Si-rich jets, and a high Ni/Fe mass ratio ($0.08\pm0.015$) in the base of NE jet. IGEs ejecta exhibit systematically higher Doppler velocities and broadenings than IMEs ejecta in most regions, with maximum differences of $\sim800$ km/s and $\sim1200$ km/s, respectively; Ca shows distinct (faster) kinematics from other IMEs in several SE regions. The ionization timescale and electron temperature show a robust anti-correlation, particularly for IGEs. This relation and measured parameter values could be explained by semi-analytical models with significant ejecta clumping (overdensities of $\sim10$ for IGEs and up to $\sim100$ for IMEs) and reduced historical reverse-shock velocities. Nonthermal emission accounts for a substantial fraction, with at least 47% of the 4--6 keV continuum and dominates in the western regions, where the spectrum hardens.

</details>


### [32] [Spectral Appearance of Self-gravitating AGN Disks Powered by Stellar Objects: Universal Effective Temperature in the Optical Continuum and Application to Little Red Dots](https://arxiv.org/abs/2602.06954)
*Yi-Xian Chen,Hanpu Liu,Ruancun Li,Bingjie Wang,Yilun Ma,Yan-Fei Jiang,Jenny Greene,Eliot Quataert,Jeremy Goodman*

Main category: astro-ph.HE

TL;DR: The paper proposes a new model where self-gravitating accretion disks around supermassive black holes exhibit a universal outer temperature of ~4000-4500K, matching the properties of Little Red Dots (LRDs). This 'disk Hayashi limit' decouples the disk's optical continuum temperature from accretion rate, mass, and viscosity. The model explains LRD emission via stellar heating in the outer disk and suppressed UV/X-ray variability, suggesting an evolutionary link between LRDs and AGNs when accretion parameters cross a critical threshold.


<details>
  <summary>Details</summary>
Motivation: To address inconsistencies in previous disk models of LRDs that required fine-tuned parameters and lacked a physical basis for their optical properties. The authors aim to provide a unified framework connecting LRDs with AGNs by considering self-gravitating disks and stellar contributions to disk emission.

Method: The authors use dust-poor opacity tables to simulate optically thick accretion disks, incorporating radial variations in accretion rates and self-gravity effects. They analyze the impact of stellar populations embedded in the disk on heating the outer regions and hollowing the inner disk, suppressing UV/X-ray variability. Global disk models are constructed to map parameter spaces defining LRD and AGN regimes.

Result: The derived 'disk Hayashi limit' establishes a universal ~4000-4500K outer temperature independent of key parameters. This model explains LRD optical emission without parameter tuning, identifies a critical accretion-to-viscosity threshold (~0.1 solar masses/yr) for LRD existence, and predicts transitions to non-self-gravitating AGNs with increased metallicity and dust.

Conclusion: The study demonstrates that self-gravitating accretion disks naturally explain LRD characteristics and their connection to AGNs, offering a physically motivated evolutionary pathway. It predicts observational signatures like non-variable UV stellar emission and far-infrared transitions, providing testable criteria for distinguishing disk phases in high-redshift sources.

Abstract: We revisit the spectral appearance of extended self-gravitating accretion disks around supermassive black holes. Using dust-poor opacity tables, we show that all optically thick disk solutions possess a universal outer effective temperature of $T_{\rm eff}\sim 4000-4500$K, closely resembling compact, high-redshift sources known as Little Red Dots (LRDs). Assuming the extended disk is primarily heated by stellar sources, this ``disk Hayashi limit" fixes the dominant optical continuum temperature of the disk spectrum independent of accretion rate $\dot{M}$, black hole mass $M_\bullet$, and disk viscosity $α$, and removes the parameter-tuning required in previous disk interpretations of LRDs. We construct global self-gravitating accretion disk models with radially varying accretion rates, suggesting that burning of embedded stellar objects can both efficiently power the emission of the outer disk and hollow out the inner disk, strongly suppressing variable UV/X-ray associated with a standard quasar. The resulting disk emission is dominated by a luminous optical continuum while a separate, non-variable UV component arises from stellar populations on the nuclear to galaxy scale. We map the optimal region of parameter space for such systems and show that LRD-like appearances are guaranteed for $\dot{M}/α\gtrsim 0.1 M_\odot /{\rm yr}$, a threshold insensitive to $M_\bullet$, below which the system may transition into classical non-self-gravitating AGN disks, potentially a later evolution stage. We expect this transition to be accompanied by the enhancement of metallicity and production of dust, giving rise to far infrared emission. This picture offers a physically motivated and quantitative framework connecting LRDs with AGNs and their associated nuclear stellar population.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [33] [Dark Matter Heating of Compact Stars Beyond Capture: A Relativistic Framework for Energy Deposition by Particle Beams](https://arxiv.org/abs/2602.06111)
*Jaime Hoefken Zink,Shihwen Hor,Maura E. Ramirez-Quezada*

Main category: hep-ph

TL;DR: The paper develops a general relativistic framework to compute particle interactions with compact objects like neutron stars and white dwarfs, focusing on directed beams from astrophysical sources like blazars. It analyzes how gravitational focusing and energy deposition can lead to observable heating effects.


<details>
  <summary>Details</summary>
Motivation: Existing models assume isotropic dark matter fluxes but realistic sources produce directional beams requiring consistent treatment of gravitational focusing and energy deposition.

Method: General relativistic formalism using geodesic congruences to map asymptotic particle flux to local densities, incorporating capture probability, multi-stream regions, and optical depth effects for arbitrary particles and interaction models.

Result: Applied to 324 blazar-produced boosted dark matter beams interacting with white dwarfs and neutron stars, demonstrating efficient energy deposition and heating under specific conditions.

Conclusion: Compact objects can act as detectors for directional particle beams, with the framework identifying scenarios where their heating signatures could be observable.

Abstract: Compact astrophysical objects, such as neutron stars and white dwarfs, can act as detectors of energetic particle fluxes originating from astrophysical accelerators. While most existing capture and heating calculations assume isotropic very low energetic incident fluxes from the halo dark matter, many realistic sources produce highly directional beams or jets, for which gravitational focusing, trajectory multiplicity, and local energy deposition must be treated consistently. In this work, we develop a general relativistic formalism to compute the local density, capture probability, and energy deposition of particles arriving as directed beams onto compact objects. The framework is based on the mapping of an asymptotic particle flux to local densities through geodesic congruences, allowing for gravitational focusing, multi-stream regions, and optical depth effects to be incorporated in a unified way. The formalism applies to arbitrary particle species and interaction models, and separates capture from through-going energy deposition in a frame-consistent manner. As an explicit application, we consider relativistic particle beams generated in astrophysical jets and evaluate their interaction with two compact objects samples: a white dwarf and a neutron star. In particular, we illustrate the framework using boosted dark matter produced in a list of 324 blazars as a representative case study, computing the resulting fluxes and the associated heating in the selected stars. Additional regimes such as the interaction roof and geometric limit are discussed, highlighting the conditions under which compact objects can efficiently convert incident beam energy into observable heating.

</details>


### [34] [Scalar Tsunamis from Black Hole Formation](https://arxiv.org/abs/2602.06112)
*Arturo de Giorgi,Yeray Garcia del Castillo,Joerg Jaeckel*

Main category: hep-ph

TL;DR: The paper examines the release of scalar fields around stars collpasing into black holes, incorporating general relativity and improved initial field modeling, finding energy release similar to flat space estimates but with modified spectra.


<details>
  <summary>Details</summary>
Motivation: To improve previous estimates of scalar field configurations at large distances by accounting for general relativity and better initial conditions modeling.

Method: Incorporating general relativistic effects and enhanced modeling of initial field configurations to analyze scalar field release when massive objects collapse into black holes.

Result: The total energy released is comparable to flat space estimates, but the spectral distribution shows noticeable corrections from previous calculations.

Conclusion: General relativistic effects and accurate initial modeling refine our understanding of scalar field emission, indicating significant spectral changes while energy scales remain consistent with earlier flat space approximations.

Abstract: Stars and other macroscopic objects may be surrounded by potentially large field configurations of very light scalars coupled to ordinary matter. If the star ends in a black hole, e.g. via a supernova or a neutron star merger, the source vanishes, and the field is released. In this paper, we improve on previous estimates for the field configurations arriving at large distances by including the effects of general relativity and an improved modelling of the initial field configurations. The total amount of energy released is typically of the same order of magnitude as suggested by simple flat space estimates. The spectrum receives noticeable corrections.

</details>


### [35] [Probing Extended Higgs Sectors via Multi-Top Events from Higgs Pair Decays in 2HDM Type-I at the HL-LHC](https://arxiv.org/abs/2602.06217)
*Ijaz Ahmed,M. Ibad,Farzana Ahmad,Jamil Muhammad*

Main category: hep-ph

TL;DR: The paper proposes using multi-top quark final states in the context of the Two Higgs Doublet Model (2HDM) Type-I to probe new physics at the High-Luminosity LHC. It focuses on production processes involving up to four top quarks and analyzes sensitivity to an extended Higgs sector under specific benchmark conditions.


<details>
  <summary>Details</summary>
Motivation: To explore new physics beyond the Standard Model by investigating high-multiplicity top quark events, leveraging the unique coupling of top quarks to scalars in the 2HDM framework.

Method: Simulates $pp$ collisions at 14 TeV for processes like $t\bar{t}H$, $t\bar{t}A$, $HA$, $HH^{±}$, $AH^{±}$ using benchmark points with degenerate Higgs masses (500 GeV) and varying $\tanβ$. Event selection applies strict jet cuts to reduce SM backgrounds.

Result: Signal significance surpasses $5σ$ at 4000 $fb^{-1}$ integrated luminosity across all channels, demonstrating the HL-LHC's sensitivity to the extended Higgs sector under the alignment limit conditions.

Conclusion: Multi-top final states in 2HDM Type-I provide a viable discovery channel for new physics at HL-LHC, especially with higher luminosity, suggesting robust detection potential for extended Higgs sectors.

Abstract: This study investigates the production of multi-top quark events final states containing up to four top quarks as a probe for new physics beyond the Standard Model (SM) within the framework of the Two Higgs Doublet Model (2HDM) Type-I. Focusing on the High-Luminosity Large Hadron Collider (HL-LHC) at a center-of-mass energy of $\sqrt{s} = 14$TeV, we analyze associated production processes including $pp \to t\bar{t}H$, $pp \rightarrow t\bar{t}A$, $pp \rightarrow HA$, $pp \rightarrow HH^{\pm}$, and $pp \rightarrow AH^{\pm}$. The simulation pipeline incorporates theoretical constraints and branching ratios. By targeting these high-multiplicity final states, the research aims to establish the sensitivity of the HL-LHC to an extended Higgs sector, utilizing the top quark's unique coupling to the scalar field as a primary discovery channel. The analysis is centered on two benchmark points characterized by a degenerate mass spectrum ($m_H = m_A = m_{H^{\pm}} = 500$GeV) in the alignment limit ($\sin(β- α) \to 1$), with $\tanβ$ varied between 1 and 3. Event selection was performed with stringent cuts on jet multiplicity to suppress dominant SM backgrounds, such as $t\bar{t}W$, $t\bar{t}Z$, and $WWZ$. Our results indicate that signal significance improves substantially as the integrated luminosity increases from $3000~fb^{-1}$ to $4000~fb^{-1}$, with all investigated channels exceeding the $5σ$ discovery threshold.

</details>


### [36] [Transeverse-Momentum Subtraction for Semi-Inclusive Deep-Inelastic Scattering](https://arxiv.org/abs/2602.06364)
*Jun Gao,Hai Tao Li,Hua Xing Zhu,Yu Jiao Zhu*

Main category: hep-ph

TL;DR: The paper presents a transverse-momentum subtraction method to efficiently calculate higher-order QCD corrections for identified hadron production in Semi-Inclusive Deep-Inelastic Scattering, demonstrating its effectiveness at next-to-next-to-leading order and applicability up to next-to-next-to-next-to-leading order for both unpolarized and polarized processes.


<details>
  <summary>Details</summary>
Motivation: To precisely study the proton's 3D momentum and spin structure through SIDIS, requiring accurate calculations of QCD corrections to improve fragmentation function analyses in global studies.

Method: Development of a transverse-momentum subtraction approach tailored for detected hadrons, enabling efficient computation of higher-order QCD corrections in SIDIS processes.

Result: Successful next-to-next-to-leading order QCD calculations with fully differential phenomenological applications, providing crucial inputs for global analyses of fragmentation functions.

Conclusion: The proposed method is robust for precision SIDIS studies, extending applicability to higher orders and both unpolarized/polarized scenarios, thereby enhancing QCD-based proton structure investigations.

Abstract: Semi-Inclusive Deep-Inelastic Scattering provides unique access to the three-dimensional momentum and spin structure of the proton, enabling precise studies of parton dynamics and hadronization in QCD. We present a transverse-momentum subtraction approach applied to the detected hadron that enables efficient and precise calculation of higher-order QCD corrections to identified hadron production in Semi-Inclusive Deep-Inelastic Scattering. We demonstrate the success of the method through a next-to-next-to-leading order QCD calculation and provide fully differential phenomenological applications, which provide important ingredients for global analyses of fragmentation functions. Our method is applicable to next-to-next-to-next-to-leading order QCD corrections for both unpolarized and polarized semi-inclusive deep-inelastic scattering.

</details>


### [37] [CoLLM: AI engineering toolbox for end-to-end deep learning in collider analyses](https://arxiv.org/abs/2602.06496)
*W. Esmail,A. Hammad,M. Nojiri*

Main category: hep-ph

TL;DR: The paper introduces \coll, a tool that uses large language models to generate analysis code and automate deep learning workflows in collider experiments, while providing a GUI to reduce the need for programming expertise.


<details>
  <summary>Details</summary>
Motivation: To lower the coding burden and simplify technical complexity in modern collider analyses, which require intricate event selections and advanced deep learning techniques.

Method: \coll leverages pretrained large language models to generate physically consistent analysis code for event selection and automates subsequent deep learning analyses. It also includes a graphical user interface for end-to-end analysis without programming expertise.

Result: The system reduces reliance on programming or deep learning expertise, streamlining collider analysis workflows.

Conclusion: \coll effectively accelerates and automates scientific workflows in collider experiments, making advanced analyses more accessible to non-experts.

Abstract: Recent improvements in large language models have opened new opportunities for accelerating and automating scientific workflows. In parallel, modern collider analyses are becoming increasingly complex and demand substantial programming and deep learning expertise. \coll alleviates this workload by using pretrained large language models to generate physically consistent analysis code for event selection. Additionally, it automates subsequent deep learning analyses. To further reduce reliance on programming or deep learning experience, \coll provides a graphical user interface that allows users to perform end-to-end analyses through an interactive interface. The main motivation behind \coll is to lower the coding burden and simplify the technical complexity of collider analyses, which increasingly depend on sophisticated event selections and advanced deep learning methods.

</details>


### [38] [CP violation angles from H$\toττ$ decays at FCC-ee](https://arxiv.org/abs/2602.06635)
*Sofia Giappichini,Markus Klute,Matteo Presilla*

Main category: hep-ph

TL;DR: The paper proposes using the FCC-ee to precisely measure CP violation in Higgs-fermion interactions through $H\toττ$ decay in $ZH$ production at 240 GeV. It achieves a ±2.5° precision on $φ_{ττ}$ and compares results with EFT and anomalous-coupling frameworks.


<details>
  <summary>Details</summary>
Motivation: To measure CP violation in Higgs-fermion couplings with high precision by leveraging the clean environment of FCC-ee's $e^+e^-$ collisions and τ spin correlations.

Method: Analysis of $ZH\to Hττ$ events at √s=240 GeV using anomalous-coupling parametrization and Effective Field Theory (EFT) framework, focusing on τ spin correlations for CP sensitivity.

Result: Projected precision of Δφ_{ττ}=±2.5° at 68% CL, with dominant contributions from hadronic τ decays. Expected limits on CP-odd operators derived and compared to other measurements.

Conclusion: FCC-ee provides unique capability to probe Higgs CP properties via ττ decays. The approach shows synergy between EFT interpretations and alternative coupling frameworks for constraining new physics.

Abstract: Measuring charge-parity (CP) violation in Higgs-fermion interactions is a key target of future precision Higgs programs. The decay $H\toττ$ is particularly sensitive to the CP structure of the Higgs Yukawa coupling via $τ$ spin correlation, while the clean environment of $e^+e^-$ collisions at the Future Circular Collider (FCC) enables accurate reconstruction of CP-sensitive observables. In this letter, we study the sensitivity of FCC-ee to the Higgs CP state using $H\toττ$ events produced in associated $ZH$ production at $\sqrt{s}=240$ GeV. In the anomalous-coupling parametrization, we project a precision of $Δφ_{ττ}=\pm 2.5^\circ$ at 68 % CL, with the dominant contribution arising from one-prong hadronic $τ$ decays. We further interpret the analysis in the Effective Field Theory framework, deriving expected limits on the CP-odd operators, and compare them with electron and magnetic dipole moment measurements and the anomalous-coupling approach.

</details>


### [39] [Chiral phase transition with primordial black holes: Distinct phase structure and catalysis](https://arxiv.org/abs/2602.06661)
*Masanori Tanaka,Jun-Chen Wang,Jing-Jun Zhang*

Main category: hep-ph

TL;DR: PBHs influence chiral phase transitions, altering gravitational-wave signals through enhanced symmetry restoration and unique phase patterns.


<details>
  <summary>Details</summary>
Motivation: To investigate how primordial black holes (PBHs) affect the chiral phase transition and its resulting gravitational-wave signals.

Method: Used the three-flavor Nambu-Jona-Lasino model to compute the chiral effective potential in a Schwarzschild spacetime, analyzed bounce solutions, and studied phase structures near PBH event horizons.

Result: PBHs accelerate chiral symmetry restoration, create local phase transitions with mixed second and first order features, and significantly boost the inverse duration parameter β/H, leading to notable shifts in GW peak frequency and amplitude.

Conclusion: PBHs act as catalysts for chiral phase transitions, even a small PBH population can produce measurable effects on gravitational-wave spectra, offering a new probe for PBH existence and phase transition dynamics.

Abstract: We study the impact of primordial black holes (PBHs) on the chiral phase transition and its associated stochastic gravitational-wave (GW) signals. Using the three-flavor Nambu-Jona-Lasinio model, we construct the chiral effective potential in a Schwarzschild spacetime background. We find that PBHs promote chiral symmetry restoration and induce a nontrivial local phase structure in the vicinity of the event horizon simultaneously. In particular, this structure exhibits a novel chiral symmetry breaking pattern involving both second- and first-order phase transitions, a feature absent in flat spacetime. We further demonstrate that PBHs act as genuine catalysts for the chiral phase transition by analyzing the bounce solution in curved spacetime. The presence of PBHs substantially enhances the inverse duration parameter $β/H$ while leaving the overall transition strength comparable to that in flat spacetime. As a consequence, even a small population of PBHs can induce $\mathcal{O}(1)$ shifts in both the peak frequency and the peak amplitude of the GW spectrum generated by the first-order dark chiral phase transition.

</details>


### [40] [Return of the CHAMPs: A clockwork portal to charged dark matter](https://arxiv.org/abs/2602.06681)
*Debajyoti Choudhury,Vineet K. Jha,Suvam Maharana*

Main category: hep-ph

TL;DR: The paper explores the feasibility of a charged massive particle (CHAMP) as dark matter within the clockwork paradigm, addressing naturalness concerns and outlining experimental tests.


<details>
  <summary>Details</summary>
Motivation: To investigate if a charged dark matter particle can exist naturally without requiring finely tuned parameters, despite strict phenomenological constraints.

Method: The authors use the clockwork mechanism to construct a model where CHAMP's charge arises naturally, then analyze it against theoretical and experimental constraints.

Result: The model allows for a viable parameter space for charged DM, avoiding unnatural fine-tuning, and identifies testable predictions at the LHC and future detection experiments.

Conclusion: Charged dark matter is a viable candidate within the clockwork framework, offering a natural solution and opening avenues for experimental verification.

Abstract: While Dark Matter (DM) is conventionally assumed to be chargeless, the possibility of a charged massive particle (CHAMP) as the DM particle remains alive. With phenomenological constraints on the charge being very severe, such a scenario is often sought to be dismissed, citing naturalness. We demonstrate here that such a (mini)charged DM can be realized within the clockwork paradigm, without the need to invoke unnaturally small parameters. The model is examined against constraints, theoretical and experimental, and the phenomenologically admissible parameter space is delineated. Several intriguing tests, at the LHC as well as at future direct and indirect detection experiments, are pointed out.

</details>


### [41] [C-parity and Regge Intercepts](https://arxiv.org/abs/2602.06683)
*Vladimir A. Petrov*

Main category: hep-ph

TL;DR: The paper derives constraints on the intercepts of C-parity definite Regge pole trajectories by analyzing how C-even and C-odd exchanges influence elastic scattering forces between hadrons.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between Regge pole trajectory intercepts and the nature of nuclear forces (attraction/repulsion) in hadronic elastic scattering based on C-parity considerations.

Method: Apply the rule that C-even exchanges correspond to attractive forces in hadron-hadron scattering, while C-odd exchanges lead to attraction in particle-antiparticle and repulsion in particle-particle scattering to constrain Regge trajectory intercepts.

Result: Establishes quantitative constraints on the intercept values of Regge poles with definite C-parity by linking their exchange effects to observed scattering force behaviours.

Conclusion: The analysis provides a theoretical framework connecting Regge trajectory parameters to fundamental interactions in hadron dynamics through C-parity based force interpretations.

Abstract: In this work, we obtain constraints on the intercepts of the Regge pole tra jectories (which we believe to have definite C-parity) based on the rule that C-even exchanges correspond to attractive forces between hadrons undergo ing elastic scattering, and C-odd exchanges correspond to attractive forces in the case of particle-antiparticle scattering and repulsive forces in the case of particle-particle scattering.

</details>


### [42] [Physics of strong electromagnetic fields in relativistic heavy-ion collisions](https://arxiv.org/abs/2602.06697)
*Koichi Hattori*

Main category: hep-ph

TL;DR: The paper explores the roles of strong electromagnetic fields in relativistic heavy-ion collisions, emphasizing their importance for understanding quark-gluon plasma dynamics and electromagnetic processes in ultraperipheral collisions.


<details>
  <summary>Details</summary>
Motivation: To advance theoretical and experimental understanding of quark-gluon plasma dynamics and electromagnetic processes in ultraperipheral collisions through the study of strong electromagnetic fields generated in heavy-ion collisions.

Method: Analyzes the various roles played by strong electromagnetic fields in relativistic heavy-ion collision environments, focusing on their impact on QGP properties and ultraperipheral collision phenomena.

Result: Highlights the necessity for further theoretical frameworks and experimental efforts to fully comprehend the interplay between electromagnetic fields and QGP dynamics in both central and peripheral collisions.

Conclusion: Future developments in theory and experiments are crucial to unravel the complex dynamics involving strong electromagnetic fields and their role in QGP behavior and ultraperipheral processes.

Abstract: I discuss several roles of the strong electromagnetic fields created by relativistic heavy-ion collisions. These phenomena call for theoretical and experimental developments to understand dynamics of quark-gluon plasma (QGP) as well as purely electromagnetic processes in the ultraperipheral collisions.

</details>


### [43] [Spin Light of neutrino in polarized matter](https://arxiv.org/abs/2602.06708)
*Alexander Grigoriev,Alexei Ternov*

Main category: hep-ph

TL;DR: The paper explores how the spin light of neutrinos (SLν) is affected by matter polarization in neutron stars, showing that polarization can either enhance or suppress this radiation and creates an observable asymmetry, potentially aiding the study of NS and magnetars' internal structures.


<details>
  <summary>Details</summary>
Motivation: To investigate the impact of net matter polarization on neutrino spin light (SLν) properties, leveraging the possibility of nuclear matter spin-polarization in dense astrophysical environments.

Method: The study analyzes SLν radiation under conditions of spin-polarized matter, examining how such polarization influences radiation intensity and directional asymmetry in dense stellar environments like neutron stars.

Result: Matter polarization was found to either enhance or completely suppress SLν radiation, while introducing a characteristic asymmetry in the total radiation output from compact objects, dependent on both polarization and magnetic field properties.

Conclusion: These findings suggest that observing SLν radiation could offer insights into the internal structure of neutron stars and magnetars, advancing our understanding of dense matter physics and astrophysical environments.

Abstract: The Spin Light of neutrino ($SLν$) is an electromagnetic radiation of the neutrino magnetic moment emitted when neutrino moves in external conditions (fields or matter). The effect can be of significance in the extremely dense matter of compact astrophysical objects such as neutron stars (NS). If detected, this radiation could provide a fair opportunity to study the properties of neutrinos and the medium through which they move, since the properties of the radiation depend on both. Motivated by the possibility of the nuclear matter spin-polarization, in this paper, we study the new properties to $SLν$ obtained under the influence of net matter polarization. We demonstrate that the polarization can enhance or completely suppress the radiation. Also, it introduces a characteristic asymmetry into the total radiation from the compact object, which could be an observable feature dependent on the matter polarization and the magnetic field inside the stellar (if the field is connected to the stellar matter polarization). The research may have implications for the physics of NS and magnetars, bringing us closer to the possibility of studying their internal structure.

</details>


### [44] [Cavity, lumped-circuit, and spin-based detection of axion dark matter: differences and similarities](https://arxiv.org/abs/2602.06726)
*Deniz Aybas,Hendrik Bekker,Dmitry Budker,Wei Ji,On Kim,Younggeun Kim,Derek F. Jackson Kimball,Jia Liu,Xiaolin Ma,Chiara P. Salemi,Yannis K. Semertzidis,Alexander O. Sushkov,Kai Wei,Arne Wickenbrock,Yuzhe Zhang*

Main category: hep-ph

TL;DR: The paper presents a comparative review of various haloscope concepts for detecting axion-like dark matter, focusing on signal generation, noise, analysis, and scanning strategies, providing a unified framework to optimize future experiments.


<details>
  <summary>Details</summary>
Motivation: To establish a common framework for comparing different haloscope classes and clarify their sensitivities across mass and coupling regimes for axion detection.

Method: Summarizes key properties of ultralight dark matter, analyzes four haloscope types (cavity, Earth-scale, lumped-element, spin), compares their signalshapes, noise sources, SNR definitions, and scanning strategies.

Result: Unified framework clarifies shared concepts and essential differences between haloscope classes, identifies optimal scan strategies based on coherence, bandwidth, and noise.

Conclusion: The framework synthesizes current methods and provides guidance for optimizing future haloscope experiments in dark matter searches.

Abstract: Axions and axion-like particles are compelling candidates for ultralight bosonic dark matter, forming coherent oscillating fields that can be probed by experiments known as haloscopes. A broad range of haloscope concepts has been developed, including resonant cavity haloscopes, lumped-element circuit detectors, and spin-based experiments, each sensitive to different axion couplings and mass ranges. Rather than attempting an exhaustive survey of all existing approaches, this comparative review provides a unified framework for the major haloscope classes, establishing a common language for the descriptions of signal generation, noise properties, data analysis, and scanning strategies.
  Key properties of ultralight bosonic dark matter relevant for detection are summarized first, including coherence time, spectral linewidth, and stochasticity under the standard halo model. The discussion then compares cavity, Earth-scale, lumped-element, and spin haloscopes, focusing on expected signal shapes, dominant noise sources, and statistical frameworks for axion searches. Particular emphasis is placed on consistent definitions of signal-to-noise ratio and on how detector bandwidth, axion coherence, and noise characteristics determine optimal scan strategies.
  By systematically comparing operating principles and performance metrics across these detector families, this framework clarifies shared concepts as well as the essential differences that govern sensitivity in different mass and coupling regimes. The resulting perspective synthesizes current search methodologies and offers guidance for optimizing future haloscope experiments.

</details>


### [45] [Direct Detection and Cosmological Constraints of Dark Matter with Dark Dipoles](https://arxiv.org/abs/2602.06861)
*Takumi Kuwahara,Jun-Chen Wang,Shu-Run Yuan*

Main category: hep-ph

TL;DR: This paper explores sub-GeV fermionic dark matter interacting via dipole operators mediated by a dark photon, focusing on direct detection methods and cosmological constraints. It finds that direct detection probes, especially semiconductors like skipper-CCD, are crucial for constraining electric dipole couplings, while cosmological limits (thermal relic, CMB, BBN) primarily affect magnetic dipole interactions. Future experiments could further explore viable parameter spaces.


<details>
  <summary>Details</summary>
Motivation: To investigate the viability of fermionic dark matter with dipole interactions through a dark photon mediator, particularly for sub-GeV masses, where conventional methods are less effective. The study aims to bridge gaps left by existing cosmological constraints and assess the potential of next-generation semiconductor detectors.

Method: The analysis combines theoretical modeling of dark matter interactions via electric/magnetic dipole operators, simulations of direct detection signals (including Migdal effects and dark matter-electron scattering), and cosmological bounds (thermal relic abundance, CMB, BBN). Semiconductor targets and low-threshold experiments are emphasized for their sensitivity to sub-GeV masses.

Result: Cosmological observations (especially for magnetic dipole interactions) have constrained much of the parameter space, leaving narrow viable regions for sub-GeV dark matter. Direct detection experiments, particularly using semiconductor technologies like skipper-CCD, show promise in probing remaining parameter spaces with future low-threshold setups.

Conclusion: Sub-GeV dark matter with dipole interactions remains viable where cosmological constraints are weak, and future semiconductor-based direct detection experiments are critical to further constrain or discover such particles. The study highlights the necessity of both experimental advancements and theoretical modeling for dark sector dipole portals.

Abstract: We study a fermionic dark matter candidate that couples to the standard model particles exclusively through electric and magnetic dipole operators mediated by a massive dark photon. Such dipole portals naturally arise in dark sectors where the dark matter is neutral under a hidden $U(1)_D$, and they lead to phenomenology distinct from conventional vector-current interactions. We consider the direct-detection signals arising from dark matter-nucleus scattering including the Migdal effect, dark matter-electron scattering, and semiconductor targets, which allow sensitivity to sub-GeV dark matter masses, together with the cosmological bounds from such as thermal relic abundance, cosmic microwave background, big-bang nucleosynthesis, and cosmic-rays. We find that the dark dipole coupling can be largely constrained by direct detection (in particular, electric dipole coupling). However, the cosmological observations have already constrained most of the parameter space, in particular for magnetic dipole interactions of $U(1)_D$ for sub-GeV dark matter. For the dark matter mass below 10 MeV, the semiconductor (in particular, using skipper-CCD) experiments can play a crucial role in probing the dark dipole interactions: future low-threshold experiments utilizing the semiconductor targets can further extend the constraints. Our results have demonstrated that the sub-GeV dark matter with dark dipole interactions can be still safe from the direct-detection constraints, and the future low-threshold semiconductor experiments may play a significant role in constraining the dark dipole interactions.

</details>


### [46] [Study of $B \to K_0^*(1430)\,\ell^+ \ell^-$ Decay in the Standard Model and Scalar Leptoquark Scenario](https://arxiv.org/abs/2602.06892)
*M. Dadashzadeh,K. Azizi*

Main category: hep-ph

TL;DR: This study investigates the rare decay B→K₀*₁(1430)ℓ⁺ℓ⁻ to detect potential new physics beyond the Standard Model (SM). It compares SM predictions with scalar leptoquark contributions, focusing on observables like differential decay rates, branching ratios, and polarization asymmetries in regions less influenced by charmonium effects, guiding future Belle II and LHCb experiments.


<details>
  <summary>Details</summary>
Motivation: To probe new physics beyond the SM by studying rare B decays sensitive to leptoquarks, which could explain discrepancies in flavor physics.

Method: Analyzes the SM and scalar leptoquark contributions to compute key observables (differential decay rate, branching ratios, forward-backward asymmetry, lepton polarizations) and identifies q² regions minimally affected by long-distance charmonium effects.

Result: Predictions for observables show sensitivity to leptoquark parameters, highlighting specific q² regions optimal for distinguishing SM from new physics scenarios.

Conclusion: The analysis provides a framework for future experiments like Belle II and LHCb to test leptoquark hypotheses and constrain new physics models.

Abstract: This study examines the rare decay $B \to K_0^*(1430)\,\ell^+ \ell^-$ as a possible probe for new physics beyond the standard model (SM). We first analyze this channel within the SM and then include scalar leptoquark (LQ) contributions. We provide predictions for key observables, like differential decay rate, branching ratio, ratio of branching fractions at different channels, forward-backward asymmetry and different lepton polarizations, and assess their sensitivity to leptoquark scenarios, highlighting $q^2$ regions less affected by the long-distance charmonium effects. The results can be useful for future Belle II and LHCb measurements.

</details>


### [47] [MadSpace -- Event Generation for the Era of GPUs and ML](https://arxiv.org/abs/2602.06895)
*Theo Heimel,Olivier Mattelaer,Ramon Winterhalder*

Main category: hep-ph

TL;DR: MadSpace is a new C++ library with GPU support for phase-space and event generation in particle physics, offering adaptive and neural importance sampling, efficient compute graphs, and integration with machine learning tools.


<details>
  <summary>Details</summary>
Motivation: To provide a versatile and efficient framework for particle physics simulations, enabling faster and more accurate event generation through advanced sampling techniques and GPU acceleration.

Method: MadSpace uses a compute-graph-based architecture to handle phase-space construction, incorporating both traditional methods and neural network-based approaches like normalizing flows. It supports batch processing and end-to-end GPU workflows with Python integration for ML compatibility.

Result: The library offers significant performance improvements with its adaptive sampling and GPU acceleration, capable of seamless integration with PyTorch for machine learning applications.

Conclusion: MadSpace advances computational efficiency in particle physics by combining modular design, advanced sampling techniques, and GPU-accelerated computing, setting a new standard for event generation.

Abstract: MadSpace is a new modular phase-space and event-generation library written in C++ with native GPU support via CUDA and HIP. It provides a unified compute-graph-based framework for phase-space construction, adaptive and neural importance sampling, and event unweighting. It includes a wide range of mappings, from the standard MadGraph multi-channel phase space to optimized normalizing flows with analytic inverse transformations. All components operate on batches of events and support end-to-end on-device workflows. A high-level Python interface enables seamless integration with machine-learning libraries such as PyTorch.

</details>


### [48] [QED Effects in PDFs -- A Les Houches Comparison Study](https://arxiv.org/abs/2602.06908)
*Thomas Cridge,Juan Cruz Martinez,Joey Huston*

Main category: hep-ph

TL;DR: The paper examines the impact of including Quantum Electrodynamics (QED) in proton structure function analyses, particularly focusing on the NNPDF4.0 parton distribution functions (PDFs), which show the most significant changes when QED is incorporated. The study highlights previously neglected effects that could grow in importance as experimental precision improves.


<details>
  <summary>Details</summary>
Motivation: To investigate how QED effects, though small (around 1%), influence PDF determinations and could become significant as experimental uncertainties shrink, necessitating precise considerations for future high-precision studies.

Method: The authors compare QCD+QED PDFs from various global fitting groups with QCD-only PDFs, with a detailed analysis of the NNPDF4.0 set due to its notable sensitivity to QED inclusion. They assess changes in PDF shapes and uncertainties caused by these effects.

Result: Including QED in PDF fits leads to measurable changes in the proton's internal structure, especially in the NNPDF4.0 set, where QED significantly alters PDF shapes and uncertainty estimates, suggesting future precision experiments must account for these effects.

Conclusion: QED contributions, though currently peripheral, may substantially affect PDF results as precision increases. Prioritizing accurate QED inclusion in PDF analyses is critical for maintaining reliability in high-precision physics studies.

Abstract: In the last decade, and even more so in the last few years, our knowledge of the internal structure of the proton has become more accurate and precise thanks to the large amount of data available and developments in theory and methodology. The reduction of the uncertainties associated to these developments has brought previously neglected effects into focus as their typical magnitude are competitive with the size of the uncertainties. One such effect is the inclusion of QED into PDF fits. Typically this is a percent effect, and thus while theoretically important, it has had a relatively limited impact on phenomenological studies up to this point. In this proceeding we study some of the effects which, while peripheral to the inclusion of QED in the proton, can considerably change the relative size and shape of the QCD+QED fit with respect to the QCD only determination. These may become important in the future as precision continues to increase. After a comparison of the QCD+QED PDFs with the QCD only PDFs of various global PDF fitting groups, we focus largely upon NNPDF4.0, which shows the biggest effect when including QED. Focusing largely on a single set of PDFs also enables more subtle effects to be analysed, making it an ideal candidate for this study.

</details>


### [49] [Parametric-Resonance Production of QCD Axions](https://arxiv.org/abs/2602.06922)
*Pirzada,Yu Gao,Qiaoli Yang*

Main category: hep-ph

TL;DR: The paper shows that primordial temperature fluctuations during the QCD phase transition cause parametric resonance in axion field evolution, shifting the predicted axion mass range for dark matter to 10^-4−10^-3 eV, higher than previously thought.


<details>
  <summary>Details</summary>
Motivation: To explain the observed dark matter abundance by addressing limitations in the canonical axion mass window and exploring new mechanisms in axion production during cosmic phase transitions.

Method: Analyzing the interaction between primordial temperature fluctuations and the axion mass during the QCD phase transition, leveraging parametric resonance alongside the misalignment mechanism.

Result: The interplay between parametric resonance and the misalignment mechanism shifts the axion mass window to higher values (10^-4−10^-3 eV), expanding the parameter space for dark matter detection experiments.

Conclusion: The enhanced axion production mechanism suggests that current and future dark matter experiments should target higher mass ranges to Fully explore the shifted axion mass window.

Abstract: We demonstrate that dark matter axion production is enhanced through a natural and unavoidable mechanism: primordial temperature fluctuations periodically modulate the axion mass during the QCD phase transition, thereby triggering parametric resonance in axion field evolution. This interplay between parametric resonance and the misalignment mechanism shifts the predicted axion mass window for the observed dark matter abundance to $10^{-4}-10^{-3} \, \text{eV}$, displacing the canonical axion mass window to previously unexplored higher ranges.

</details>


### [50] [Revisiting the Electroweakino Sector of the Baryon Number Violating MSSM at the HL-LHC with Deep Neural Networks](https://arxiv.org/abs/2602.06957)
*Rahool Kumar Barman,Arghya Choudhury,Subhadeep Sarkar*

Main category: hep-ph

TL;DR: The study investigates the sensitivity of direct electroweakino production at the HL-LHC in a simplified model with wino-like and bino-like neutralino assumptions, using R-parity violating operators. Trained MLP classifiers on final state particle data distinguish signals from backgrounds, projecting wino mass reach up to ~700-900 GeV depending on the RPV operators and channels.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of the HL-LHC to detect electroweakino particles under R-parity violation scenarios, which could extend current mass probes beyond previous experiments and provide insights into beyond-Standard-Model physics.

Method: Uses a simplified model with mass-degenerate wino-like charginos/neutralinos and a bino-like LSP. Examiners three channels involving Wh and WZ mediated processes with specific RPV operators. Trains MLPs on four-momenta and observables to separate signal from SM backgrounds.

Result: Projected 2σ sensitivity reaches wino masses up to ~900 GeV (Wh 1ℓ+2b), ~780 GeV (Wh 1ℓ+2γ), and ~880 GeV (WZ 3ℓ) for λ' '₁₁₂, and ~700 GeV (Wh 1ℓ+2γ with λ' '₁₁₃) and ~850 GeV (WZ 3ℓ) for sole λ' '₁₁₃ coupling.

Conclusion: The HL-LHC can significantly extend the sensitivity for electroweakino detection with R-parity violation, with mass reach dependent on RPV operators and analysis channels. MLP-based classifiers improve signal discrimination over traditional methods.

Abstract: We study the projected sensitivity of direct electroweakino production $pp \to \tildeχ_1^{\pm} \tildeχ_2^0$ at the HL-LHC in a simplified framework with wino-like, mass degenerate $\tildeχ_1^{\pm}$ and $\tildeχ_2^0$, and a bino-like lightest neutralino $\tildeχ_1^0$, assuming R-parity violating~(RPV) through the baryon number violating $λ^{\prime \prime}_{112}u^c d^c d^c$ and $λ^{\prime \prime}_{113}u^c d^c b^c$ operators. We consider three channels with the $λ^{\prime \prime}_{112}u^c d^c d^c$ RPV operator: $Wh$ mediated $1\,\ell + 2\,b + \rm E{\!\!\!/}_T$, $Wh$ mediated $1\,\ell + (\geq 2\,j) + 2\, γ+ \rm E{\!\!\!/}_T$, and $WZ$ mediated $3\ell + (\geq 2 j) + \rm E{\!\!\!/}_T$. In each channel, we train benchmark-specific multi-layer perceptrons (MLPs), analogous to signal-region classifiers, on the four-momenta of the final state particles along with a small set of higher-level observables to distinguish the signal from the dominant SM backgrounds. We find that the HL-LHC will be able to probe winos up to $\sim 900~$GeV, $\sim 780~$GeV, and $\sim 880~$GeV in the $Wh$ mediated $1\,\ell + 2\,b + \rm E{\!\!\!/}_T$, $Wh$ mediated $1\,\ell + (\geq 2\,j) + 2\, γ+ \rm E{\!\!\!/}_T$, and $WZ$ mediated $3\ell + (\geq 2 j) + \rm E{\!\!\!/}_T$ channels, respectively, for $m_{\tildeχ_1^0} \sim 50~$GeV, in the presence of $λ^{\prime \prime}_{112}u^c d^c d^c$ couplings, at $2σ$ sensitivity. In case the $λ^{\prime \prime}_{113}u^c d^c b^c$ operator is solely switched on, the projected sensitivity for winos reach up to $\sim 700~$GeV for $Wh$ mediated $1\,\ell + (\geq 1\,b)\, + (\geq 1j)\, + 2\, γ+ \rm E{\!\!\!/}_T$ and $\sim 850~$GeV for the $WZ$ mediated $3\ell + (\geq 1 b) + \rm E{\!\!\!/}_T$ channel.

</details>


### [51] [Hard thermal contributions to phase transition observables at NNLO](https://arxiv.org/abs/2602.06962)
*Fabio Bernardo,Mikael Chala,Luis Gil,Philipp Schicho*

Main category: hep-ph

TL;DR: The paper constructs a high-temperature effective field theory for gauge-Higgs models up to O(g^6), integrating out hard modes to three-loop level. It analyzes the impact of higher-dimensional operators and higher-loop corrections on thermodynamic parameters, showing that one-loop dimension-six effects dominate over higher-loop contributions. The study also derives gauge-independent masses and couplings, and identifies consistency in renormalization across dimensions.


<details>
  <summary>Details</summary>
Motivation: To accurately model thermal effects in gauge-Higgs systems for gravitational-wave observables, requiring precise inclusion of higher-order corrections and renormalization to ensure consistency and reliability of theoretical predictions.

Method: Three-loop integration of hard modes, use of next-to-next-to-leading order effective potential, derivation of scalar/Debye masses for U(1) and SU(N) models, calculation of two-loop quartic couplings for Abelian Higgs model, verification of gauge independence, and analysis of master integrals' consistency between 4d and 3d renormalization.

Result: One-loop dimension-six operators dominate two/three-loop super-renormalizable contributions. Derived masses/couplings are gauge-independent. No new master integrals needed, but previous works missed some contributions in master integrals related to 4d/3d renormalization consistency.

Conclusion: Higher-dimensional operators are crucial for precision calculations in gauge-Higgs thermal models. The framework establishes a reliable method for gravitational-wave-relevant parameters, highlighting the necessity of including dimension-six effects and resolving inconsistencies in master integral treatments.

Abstract: To construct the high-temperature effective field theory of gauge-Higgs models up to $\mathcal{O}(g^6)$ in the gauge coupling, we integrate out hard modes to three-loop level and use the next-to-next-to-leading order effective potential. For the Abelian Higgs model, we quantify the impact of both higher-dimensional operators and higher-loop corrections on thermodynamic parameters relevant for gravitational-wave observables, finding that one-loop dimension-six effects typically dominate over two- and three-loop corrections to super-renormalizable parameters for the strongest transitions. We derive the three-loop scalar and Debye masses for the ${\rm U(1)}$ and ${\rm SU}(N)$ gauge-Higgs models, as well as the two-loop quartic couplings for the Abelian case, show gauge independence of physical parameters, and demonstrate that no new master integrals are required for the matching, while consistency of 4d and 3d renormalizability points to previously missing contributions in these master integrals.

</details>
