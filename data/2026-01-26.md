<div id=toc></div>

# Table of Contents

- [hep-ph](#hep-ph) [Total: 13]
- [astro-ph.HE](#astro-ph.HE) [Total: 6]
- [gr-qc](#gr-qc) [Total: 11]
- [astro-ph.IM](#astro-ph.IM) [Total: 7]


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [1] [Searching for Ultralight Scalar Dark Matter with Clocks in Low Earth Orbit](https://arxiv.org/abs/2601.16259)
*Dawid Brzeminski,Aaron Pierce*

Main category: hep-ph

TL;DR: Ultralight dark matter (DM) with quadratic couplings to Standard Model particles can be shielded by Earth's atmosphere, limiting ground-based experiments. Space-based quantum clocks, especially on the International Space Station (ISS), can detect DM masses above ~10^-9 eV by measuring orbit-averaged effects. For DM masses above ~10^-10 eV, Earth's gravity creates a dipole effect detectable in Low Earth Orbit (LEO), offering a cross-check and improved sensitivity. Optical and nuclear space clocks could provide leading constraints in some scenarios.


<details>
  <summary>Details</summary>
Motivation: The challenge of detecting ultralight dark matter due to atmospheric shielding and the need for space-based experiments to access higher DM mass ranges. The goal is to exploit quantum clock sensitivity to fundamental parameter variations caused by DM fields.

Method: Analyzing DM density modification near macroscopic bodies with quadratic couplings. Proposing space-based quantum clocks' use to measure orbit-averaged DM effects. Studying dipole modulation in LEO orbits for DM mass regimes where de Broglie wavelength < Earth radius.

Result: Space-based clocks on ISS can probe DM masses ≥10^-9 eV, with LEO dipole effects enabling cross-checks and sensitivity boosts. Nuclear clocks in orbit could access parameter space beyond ground experiments.

Conclusion: Quantum clocks in space offer superior capability for ultralight DM detection compared to ground methods. The combined use of orbit-averaged signals and orbital dipole modulation provides robust validation and improved constraints.

Abstract: The density of ultralight dark matter can be modified in the vicinity of macroscopic bodies when the dark matter possesses quadratic couplings to the Standard Model. If these couplings are sufficiently strong, Earth's atmosphere acts to shield the dark matter, thereby limiting the effectiveness of laboratory-based experiments. Experiments performed at altitudes exceeding the dark matter de Broglie wavelength experience the same orbit-averaged field amplitude as in the absence of scattering. Quantum clocks are capable of detecting variations in fundamental parameters due to the dark matter background. If based on the International Space Station, they are therefore well-suited to probe dark matter masses $m_{\rm DM}\gtrsim 10^{-9} \text{\, eV}$. Moreover, when the dark matter de Broglie wavelength is smaller than Earth's radius ($m_{\rm DM} \gtrsim 10^{-10}$ eV), the dark matter profile around Earth exhibits a dipole feature. In Low Earth Orbits this dipole temporally modulates potential dark matter signals. This provides a powerful cross-check of the orbit-averaged effect and can enhance the sensitivity of these experiments. We find optical clocks could give rise to world-leading constraints in some cases. Orbiting nuclear clocks could probe even more of the parameter space inaccessible to ground-based experiments.

</details>


### [2] [UV cut-off of the Standard Model and proton decays](https://arxiv.org/abs/2601.16297)
*Ryuichiro Kitano,Shohei Okawa*

Main category: hep-ph

TL;DR: The paper proposes a framework where a broken baryon/lepton symmetry explains proton stability and tiny neutrino masses via dimension-five operators, consistent with Super-Kamiokande's proton decay observation, with Hyper-Kamiokande expected to observe many events.


<details>
  <summary>Details</summary>
Motivation: To address the Standard Model's issues of proton decay non-observation and neutrino mass scales by invoking accidental symmetries and higher-dimensional operators, suggesting a beyond-Standard-Model theory at ~10^11 GeV.

Method: Introduces a framework with a scale Λ where higher-dimensional operators (like dimension-five neutrino mass terms) arise from a fundamental theory, using partial compositeness in a composite Higgs model to generate flavor structures via Yukawa couplings.

Result: Proton decay lifetime matches Super-Kamiokande's p→π⁰μ⁺ event; predicts Hyper-Kamiokande will observe many such events. Neutrino masses at sub-eV scales emerge naturally with Λ ≈10¹¹ GeV.

Conclusion: This approach unifies neutrino mass and proton stability explanations, aligning with experimental data and suggesting upcoming experiments will test the model.

Abstract: Non-observation of proton decays as well as the smallness of the neutrino masses can naturally be explained by the accidental baryon and lepton number symmetry in the Standard Model, where the approximate symmetries are a consequence of the absence of the baryon or lepton number violating operators at the renormalizable level. The neutrino masses at sub-eV scales can be explained by the presence of the dimension-five, $\ell\ell HH/Λ$, term in the Lagrangian, suggesting that a more fundamental theory takes over beyond the energy scale $Λ$. We consider the possibility that the theory above the scale $Λ$ generates general higher dimensional operators with the flavor structure implied by the Yukawa interactions in the Standard Model. Such a set-up can be realized, for example, in the composite Higgs scenario with partial compositeness of fermions. The fermion masses and the neutrino masses are explained for $Λ\sim 10^{11}$GeV. The lifetime of proton in this scenario is, interestingly, consistent with the observed event of the $p \to π^0 μ^+$ decay at the Super-Kamiokande experiment. The Hyper-Kamiokande experiments should see a large number of events soon after the data taking.

</details>


### [3] [Axial Anomaly, entanglement and polarization](https://arxiv.org/abs/2601.16304)
*O. V. Teryaev*

Main category: hep-ph

TL;DR: The paper explores the entanglement between photons via pion decays and axial anomaly, relating it to the Einstein-Podolsky-Rosen-Bohm-Aharonov effect. It highlights non-locality in angular momentum conservation, time-like pion transitions, and interactions with vector mesons in magnetic fields, with implications for experiments.


<details>
  <summary>Details</summary>
Motivation: To study non-local effects and entanglement in quantum field theory through pion decays, and to understand momentum conservation anomalies in both space-time separated systems and external fields.

Method: Analyzes pion decay dynamics controlled by axial anomaly, examines time-like pion transition formfactors for dilepton polarization, and investigates vector meson behavior in magnetic fields using vacuum conductivity models.

Result: Demonstrates non-causal, non-local angular momentum conservation; identifies measurable dilepton polarization effects via time-like formfactors; reveals vacuum conductivity's role in longitudinal polarization of vector mesons in heavy-ion collisions.

Conclusion: Such quantum anomalies and entanglement phenomena provide experimentally testable frameworks for understanding non-locality in particle physics and vacuum interactions under extreme conditions.

Abstract: The (pion) decays controlled by axial anomaly imply the specific entanglement between photons having also the counterparts for classical electromagnetic waves. This is also a specific case of Eisnstein-Podolsky-Rosen-Bohm-Aharonov effect. The absence of causality and non-locality in (angular) momentum conservation is manifested, being especially clear for the generalization to the case of time rather than space separation corresponds to the polarization of dileptons described by time-like pion transition formfactors which may be studied experimentally. The similar decays in external magnetic field manifest the interplay with vacuum conductivity in external magnetic field and longitudinal polarization of vector mesons observed in heavy-ion collisions.

</details>


### [4] [Universality of Gluon Saturation from Physics-Informed Neural Networks](https://arxiv.org/abs/2601.16391)
*Wei Kou,Xurong Chen*

Main category: hep-ph

TL;DR: The paper introduces a Physics-Informed Neural Network (PINN) using a Teacher-Student strategy to model the color dipole amplitude in high-energy QCD, achieving model-independent results that universally predict cross-sections without parameter tuning.


<details>
  <summary>Details</summary>
Motivation: To address discrepancies between inclusive and diffractive measurements in QCD and avoid ad-hoc parametric assumptions, ensuring universality of the gluon saturation scale.

Method: Uses a Teacher-Student PINN approach where the Teacher is the Balitsky-Kovchegov evolution in momentum space, and the Student is trained on HERA F2 data without initial state assumptions.

Result: The derived dipole amplitude successfully predicts J/psi photoproduction cross-sections without retuning parameters, confirming gluon saturation scale universality.

Conclusion: PINNs provide a transformative, parameter-free method for exploring non-perturbative QCD, validating their potential in high-energy physics.

Abstract: The universality of the color dipole amplitude is a cornerstone of high-energy Quantum Chromodynamics (QCD). However, standard phenomenological approaches typically rely on rigid parametric ansatzes and often require ad-hoc geometric adjustments to reconcile inclusive and diffractive measurements. To resolve this tension, we introduce Physics-Informed Neural Networks (PINNs) employing a ``Teacher--Student'' strategy. The rigorous momentum-space Balitsky-Kovchegov evolution dynamics act as the ``Teacher,'' constraining the solution manifold, while the network ``Student'' is refined against inclusive HERA $F_2$ data. This approach extracts a model-independent dipole amplitude without assuming initial states. Strikingly, we demonstrate that this amplitude -- without parameter retuning or geometric rescaling -- successfully predicts exclusive $J/ψ$ photoproduction cross-sections. This zero-parameter prediction rigorously confirms the universality of the gluon saturation scale and establishes PINNs as a transformative paradigm for uncovering non-perturbative QCD structures.

</details>


### [5] [PanopTag: Simultaneously Tagging All Jets in a Particle Collision Event](https://arxiv.org/abs/2601.16417)
*Umar Sohail Qureshi,Brendon Bullard,Ariel Schwartzman*

Main category: hep-ph

TL;DR: PanopTag introduces a novel encoder-decoder architecture for simultaneous multi-jet tagging, leveraging event-level context through cross-attention mechanisms, significantly outperforming traditional single-jet methods in heavy-flavor tagging.


<details>
  <summary>Details</summary>
Motivation: Current single-jet tagging methods ignore correlations between jets and event context, limiting their performance. There's a need to exploit inter-jet relationships and global event features for better classification.

Method: PanopTag uses an encoder-decoder framework where jets' kinematics act as queries to cross-attend particle flow object embeddings, enabling simultaneous tagging of all jets while capturing event-wide correlations.

Result: Achieved substantial performance improvements over state-of-the-art single-jet baselines in $b/c$-tagging by utilizing event-level features and jet interactions.

Conclusion: Simultaneous multi-jet tagging with event context outperforms independent approaches. PanopTag demonstrates the importance of modeling inter-jet relationships for advanced jet classification in high-energy physics.

Abstract: Jet tagging, identifying the origin of jets produced in particle collisions, is a critical classification task in high-energy physics. Despite the revolutionary impact of deep learning on jet tagging over the past decade, the paradigm has remained unchanged. In particular, jets are classified independently, one at a time. This single-jet approach ignores correlations, overlaps, and wider event context between jets. We introduce PanopTag, a new paradigm for jet tagging that departs from traditional single-jet tagging approaches. Rather than classifying jets independently, PanopTag simultaneously tags all jets by employing an encoder-decoder architecture that uses jet kinematics as queries to cross-attend to particle flow object embeddings. We evaluate PanopTag on heavy-flavor $(b/c)$-tagging and demonstrate remarkable performance improvements over state-of-the-art single-jet baselines that are only accessible by exploiting event-level features and correlations between jets.

</details>


### [6] [Feasibility Study of Lepton Number Violation in Rare $B$ and $K$ Meson Decays](https://arxiv.org/abs/2601.16422)
*Motoi Endo,Kåre Fridell,Sho Iwamoto,Yushi Mura,Kei Yamamoto*

Main category: hep-ph

TL;DR: The paper investigates dimension-seven lepton-number-violating operators in the Standard Model effective field theory, focusing on their impact on meson decays ($B \to K νν$ and $K \to πνν$), baryogenesis, and neutrinoless double beta decay. It establishes constraints for observable meson decay rate excesses while reconciling with neutrino mass generation via the Weinberg operator.


<details>
  <summary>Details</summary>
Motivation: To explore new physics contributions beyond the Standard Model through lepton-number-violating interactions that could explain potential deviations in meson decay rates, address baryon asymmetry washout issues, and link to neutrino mass mechanisms without conflict.

Method: Analyzed dimension-seven operators in the Standard Model Effective Field Theory, calculated their contributions to specific meson decays, assessed implications for baryogenesis and neutrinoless double beta decay, and derived conditions for observable excesses in decay rates while ensuring compatibility with the Weinberg operator for neutrino masses.

Result: Identified feasible parameter regions where meson decay excesses could be observed, showing compatibility with neutrino mass generation via the Weinberg operator even with two-loop contributions from the studied operators.

Conclusion: Dimension-seven lepton-number-violating interactions provide a viable framework for testing new physics with current and future meson decay experiments, offering a consistent pathway for connecting baryogenesis, neutrinoless double beta decay, and neutrino mass mechanisms.

Abstract: We study lepton-number-violating interactions at dimension seven in the Standard Model effective field theory that contribute to the meson decays $B \to K νν$ and $K \to πνν$. Such interactions could washout the baryon asymmetry of the Universe and also contribute to the neutrinoless double beta decay, even though the interactions involve a change in down-type quark flavors. We clarify conditions under which excesses in meson decay rates over the Standard Model predictions can be successfully observed. We also show that, although these interactions contribute to neutrino masses at the two-loop level, the Weinberg operator can be introduced consistently without spoiling the scenario.

</details>


### [7] [Effective Field Theory Description of Light Dilaton](https://arxiv.org/abs/2601.16534)
*Qing-Hong Cao,Jian-Nan Ding,Bing-Hui Ge,Hao Sun,Jiang-Hao Yu*

Main category: hep-ph

TL;DR: The paper constructs a systematic effective field theory (EFT) for dilatons, addressing the lack of a low-energy framework, and analyzes their phenomenology across MeV-scale and ultralight mass regimes, providing constraints and future experimental projections.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive low-energy EFT framework for dilatons, which are an alternative to axion-like particles but lacked such a structure previously, enabling detailed phenomenological analysis across their entire mass spectrum.

Method: Develop a manifestly scale-invariant regularization EFT for dilatons, deriving universal couplings to the trace anomaly, building a hierarchical EFT tower connecting UV conformal and IR sectors, including dilaton-extended SMEFT, low-energy EFT up to dimension-7, and chiral Lagrangians. Analyze two mass regimes with distinct behaviors (conventional vs. wave-like particles) using experimental data and projections.

Result: Derived constraints for MeV-scale dilatons from LHC, meson decays, and supernova cooling data; outlined sensitivities for ultralight dilatons as dark matter using atomic clocks and atom interferometers. Established a unified EFT framework for future studies.

Conclusion: The developed EFT framework successfully bridges the gap between UV and IR physics for dilatons, enabling rigorous phenomenological exploration across all mass scales. It opens avenues for testing dilaton models through diverse experimental probes, offering a versatile toolset for particle physics and cosmology.

Abstract: Dilatons, the CP-even pseudo-Nambu-Goldstone bosons arising from spontaneous scale symmetry breaking, offer a compelling alternative to axion-like particles (ALPs) yet lack a comprehensive low-energy framework. We address this by constructing a systematic effective field theory (EFT) for the dilaton based on a manifestly scale-invariant regularization scheme. This approach derives universal linear couplings to the trace anomaly while preserving consistent renormalization group evolution. We establish a hierarchical EFT tower connecting the ultraviolet conformal sector to the infrared, encompassing the dilaton-extended SMEFT, low-energy EFT up to dimension-7, and a chiral Lagrangian describing meson and baryon interactions. We perform a comprehensive phenomenological analysis across two distinct mass regimes, where dilaton manifests as either conventional particle or wave-like particle. For MeV-scale dilatons behaving as conventional particles, we obtain constraints from LHC production, semi-invisible $B$- and $K$-meson decays, and supernova cooling. For ultralight dilatons acting as dark matter, we project sensitivities for atomic clocks and atom interferometers. This unified EFT framework would pave the way for extended phenomenological studies across the full mass spectrum of the light dilaton.

</details>


### [8] [Description of Charged\text{-}Particle Multiplicity Distributions in High\text{-}Energy Proton\text{-}Proton Collisions Based on a Two-Component Model and Examination of Parton Distribution Functions](https://arxiv.org/abs/2601.16569)
*Zhixiang Yang,Jianhong Ruan*

Main category: hep-ph

TL;DR: The paper uses a two-component model combining gluon-gluon fusion and quark recombination processes to analyze charged-particle pseudorapidity densities in high-energy proton-proton collisions at the LHC. It incorporates MD-DGLAP evolved PDFs and KMR-derived UPDFs, successfully matching ATLAS data and highlighting gluon dominance at √s ≥ 900 GeV.


<details>
  <summary>Details</summary>
Motivation: To test QCD in the small-x, gluon-dominated regime and validate a minimal framework for predicting charged-particle multiplicities.

Method: 1. Evolve PDFs via MD-DGLAP equations. 2. Convert PDFs to UPDFs using KMR scheme. 3. Use two-component model (gluon-gluon fusion and quark recombination) to predict pseudorapidity distributions. 4. Compare with ATLAS data and alternative PDF sets.

Result: The model accurately reproduces ATLAS measurements. Gluon-gluon fusion dominates at √s ≥ 900 GeV. MD-DGLAP-based PDFs are validated, and discrepancies among different PDF sets in the small-x region are identified.

Conclusion: The framework supports the robustness of MD-DGLAP evolution and small-x gluon dynamics. Future studies should address uncertainties in PDF sets and explore higher collision energies.

Abstract: High-energy proton-proton collisions at the LHC offer a stringent test of Quantum Chromodynamics (QCD) in the small-$x$, gluon-dominated regime. This study focus on a minimal, gluon-driven framework to describe the charged-particle multiplicities and their pseudorapidity densities in high energy collisions. The two-component model taken here includes the hard gluon-gluon fusion process and the soft quark recombination process, which directly relates to both integrated and unintegrated parton distributions. We begin by evolving Parton Distribution Functions (PDFs) using the Modified Dokshitzer-Gribov-Lipatov-Altarelli-Parisi (MD-DGLAP) equations. These PDFs are then converted into unintegrated PDFs (UPDFs) via the Kimber-Martin-Ryskin (KMR) scheme. The resulting PDFs and UPDFs are incorporated into the two-component model to predict the charged-particle pseudorapidity density $\left(1 / N_{\mathrm{ev}}\right) d N_{\mathrm{ch}} / d η$ in $pp$ collisions at LHC energies. Our predictions are compared to the data from the ATLAS experiment, revealing that the model effectively captures the features of the observed pseudorapidity distributions, despite its simplicity. Within this framework, the gluon-gluon fusion processes are found to dominate particle production for $\sqrt { s } \ge 9 0 0 \ \mathrm { GeV }$.These findings provide phenomenological support for MD-DGLAP-based PDFs and the associated small-$x$ gluon dynamics. Furthermore,a comparative analysis of results from alternative PDF sets--including CTEQ, MSHT, NNPDF, HERAPDF, and GRV--is performed, with particular focus on examining their consistency with the relative shapes of experiment data in the small-$x$ region.

</details>


### [9] [Flavour-Changing Neutral Current Top Decays in the Three Higgs Doublet Model](https://arxiv.org/abs/2601.16647)
*Baradhwaj Coleppa,Benjamin Fuks,Akshat Khanna*

Main category: hep-ph

TL;DR: The paper investigates rare top quark decays in the democratic Three Higgs Doublet Model with a $Z_3$-symmetric potential, showing that loop-induced branching ratios can exceed SM expectations and be detectable at the High-Luminosity LHC.


<details>
  <summary>Details</summary>
Motivation: To explore rare top quark flavor-changing neutral current decays and their sensitivity to the extended scalar sector of the Three Higgs Doublet Model under specific symmetry and natural flavour conservation conditions.

Method: Computed one-loop contributions for $t \to qX$ decays, analyzed parameter space under theoretical and experimental constraints, examined alignment-limit scenarios for CP-even Higgs hierarchies.

Result: Branching ratios in certain scenarios exceed Standard Model predictions, particularly with light non-standard scalars, potentially detectable at High-Luminosity LHC.

Conclusion: Rare top decays provide a viable probe for the Three Higgs Doublet Model's scalar sector, especially in alignment-limit scenarios with accessible non-standard Higgs states.

Abstract: We study flavour-changing neutral current decays of the top quark in the democratic Three Higgs Doublet Model featuring a $Z_3$-symmetric scalar potential and Natural Flavour Conservation. In this framework, while such processes are absent at tree-level, the extended scalar sector induces new one-loop contributions to rare top decays. We compute the branching ratios for processes of the form $t \to q X$ (with $q = u, c$ and $X$ denoting a boson of the model), and explore the viable regions of the parameter space under theoretical consistency conditions and current experimental constraints. Several alignment-limit scenarios corresponding to different hierarchies among the CP-even Higgs states are analysed, and we find that the predicted branching ratios can significantly exceed their Standard Model expectations while remaining consistent with existing limits. In particular, we identify scenarios with light non-standard scalars that can lead to rates within the projected sensitivity of the High-Luminosity LHC. Our results therefore highlight rare top decays as a promising probe of the extended scalar sector of the Three Higgs Doublet Model.

</details>


### [10] [Thermodynamic geometry in hadron resonance gas model at real and imaginary baryon chemical potential and a simple sufficient condition for quark deconfinement](https://arxiv.org/abs/2601.16762)
*Riki Oshima,Hiroaki Kouno,Motoi Tachibana,Kouji Kashiwa*

Main category: hep-ph

TL;DR: This paper investigates the thermodynamic geometry of the hadron resonance gas model with and without excluded volume effects (EVE), focusing on phase structures in both real and imaginary baryon chemical potential (mu) regions. Key findings include phase transitions in high mu regions with EVE, a connection between lattice QCD results and the R=0 scalar curvature criterion, and a sufficient condition for quark deconfinement involving baryon density.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore how excluded volume effects influence the phase diagram of the hadron resonance gas model, particularly in relation to QCD critical phenomena and phase transitions at finite baryon chemical potential. This helps bridge theoretical models with lattice QCD predictions.

Method: The authors calculate the scalar curvature R using the hadron resonance gas model with/without EVE, applying the R=0 criterion to analyze phase structures across the mu^2-T plane. They also use baryon number fluctuations to determine limitation temperatures and derive a condition linking baryon density to deconfinement.

Result: With EVE, complex phase structures emerge in high real mu regions and Roberge-Weiss-like regions with imaginary mu. The critical point from LQCD aligns with calculated limitation temperatures. A sufficient condition n_B > 1/(2v_B) for deconfinement is established.

Conclusion: Excluded volume effects are crucial for accurate phase diagrams at high baryonic densities, and the R=0 criterion effectively links lattice QCD feasible regions to real chemical potential regimes. The derived condition provides a practical threshold for quark deconfinement studies.

Abstract: The thermodynamic geometry of the hadron resonance gas model with (without) excluded volume effects (EVE) of baryons is investigated. The case with imaginary mu, where mu is the baryon chemical potential, is investigated as well as the one with real mu. We calculate the scalar curvature R and use the R=0 criterion to investigate the phase structure in the mu^2-T plane where T is the temperature. The curve on which R=0 continues analytically from the imaginary mu region, where the lattice QCD is feasible, to the real mu one. In the presence of EVE, there are rich phase structures in the large real mu region as well as the Roberge-Weiss like region where mu is imaginary and a singularity appears, while there is no phase structure in the large real $μ$ region in the absence of EVE. The limitation temperature of the baryon gas is also obtained by using the baryon number fluctuation. The LQCD predicted critical point locates almost on the curve of the limitation temperature we determined. A simple sufficient condition, n_B>1/(2v_B)$, is obtained for the quark deconfinement in the large real mu region, where n_B and v_B are the net baryon number density and the volume of a baryon, respectively.

</details>


### [11] [The Scattering Algebra of Physical Space: Squared Massive Constructive Amplitudes](https://arxiv.org/abs/2601.16814)
*Moab Croft,Neil Christensen*

Main category: hep-ph

TL;DR: The paper uses the Algebra of Physical Space (APS) to connect spinor formalism with massive amplitudes in the Constructive Standard Model (CSM), introducing Scattering Algebra (SA) to establish equivalence between the two formalisms. It highlights the role of Lorentz rotors and ray spinors, demonstrating the geometric approach's effectiveness for future research.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between traditional CSM and APS formalisms by establishing a novel equivalency (SA) and to explore geometric methods for handling massive amplitudes in particle physics.

Method: Derive a correspondence between CSM spinors and APS Lorentz rotors via the Scattering Algebra (SA), perform example calculations to validate consistency between frameworks, and analyze the geometric structure of ray spinors.

Result: Confirmed consistent results between CSM and APS-CSM using SA; identified Lorentz rotors as counterparts to CSM spinors, enabling geometric interpretations of massive amplitudes. Demonstrated index-free formalism efficacy for further studies.

Conclusion: APS offers a powerful geometric approach for particle physics calculations, particularly for massive cases. Future work includes extending to massless cases, amplitude construction, and integrating Wigner little group methods within the APS framework.

Abstract: The Algebra of Physical Space (APS) is used to explore the Constructive Standard Model (CSM) of particle physics. Namely, this paper connects the spinor formalism of the APS to massive amplitudes in the CSM. A novel equivalency between traditional CSM and APS-CSM formalisms is introduced, called the Scattering Algebra (SA), with example calculations confirming the consistency of results between both frameworks. Through this all, two significant insights are revealed: The identification of traditional CSM spin spinors with Lorentz rotors in the APS, and the connection of the CSM to various formalisms through ray spinor structure. The CSM's results are replicated in massive cases, showcasing the power of the index-free, matrix-free, coordinate-free, geometric approach and paving the way for future research into massless cases, amplitude-construction, and Wigner little group methods within the APS.

</details>


### [12] [Cosmic ray electron boosted light dark matter: Implications of LZ 2025 data](https://arxiv.org/abs/2601.16903)
*Sk Jeesun,Anirban Majumdar*

Main category: hep-ph

TL;DR: The paper proposes using cosmic ray upscattering to detect sub-MeV electrophilic dark matter in LZ 2025 experiments, improving constraints by ~1x over XENONnT and outperforming some neutrino detector limits in specific mediator mass scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing multi-ton detectors are ineffective for sub-MeV dark matter due to insufficient sensitivity, while neutrino fog limits traditional direct detection. Cosmic ray interactions create high-energy dark matter subpopulations detectable via current experiments.

Method: Analysis of LZ 2025 (WS2024) data focusing on dark matter particles energized by cosmic rays. Both energy-independent and energy-dependent cross-section models are evaluated, emphasizing mediator mass effects on event rates and constraints.

Result: LZ 2025 constraints improved by ~1x over XENONnT for energy-independent cross-sections and remain stronger even with energy-dependent interactions. Outperforms neutrino detectors in some mediator parameter spaces, excluding newly accessible regions.

Conclusion: Cosmic ray boosted dark matter detection is a viable strategy enhancing sensitivity to sub-MeV particles. LZ 2025 shows significant improvement over prior experiments and expands explored parameter space beyond neutrino detector capabilities.

Abstract: Current multi-ton detectors put stringent constraints on the GeV-scale galactic dark matter, pushing the allowed cross-section almost towards the neutrino fog, yet remain mostly insensitive to the light dark matter. Cosmic rays can upscatter the non-relativistic halo dark matter particles, making a sub-population of them gain sufficient kinetic energy to be discernible in current direct search experiments. In this work, we explore this alternate strategy to probe sub-MeV electrophilic dark matter boosted by cosmic rays with the latest data of LZ 2025 (WS2024 run) and improve the constraint on the MeV scale dark matter by almost $\sim\mathcal{O}(1)$ compared to the previous XENONnT limit for energy-independent cross-section. Using realistic energy-dependent cross-sections, we also analyse such a scenario, where the associated mediator mass plays a crucial role in governing the event rate and hence the expected limits too. With energy-dependent cross-sections, our obtained limits also remain stronger than the existing constraints from current direct detection experiments. Even compared to the limits from the neutrino detectors with a larger target size, LZ 2025 can put stringent constraints in certain parameter space of the mediator, excluding the previously unexplored regions.

</details>


### [13] [NLO QCD corrections to the electroweak production of a Higgs boson pair in the quark-antiquark channel](https://arxiv.org/abs/2601.16924)
*Marco Bonetti,Gudrun Heinrich,Philipp Rendler,William J. Torres Bobadilla*

Main category: hep-ph

TL;DR: The paper calculates next-to-leading order QCD corrections for Higgs boson pair production via massless quark-antiquark channels, providing analytic two-loop amplitudes. Though small in total cross sections (~<1%), differential observables show up to 10% effects, necessitating inclusion for precise LHC comparisons.


<details>
  <summary>Details</summary>
Motivation: To improve precision in theoretical predictions for Higgs boson pair production at the LHC, especially for differential observables, by incorporating higher-order QCD corrections previously unaccounted for in the quark-antiquark channel.

Method: Computed next-to-leading order (NLO) QCD corrections for the electroweak boson loop-induced Higgs pair production in the massless quark-antiquark channel, deriving analytic two-loop amplitude expressions.

Result: Found that NLO QCD corrections contribute less than 1% to total cross sections but cause up to 10% changes in differential distributions, significant enough to affect comparisons with experimental data.

Conclusion: The calculated QCD corrections are essential for accurate analysis of Higgs boson pair production data at the LHC, particularly when interpreting differential measurements.

Abstract: Higgs boson pair production in the massless quark-antiquark channel proceeds at leading order (LO) via electroweak boson loops. We calculate the next-to-leading order QCD corrections to this process. For the corresponding two-loop amplitudes, an analytic representation has been achieved. Even though the size of this contribution at the level of total cross sections is below 1% compared to the LO gluon channel, the effect on differential observables can be in the 10% range and therefore this contribution should be taken into account when comparing to LHC data.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [14] [Measuring the Black Hole and Accretion Parameters of Sagittarius A* from EHT Observations using a Semi-Analytic Model](https://arxiv.org/abs/2601.16267)
*Braden J. Marazzo-Nowicki,Paul Tiede,Dominic O. Chang,Daniel C. M. Palumbo,Michael D. Johnson*

Main category: astro-ph.HE

TL;DR: The Event Horizon Telescope Collaboration analyzes variable Sagittarius A* data using Bayesian methods to infer black hole properties, revealing constrained parameters like inclination, emission peak location, spin orientation, while noting limitations in spin and magnetic field constraints due to 2017 observations.


<details>
  <summary>Details</summary>
Motivation: To quantify Sagittarius A*'s intrinsic variability and distinguish stable features of strong gravity and accretion environment from variable emissions, addressing challenges posed by rapid source structure changes.

Method: Bayesian inference on EHT data segments with a semi-analytic model and hierarchical stacking, fitting parameters while accounting for complex station gains, validated against synthetic GRMHD simulations.

Result: Constraints on observer inclination (nearly face-on), emission peak near horizon, vertical spin orientation, and equatorial emission dominance, with uncertainty quantification separating variability and model impacts.

Conclusion: While some parameters like inclination and emission location are well-constrained, black hole spin and magnetic fields remain poorly defined due to observational data limitations, emphasizing the need for future observations.

Abstract: The Event Horizon Telescope (EHT) Collaboration produced the first image of the apparent shadow of the central black hole of Sagittarius\,A$^*$ (\sgra). \sgra source structure varies significantly on timescales shorter than the duration of an observation, preventing improved data coverage through Earth rotation aperture synthesis. This rapid variability provides the opportunity to quantify intrinsic variability and separate time-variable emission features from stable signatures of strong gravity and the accretion environment. To infer the properties \sgra and its surrounding accretion flow, we perform Bayesian inference on a series of EHT data segments (``snapshots''). We directly fit parameters of a semi-analytic emission model jointly with complex station gains to snapshot visibilities, then extract estimates of the time-averaged, persistent source structure and temporal variability by stacking snapshots in a Bayesian hierarchical model. This approach successfully reproduces parameters of General Relativistic Magnetohydrodynamics simulations using synthetic EHT observations. Even with physically motivated assumptions about the \sgra environment, black hole spin and magnetic field parameters are poorly constrained by 2017 EHT observations. Our inference constrains other parameters, favoring a nearly face-on observer inclination ($θ_{\rm o} = 9.2\degree \pm 3.6 \degree \pm_{\rm v} 11.6\degree$), an emission peak near the horizon ($R_{\rm peak} = 4.9 \pm 0.1 \pm_{\rm v} 0.5\,GM/c^2$), near-vertical projected spin position angle ($p.a. = 7.3\degree \pm 7.08 \degree \pm_{\rm v} 43.5\degree$ counterclockwise from vertical), and dominant emission $43.4\degree \pm 2.0\degree \pm_{\rm v} 5.9\degree$ above the equatorial plane, where we separate average structure uncertainty ($\pm$) from the impacts of temporal variability and model misspecification ($\pm_{\rm v}$).

</details>


### [15] [Hot, Retrograde Tilted MADs: Misaligned, Precessing, and Shaped by Electromagnetic Torques](https://arxiv.org/abs/2601.16370)
*Sajal Gupta,Jason Dexter*

Main category: astro-ph.HE

TL;DR: The study explores how tilted magnetically arrested accretion disks (MADs) around black holes align or precess, with simulations showing prograde disks align in two stages while retrograde ones precess persistently. Electromagnetic torques drive alignment but are opposed in retrograde cases by hydrodynamic fluxes.


<details>
  <summary>Details</summary>
Motivation: Understanding the dynamics of tilted accretion disks in MAD states, which occur in systems like X-ray binaries and AGN, is crucial for interpreting observational phenomena such as quasi-periodic oscillations.

Method: 3D global GRMHD simulations were conducted, varying black hole spin and disk tilt angles. Torque equations were derived and evaluated in disk-aligned frames to analyze alignment mechanisms.

Result: Prograde MADs align via rapid then slow phases. Retrograde MADs exhibit solid-body precession 4x faster than weakly magnetized flows, resistant to alignment due to opposing hydrodynamic effects. A preliminary model explains the two-stage alignment.

Conclusion: Retrograde MADs provide a potential explanation for low-frequency QPOs in BH XRBs. Insights into orbital-plane alignment dynamics and torque interactions between electromagnetic and hydrodynamic forces are advanced.

Abstract: Tilted accretion disks in the magnetically arrested (MAD) state may be present in X-ray binaries and active galactic nuclei such as Sgr A* and M87. We have carried out 3D global GRMHD simulations to study the evolution of these accretion flows as a function of black hole spin and misalignment angle. Prograde MADs align with the spin through a two-stage process: an initial rapid alignment phase that operates on the magnetic flux saturation timescale, followed by a slower, spin-independent phase. In contrast, retrograde MADs remain persistently misaligned regardless of the black hole spin, displaying solid-body precession at rates four times higher than weakly magnetized flows at the same spin magnitude. By deriving torque equations in ideal GRMHD and evaluating them in a frame aligned with instantaneous disk orientation, we demonstrate that electromagnetic (EM) torques always act to align the disk with the BH spin, but are countered by opposing hydrodynamic fluxes in retrograde flows. We further develop a preliminary empirical model to explain the cause of two-stage prograde alignment and discuss the possibility of alignment in the retrograde MAD. Strongly magnetized, retrograde, misaligned accretion disks provide a candidate scenario for the low-frequency quasi-periodic oscillations in black hole X-ray binaries.

</details>


### [16] [GRBAlpha, VZLUSAT-2 and GRBBeta -- GRB observations with CubeSats](https://arxiv.org/abs/2601.16609)
*Jakub Ripa,Marianna Dafcikova,Andras Pal,Norbert Werner,Masanori Ohno,Laszlo Meszaros,Filip Munz,Balazs Csak,Gabor Galgoczi,Nikola Husarikova,Tomas Vitek,Pavel Kosik,Michaela Duriskova,Martin Kolar,Lea Szakszonova,Michal Pazderka,Filip Hroch,Martin Topinka,Yasushi Fukazawa,Hiromitsu Takahashi,Tsunefumi Mizuno,Masato Yokota,Jean-Paul Breuer,Kazuhiro Nakazawa,Hirokazu Odaka,Yuto Ichinohe,Peter Hanak,Miroslav smelko,Ivo Vertat,Tomas Urbanec,Ales Povalac,Miroslav Kasal,Jakub Kapus,Jan Hudec,Marcel Frajt,Maksim Rezenov,Vladimir Daniel,Petr Svoboda,Juraj Dudas,Martin Sabol,Robert Laszlo,Martin Koleda,Hsiang-Kuang Chang,Tsung-Che Liu,Chih-Hsun Lin,Chin-Ping Hu,Che-Chih Tsao,Kaustubha Sen,Chih-En Wu,Aaron Tohuvavohu,Suresh Sivanandam,Mark Barnet*

Main category: astro-ph.HE

TL;DR: The article summarizes results from three CubeSat missions (GRBAlpha, VZLUSAT-2, and GRBBeta) using CsI(Tl) scintillator detectors for gamma-ray transient monitoring, including detection of significant GRBs and radiation environment studies.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the effectiveness of CubeSats for long-term (over 3 years) detection of gamma-ray bursts (GRBs) and study radiation effects on Silicon Photomultipliers (SiPMs) in LEO.

Method: The CubeSats carried CsI(Tl) detectors read out by SiPMs. GRBAlpha (1U), VZLUSAT-2 (3U), and GRBBeta (2U) were launched to polar/low Earth orbits; data was collected over 3-4 years.

Result: Detected ~360 gamma-ray transients, including notable GRBs like GRB 221009A and GRB 230307A. Demonstrated CubeSat capability for multi-year missions and radiation environment monitoring.

Conclusion: CubeSats with CsI/SiPM detectors are viable for prolonged GRB monitoring and provide valuable data on radiation damage, enabling future cost-effective space missions.

Abstract: Results from GRBAlpha, VZLUSAT-2 and GRBBeta CubeSats and their on-board gamma-ray detectors for monitoring transients are summarised in this article. GRBAlpha was a 1U CubeSat launched in March 2021 to a 550 km altitude polar orbit carrying a CsI(Tl) scintillator gamma-ray burst (GRB) detector with a sensitive range of approximately 30-900 keV. It successfully operated for over four years until June 2025 when it de-orbited. VZLUSAT-2 was a 3U CubeSat launched in January 2022 to a 535 km altitude polar orbit and de-orbited in November 2025 after almost four years of smooth operation. It carried on board two GRB detectors very similar to the one used on GRBAlpha. Both missions have detected about 360 gamma-ray transients, including over 170 long and short gamma-ray bursts (GRBs), and including the most intense GRB ever recorded GRB 221009A and the second brightest GRB 230307A. The new family member, GRBBeta 2U CubeSat, integrated at Masaryk University, was launched in July 2024 to a 580 km altitude, 62 degree inclination orbit. It has been detecting GRBs since its launch without any trouble. Gamma-ray detectors on these nanosatellites are based on CsI(Tl) scintillator readout by silicon photomultipliers (SiPMs). These missions also provide a unique opportunity to study the radiation damage of SiPMs in the low Earth orbit environment and monitor the radiation belts. We have demonstrated that CubeSats can be used in missions lasting beyond three years and routinely detect GRBs.

</details>


### [17] [Investigating Twin Star Equation of States in Light of Recent Astrophysical Observations](https://arxiv.org/abs/2601.16674)
*Shamim Haque,Atharva Shinde,Asim Kumar Saha,Tuhin Malik,Ritam Mallick*

Main category: astro-ph.HE

TL;DR: The paper explores the existence of twin stars by analyzing hybrid star equations of state under various parameters, constrained by astrophysical observations.


<details>
  <summary>Details</summary>
Motivation: To determine conditions under which twin stars can exist by studying phase transitions from hadron to quark matter, considering observational constraints from GW170817 and NICER.

Method: Constructed a parameter space with transition energy density, pressure, discontinuity strength, and quark matter speed of sound. Derived 'witch-hat' curves representing discontinuity limits and analyzed stiffness of quark matter EOS under observational constraints.

Result: Found maximum twin star mass at 2.05 M☉, strongest allowed density discontinuity (7.76ρ_sat), and upper transition density limit (4.03ρ_sat). Observational data significantly narrows viable hybrid EOS possibilities.

Conclusion: Astrophysical observations tightly constrain twin star scenarios, limiting their properties. Future observations could further validate or refute hybrid star models.

Abstract: Twin stars are predicted to exist in nature if the hadron-to-quark phase transition is strong enough to form a new branch of hybrid stars, separated from the branch of neutron stars. We adopt an agnostic approach, using transition energy density, transition pressure, the discontinuity strength, and a constant speed of sound for quark matter as our parameter space to construct a large possibility of hybrid equations of state, and thereby encapsulating a comprehensive picture of the twin star scenario. First, we report the complete conditions on our parameter space imposed by the general relativistic hydrostatic equilibrium solutions. For a fixed transition energy density and speed of sound for quark matter, we define distinct ranges of transition pressures based on the allowed strengths of discontinuity. Below a maximum transition pressure, a range of discontinuity exists that increases as the transition pressure decreases. Thereby, we identify the loci of the limits on discontinuities as the `witch-hat' curves. Based on the causality limit, the witch-hat curves can be punctured or incomplete. Strong constraints on this picture are drawn from the inferences from GW170817 and the NICER measurements. We computed the maximum mass for twin stars to be $2.05~M_\odot$, the allowed strongest discontinuity in rest-mass density to be $7.76ρ_\mathrm{sat}$, and the upper bound on transition rest-mass density to be $4.03ρ_\mathrm{sat}$. Subsequently, we compute the implications of the stiffness of the quark matter equation of state on this picture. Different confidence levels for observational inferences are considered to assess the extent of inclusion (and rejection) of hybrid equations of state and, consequently, their effects on the limits of the maximum mass of twin stars and phase transition properties.

</details>


### [18] [Pushchino Multibeam Pulsar Search. X. Observations of pulsars at declinations above $+53^o$](https://arxiv.org/abs/2601.16714)
*S. A. Tyul'bashev,G. E. Tyul'basheva,M. A. Kitaeva*

Main category: astro-ph.HE

TL;DR: The paper describes a pulsar survey using an LPA radio telescope at 110.4 MHz, covering 4100 sq.deg with enhanced sensitivity through stacking observations. 35 known pulsars were detected, and flux densities were measured for 33.


<details>
  <summary>Details</summary>
Motivation: The motivation was to conduct an efficient pulsar survey in high northern declinations (+53° to +87°) at a low radio frequency (110.4 MHz) using advanced stacking techniques to boost sensitivity.

Method: The method involved using the LPA telescope with 3.072 ms time resolution and 19.5 kHz frequency resolution over 2.5 MHz bandwidth. Observations were repeated and power spectra stacked (gaining 5-10× sensitivity) to improve detection limits. Fourier power spectra analysis was applied in a blind search setup.

Result: 35 known pulsars were detected. Flux densities were successfully measured for 33 pulsars. The survey's stacked observations achieved sensitivity improvements as claimed.

Conclusion: The LPA telescope's capability for low-frequency pulsar surveys was validated through improved detection methods, demonstrating potential for future surveys at even lower frequencies or larger sky areas.

Abstract: A search for pulsars was carried out using a Large Phased Array (LPA) radio telescope at a frequency of 110.4 MHz with a time resolution of 3.072 ms and a frequency resolution of 19.5 kHz with a 2.5 MHz bandwidth used. The survey was conducted in a site with declinations of $+53^\circ < δ< +87^\circ$. The viewing area is approximately 4100 sq.deg. The search was carried out using Fourier power spectra. To increase sensitivity, multiple observations were made in each direction in the sky, and the resulting power spectra were summarized. This made it possible to increase sensitivity by about 5-10 times, depending on the direction in the sky. A blind search opened 35 known pulsars. Estimates of the flux density for 33 pulsars have been obtained.

</details>


### [19] [[HP99] 159 -- Properties of the first Supersoft X-ray Source with a Helium star donor](https://arxiv.org/abs/2601.16789)
*Hélène Szegedi,Philip A. Charles,David A. H. Buckley,Pieter J. Meintjes,Przemek Mróz,Andrzej Udalski*

Main category: astro-ph.HE

TL;DR: [HP99] 159 is the first identified SSS with an evolved He star donor. Data supports a 2.327d orbital period and suggests sustained mass-transfer via He nova model. Light curve modeling indicates He II emission arises from outflow, not disc. These findings confirm it as a single-degenerate SN Ia progenitor with distinctive luminosity compared to LMXBs.


<details>
  <summary>Details</summary>
Motivation: To determine the origin of the SSS component in [HP99] 159 between steady He-burning vs. decaying He nova models, and to confirm its status as a SN Ia progenitor candidate.

Method: Long-term spectroscopy (SALT), multi-site photometry (SAAO/OGLE), orbital period analysis, and light curve modeling to assess inclination, He emission sources, and luminosity properties.

Result: Confirmed 2.327d orbital period, detected small He II RV modulation inconsistent with disc origin, rejected decaying nova model via light curve analysis. Established system as single-degenerate SN Ia progenitor with unique MV-Σ properties differing from LMXBs by ~1.5mag.

Conclusion: [HP99] 159 serves as critical evidence for sustained mass-transfer in single-degenerate SN Ia scenarios. Its distinct SSS properties highlight irradiation effects in such binaries, offering new insights for progenitor models.

Abstract: [HP99] 159 is remarkable as the first supersoft X-ray source (SSS) identified with an evolved helium star donor. With a likely orbital period of 1.164 d or 2.327 d, the origin of the SSS component is controversial, with the two current models being either steady He-burning on the white dwarf surface, or that it is a helium nova in the decaying phase. To help resolve this issue we present extensive new long-term spectroscopy (with SALT) and photometry (at SAAO and with OGLE) of [HP99] 159 which (a) supports 2.327 d as the orbital period, and (b) finds only a small He II radial velocity modulation. The latter is surprising as it implies a very low inclination system, whereas our light curve modelling suggests $i{\sim}50^\circ$, and hence that the He II must be produced in outflowing material further above, or beyond, the disc. We find that the decaying nova model cannot fit our OGLE light curve and the observed SSS flux level. [HP99] 159 has been essentially constant as an SSS over several decades, implying a sustained high level of mass-transfer from its He star donor, making it the only confirmed single-degenerate scenario SN Ia progenitor. We have updated the known SSS binary parameters and find a clear $\sim$1.5 mag difference in their $M_{\rm V}$ when compared to the $M_{\rm V} - Σ$ properties of LMXBs, likely due to the larger irradiated areas and more luminous donors.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [20] [Bayesian Inference of Neutron Star Properties in $f(Q)$ Gravity Using NICER Observations](https://arxiv.org/abs/2601.16227)
*Sneha Pradhan,N. K. Patra,Kai Zhou,P. K. Sahoo*

Main category: gr-qc

TL;DR: The study explores neutron star properties in symmetric teleparallel $f(Q)$ gravity using Bayesian analysis with NICER data, finding the exponential model preferred and consistent with observations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to extend Bayesian studies of neutron star observations to $f(Q)$ gravity, which has been less explored compared to General Relativity and other modified gravity theories.

Method: The study uses Bayesian inference to compare mass-radius predictions of three $f(Q)$ models (linear, logarithmic, exponential) with NICER data on four pulsars, using the DDME2 equation of state.

Result: The exponential $f(Q)$ model is statistically favored, yielding $R_{1.4} = 11.27^{+0.53}_{-0.36}\,\mathrm{km}$ and $Λ_{1.4} = 156.95^{+84.02}_{-41.73}$, both consistent with observations.

Conclusion: Neutron stars are effective probes for testing symmetric teleparallel gravity in strong field regimes, with the exponential model showing the best fit.

Abstract: In this work, we investigate neutron stars (NSs) in the strong field regime within the framework of symmetric teleparallel $f(Q)$ gravity, considering three representative models: linear, logarithmic, and exponential. While Bayesian studies of NS observations are well established in General Relativity and curvature based modified gravity theories, such analyses in $f(Q)$ gravity remain largely unexplored. We perform a Bayesian inference analysis by confronting theoretical NS mass-radius predictions with NICER observations of PSR J0030+0451, PSR J0740+6620, PSR J0437+4715, and PSR J0614+3329. The dense matter equation of state is fixed to DDME2 in order to isolate the effects of modified gravity on NS structure. Our results show that the exponential $f(Q)$ model is statistically preferred over the linear and logarithmic cases, as confirmed by Bayes factor comparisons, and exhibits well-constrained. For this model, we obtain a radius and tidal deformability at $1.4\,M_\odot$ of $R_{1.4} = 11.27^{+0.53}_{-0.36}\,\mathrm{km}$ and $Λ_{1.4} = 156.95^{+84.02}_{-41.73}$, respectively, consistent with current observational constraints. These results highlight the potential of NSs as powerful probes of symmetric teleparallel gravity in the strong field regime.

</details>


### [21] [Static hairy black hole in 4D General Relativity](https://arxiv.org/abs/2601.16254)
*Marco Astorino*

Main category: gr-qc

TL;DR: The paper generalises the Schwarzschild black hole solution by introducing a new parameter related to an external gravitational field, resulting in a non-asymptotically flat, oblate black hole with Petrov type I geometry. No singularities exist outside the horizon, and thermodynamic properties are analyzed.


<details>
  <summary>Details</summary>
Motivation: To extend Schwarzschild spacetime by adding a hair parameter that changes its Petrov type and explores non-spherical, non-asymptotically flat black holes beyond current no-hair theorem constraints.

Method: Introduced a new integrating constant representing an external gravitational field, derived the generalised metric, and analyzed curvature properties, conserved charges, and thermodynamics.

Result: A new black hole solution with oblate horizon and reduced horizon area, avoiding curvature/conical singularities outside the horizon. The solution circumvents no-hair theorems via non-asymptotic flatness and nonspherical geometry.

Conclusion: The new metric demonstrates that additional parameters can deform Schwarzschild geometry without singularities, offering new possibilities for studying black holes in external gravitational fields and beyond standard no-hair conditions.

Abstract: In four-dimensional vacuum general relativity the only known static, exact and analytical black hole solution is given by the Schwarzschild spacetime. In this paper this renowned metric is generalised by adding another integrating constant, a hair that switches the metric from the Petrov type D to the type I. This new parameter represents the intensity of an external gravitational field, which can be considered the hyperbolic generalisation of the Witten's bubble of nothing. No curvature or conical singularities are present outside the event horizon. The no hair arguments are circumvented because the metric is not asymptotically flat, and neither the black hole is spherical. The gravitational hair continuously deforms the Schwarzschild geometry: the horizon becomes oblate, while its area is reduced. Conserved charges and thermodynamic properties of the black hole are studied.

</details>


### [22] [Light propagation and quasinormal modes of a topologically charged Schwarzschild-Klinkhamer wormhole](https://arxiv.org/abs/2601.16305)
*C. F. S. Pereira,H. Belich,A. R. Soares,Marcos V. de S. Silva,R. L. L. Vitória,A. A. Araújo Filho*

Main category: gr-qc

TL;DR: The paper analyzes light propagation around a geometric defect wormhole, examining null geodesics, photon orbits, shadow formation, and gravitational lensing in weak and strong fields. It also investigates the effect of global monopole charge and calculates quasinormal modes and scalar perturbations.


<details>
  <summary>Details</summary>
Motivation: To theoretically explore how a wormhole created by a geometric defect influences light paths and gravitational lensing observables, identifying observable effects of the monopole charge and testing the viability of these parameters with current/future observations.

Method: Derivation of analytical deflection angle expansions in weak and strong fields, examination of null geodesics and shadow properties, parameter variation studies for monopole charge, and computation of quasinormal modes/solutions for scalar perturbations.

Result: Demonstrated dependence of lensing observables and shadow features on the global monopole charge, established parameter ranges for observational testing, and obtained solutions for scalar perturbations indicating characteristic quasinormal mode behavior.

Conclusion: The study shows that geometric defect wormholes produce measurable signatures in gravitational lensing and perturbation dynamics, offering potential observational tests to constrain monopole charge parameters and validate exotic spacetime geometries.

Abstract: In this work, we present a theoretical analysis of null geodesics, critical photon orbits, and shadow formation associated with a wormhole generated by a geometric defect. The propagation of light in this spacetime is examined through the deflection angle in both weak- and strong-field regimes. Analytical expansions are derived in each regime and employed to characterize gravitational lensing observables. By varying the global monopole charge, we evaluate its impact on these observables and determine parameter ranges that may be accessible to current or future observational probes. Finally, we calculate the quasinormal modes as well as the time-domain solution for scalar perturbations as well.

</details>


### [23] [Does Gravity Care About Electric Charge? A Minimalist Model and Experimental Test](https://arxiv.org/abs/2601.16325)
*Renato Vieira dos Santos*

Main category: gr-qc

TL;DR: The paper proposes a framework to test if gravity depends on electric charge by introducing a charge-mass coupling in linearized gravity, suggesting a new experiment to explore untested parameter space.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the open question of whether gravity is affected by electric charge, as existing precision tests avoid charged masses, leaving this aspect unexplored.

Method: The authors use a minimalist framework coupling electromagnetism and linearized gravity via conservation of a complex charge-mass current, predicting charge-dependent violations (Δa/g = κ(q/m)). They propose a modified torsion balance experiment where charge-to-mass ratio (q/m) is a controlled variable.

Result: The framework identifies a gap in current experiments and demonstrates how a new torsion balance setup could access untested parameter space, potentially revealing charge dependence in gravitational acceleration.

Conclusion: The study highlights that theoretical minimalism can uncover experimental opportunities, urging renewed focus on charge-dependent gravity tests to probe new physics.

Abstract: Does gravity care about electric charge? Precision tests of the weak equivalence principle achieve remarkable sensitivity but deliberately minimize electric charge on test masses, leaving this fundamental question experimentally open. We present a minimalist framework coupling electromagnetism to linearized gravity through conservation of a complex charge-mass current, predicting charge-dependent violations $Δa/g = κ(q/m)$. Remarkably, this prediction occupies unexplored experimental territory precisely because precision gravity tests avoid charge variation. We identify this as a significant gap and propose a modified torsion balance experiment where $q/m$ is treated as a controlled variable. Such an experiment could test whether gravitational acceleration depends on electric charge, probing physics in genuinely new parameter space. This work exemplifies how theoretical minimalism can reveal overlooked opportunities in fundamental physics.

</details>


### [24] [Thick Lunar Crust Amplifies Gravitational-Wave Signal](https://arxiv.org/abs/2601.16567)
*Lei Zhang,Han Yan,Xian Chen,Jinhai Zhang*

Main category: gr-qc

TL;DR: This paper proposes using the Moon as a gravitational wave (GW) detector in the 10⁻³–0.1 Hz band by creating a high-resolution model that accounts for lunar topography and internal heterogeneity. Simulations show 10-20% signal amplification in thick-crust areas due to mode coupling, enabling optimized landing site selection for future missions.


<details>
  <summary>Details</summary>
Motivation: Existing observatories cannot detect GWs in the 10⁻³–0.1 Hz band. Previous theoretical models suggesting the Moon could act as a resonant detector were hindered by uncertainties about its rugged surface and interior. Resolving this uncertainty is critical for advancing GW detection capabilities.

Method: The authors combined spectral-element simulations (3.7 km grid resolution) with normal-mode perturbation theory. This dual approach captured topographical effects and global oscillation modes, tracking how GW-induced quadrupole modes couple with higher-order eigenmodes caused by lunar heterogeneity.

Result: The model confirmed dominant quadrupole (l=2) oscillations but revealed 10-20% signal amplification in thick-crust regions. Mode-coupling transfers energy into higher-order modes, causing up to tenfold amplification at specific frequencies and locations. Detailed amplification maps were generated.

Conclusion: The Moon can be used as an accurate resonant GW detector. The identified amplification patterns provide actionable data for selecting optimal measurement sites on the lunar surface to maximize signal detection.

Abstract: Gravitational waves (GWs) in the $10^{-3}-0.1$ Hz band encode unique signatures of the early universe and merging compact objects, but they are beyond the reach of existing observatories. Theoretical models suggest that the Moon could act as a resonant detector, but the unknown influence of its rugged surface and heterogeneous interior has cast doubt on this prospect. Here, we resolve this long-standing uncertainty by constructing the first high-resolution, structurally realistic model of the lunar GW response. We achieve this by combining high-fidelity spectral-element simulations with the analytical power of normal-mode perturbation theory, thereby resolving topographical effects down to $3.7$ km grid spacing while maintaining the capacity to discern global free-oscillation patterns. This dual-methodology approach not only recovers the expected predominant quadrupole ($l=2$) oscillation mode, but also exposes a systematic signal amplification of $(10-20)\%$ in thick-crust regions. This enhancement is traced by our normal-mode analysis to a mode-coupling process, in which the original quadrupolar oscillation induced by the passing GWs distributes energy into a series of higher-order modes, the hybridized eigenmodes of the laterally heterogeneous Moon. Near certain eigen-frequencies and at specific locations, we observe up to tenfold amplification, highlighting the power of numerical simulations in resolving these structurally fine-tuned features. Our work establishes the Moon as an accurately calibrated resonant GW detector, and the resulting amplification maps provide quantitative guide for the optimal landing site selection.

</details>


### [25] [Gravitational Lensing Effect from The Revised Deser-Woodard Nonlocal Gravity](https://arxiv.org/abs/2601.16572)
*Haida Li,Xiangdong Zhang*

Main category: gr-qc

TL;DR: Analyzes gravitational lensing effects in revised Deser-Woodward nonlocal gravity, finding unique corrections in both weak and strong fields relative to General Relativity.


<details>
  <summary>Details</summary>
Motivation: To investigate how nonlocal gravity modifies gravitational lensing compared to General Relativity and other theories, potentially offering observational tests.

Method: Calculated deflection angles in weak and strong field limits for static spherically symmetric black holes using revised D-W nonlocal gravity framework.

Result: Weak field shows leading-order nonlocal corrections; strong field corrections depend linearly on ζ but are exponentially suppressed by n. Lensing shares scale invariance with GR/conformal gravity.

Conclusion: The model's lensing behavior provides a way to distinguish it from other gravity theories through astronomical observations.

Abstract: We investigate the gravitational lensing effects of a static spherically symmetric black hole (BH) within the framework of the revised Deser-Woodard (D-W) nonlocal gravity. By analyzing the deflection angle in both the weak and strong field limits, we derive several distinguishing features of the model. In the weak field limit, we report a leading-order correction to the deflection angle directly attributed to the non-local nature of the theory. In the strong field limit, we find that the lensing corrections are almost linearly dependent on the coupling parameter $ζ$ while being exponentially suppressed by the exponent parameter $n$. Furthermore, the gravitational lensing effect in the revised D-W model at a given time shares similar scale-invariant behavior to General Relativity and conformal gravity, offering a potential pathway to distinguish it from other alternatives using astronomical observations.

</details>


### [26] [Dirac-Bergmann algorithm and canonical quantization of $k$-essence cosmology](https://arxiv.org/abs/2601.16703)
*Andrés Lueiza,Andronikos Paliathanasis,Nikolaos Dimakis*

Main category: gr-qc

TL;DR: The paper presents a canonical quantization approach for k-essence cosmology in scalar-tensor theory using the Dirac-Bergmann algorithm, leading to a Wheeler-DeWitt equation. It explores quantum tunneling for phantom crossing and analyzes boundary conditions' effects on singularity avoidance and expansion rate.


<details>
  <summary>Details</summary>
Motivation: To develop a general canonical quantization framework for k-essence cosmology, addressing issues like quantum behavior of cosmological models, phantom crossing phenomena, and singularity resolution in scalar-tensor theories.

Method: 1. Apply the Dirac-Bergmann algorithm to construct the Hamiltonian and identify first/second-class constraints. 2. Introduce canonical conjugate variables via Dirac brackets for quantization. 3. Derive a Wheeler-DeWitt equation from the Hamiltonian constraint. 4. Use a tachyonic field example to study quantum tunneling-driven phantom crossing. 5. Analyze constant potential case with different boundary conditions.

Result: The Hamiltonian constraint simplifies to a quadratic form without potential, yielding a massless Klein-Gordon-like Wheeler-DeWitt equation. Quantum tunneling explains phantom crossing conditions. Boundary conditions influence singularity avoidance and mean expansion rates in the constant potential model.

Conclusion: The quantization method provides a viable framework for studying quantum aspects of k-essence cosmology. It demonstrates phantom crossing viability via tunneling and shows boundary conditions crucially affect cosmological outcomes like singularity resolution and expansion dynamics.

Abstract: We develop a general canonical quantization scheme for $k$-essence cosmology in scalar-tensor theory. Utilizing the Dirac-Bergmann algorithm, we construct the Hamiltonian associated with the cosmological field equations and identify the first- and second-class constraints. The introduction of appropriate canonically conjugate variables with respect to Dirac brackets, allows for the canonical quantization of the model. In these new variables, the Hamiltonian constraint reduces to a quadratic function with no potential term. Its quantum realization leads to a Wheeler-DeWitt equation reminiscent of the massless Klein-Gordon case. As an illustrative example, we consider the action of a tachyonic field and investigate the conditions under which a phantom crossing can occur as a quantum tunneling effect. For the simplified constant potential case, we investigate the consequences of different boundary conditions on the singularity avoidance and to the mean expansion rate.

</details>


### [27] [Ricci-Weyl curvature balance in viscous dissipative collapse: A covariant analysis of singularity censorship](https://arxiv.org/abs/2601.16706)
*Samarjit Chakraborty,Rituparno Goswami,Sunil D. Maharaj,Gareth Amery*

Main category: gr-qc

TL;DR: This paper examines the cosmic censorship conjecture in dissipative gravitational collapse with viscosity, heat flux, and anisotropic pressure using covariant methods. It identifies a curvature-balance mechanism on the apparent horizon that determines singularity visibility, supporting a weaker form of censorship.


<details>
  <summary>Details</summary>
Motivation: To investigate under what conditions a naked singularity might form in realistic astrophysical collapse scenarios involving dissipative processes, thereby testing the validity of the cosmic censorship hypothesis.

Method: The authors use the semi-tetrad covariant formalism to model spherically symmetric collapsing fluids with shear viscosity, bulk viscosity, heat flux, and anisotropic pressure. They derive a master equation for Weyl curvature evolution and analyze null geodesics to study horizon dynamics and singularity visibility.

Result: Key findings include a curvature-balance mechanism between matter (Ricci) and gravitational (Weyl) curvature on the apparent horizon that dictates whether the horizon remains null (black hole) or becomes spacelike (naked singularity). Necessary/sufficient conditions for a locally naked singularity are derived, indicating weak cosmic censorship holds in these dissipative models.

Conclusion: The results suggest that while naked singularities can arise in specific dissipative collapse conditions, they are prevented under generic assumptions, supporting a 'weak cosmic censorship' principle. This extends previous censorship analyses to more physically realistic, viscous collapse scenarios.

Abstract: We investigate the cosmic censorship conjecture in a spherically symmetric collapse with shear and bulk viscosity, heat flux, and pressure anisotropy, imposing physically reasonable energy conditions. Using the semi-tetrad covariant formalism, we derive the dynamics of the collapsing fluid, including a master equation for the evolution of the Weyl curvature, to examine the role of viscosity. The analysis of null geodesic geometry uncovers a novel curvature-balance mechanism between Ricci (matter) and Weyl (free gravitational field) curvature on the apparent horizon; this balance determines the causal nature of the horizon and thereby governs the visibility of the singularity. We then derive necessary and sufficient covariant conditions for the central singularity to be locally naked. Our findings support a weaker form of cosmic censorship and extend the covariant censorship analysis to realistic dissipative, viscous collapse.

</details>


### [28] [General orbital perturbation theory in Schwarzschild space-time](https://arxiv.org/abs/2601.16887)
*Oleksii Yanchyshen,Eva Hackmann,Claus Lämmerzahl*

Main category: gr-qc

TL;DR: The paper presents a general relativistic framework using Gaussian equations for osculating elements to model orbital parameter evolution in strong gravitational fields under perturbing forces. Examples include Kerr and q-metric spacetimes, with calculations showing agreement with known Lense-Thirring precession in post-Newtonian limits.


<details>
  <summary>Details</summary>
Motivation: To develop a comprehensive method for describing relativistic orbital dynamics in strong gravity scenarios, accounting for external perturbing forces without restrictive assumptions.

Method: Derivation of general relativistic Gaussian equations for osculating elements in Schwarzschild spacetime. Applied linear approximation solutions to Kerr and q-metric spacetime perturbations. Compared results with established Lense-Thirring precession in post-Newtonian regime.

Result: Successfully validated the framework by reproducing the Lense-Thirring effect in Kerr spacetime's post-Newtonian limit. Demonstrated applicability for analyzing orbital parameter evolution in both Kerr and q-metric perturbation scenarios.

Conclusion: The developed formalism offers a robust tool for studying relativistic orbital dynamics under various perturbations. It bridges theoretical frameworks with observable phenomena like frame-dragging effects, supporting future studies in strong-field gravity.

Abstract: We derive general relativistic Gaussian equations for osculating elements for orbits under the influence of a perturbing force without any restrictions in an underlying Schwarzschild space-time. Such a formulation provides a way to describe the evolution of orbital parameters in strong gravity relativistic settings. As examples of external forces we considered Kerr and $q$-metric space-times generated forces, for which we solve equations for osculating elements in linear approximation. For the Kerr space-time in the post-Newtonian limit, our result reproduces the well-known Lense--Thirring precession of the longitude of the ascending node.

</details>


### [29] [Anisotropy Strikes Back: Modified Gravity and Dark Matter Halos](https://arxiv.org/abs/2601.16958)
*Paolo M Bassani*

Main category: gr-qc

TL;DR: The paper investigates dark matter-like fluid behavior in spherically symmetric LTB minisuperspace models for General Relativity (GR) and Horava-Lifshitz (HL) gravity. In GR, potential deformations preserving the Dirac algebra introduce anisotropic stress, mimicking dark matter but failing to reproduce flat rotation curves. In HL gravity, specific LTB backgrounds yield ghost-free dark matter density scaling, though full backreaction solutions remain unexplored.


<details>
  <summary>Details</summary>
Motivation: To understand how symmetry properties of gravitational theories (GR/HL) generate effective dark sources and address challenges like dark matter's mass scaling and rotational dynamics within modified gravity frameworks.

Method: Deforming the GR Hamiltonian with an extra weight $+1$ density to modify the potential, analyzing algebra preservation and emergent stress-energy contributions. For HL gravity, derived non-conservation laws for dust components under deformed Dirac algebra, focusing on ghost-free conditions and infrared GR recovery.

Result: GR deformations produce anisotropic pressure fluids with isothermal mass scaling but insufficient to explain flat rotation curves. HL gravity achieves consistent dark matter-like density in select LTB backgrounds, though full dynamic solutions require future work.

Conclusion: Potential deformations in GR and HL gravity can generate effective dark matter sources under certain conditions, but GR fails rotational curve predictions while HL shows promise with constraints. Backreaction dynamics and observational validation are key next steps.

Abstract: We explore dark matter like fluids in a spherically symmetric Lemaitre Tolman Bondi (LTB) minisuperspace, tracking how symmetry properties of the Hamiltonian constraint control the emergence of effective dark sources in General Relativity (GR) and Horava Lifshitz (HL) gravity. We first deform the GR Hamiltonian by adding an extra weight $+1$ density to the potential. We show that potential deformations of this type leave the (reduced) Dirac algebra unchanged and the modification is naturally reinterpreted as an effective anisotropic stress energy contribution. While the fluid reproduces an isothermal-like mass scaling, its pressure anisotropy prevents it from giving flat rotation curves. We then turn to HL gravity, where the deformed Dirac algebra induces a controlled nonconservation law for an emergent dust component. Generalizing earlier results, we identify a restricted class of LTB backgrounds for which the HL source term yields a positive scaling dark matter density, consistent with ghost-freedom, and recovery of GR in the infrared. The analysis is conditional on a prescribed background: obtaining a fully backreacted areal radius solution consistent with the HL field equations is left as a natural direction for future work.

</details>


### [30] [Embedding Wormholes and Dyonic Black Strings in Warped Braneworlds via Local Sum Rules](https://arxiv.org/abs/2601.16969)
*G. Alencar,T. M. Crispim,Francisco S. N. Lobo*

Main category: gr-qc

TL;DR: The paper extends previous work on Local Sum Rules by constructing braneworld compact objects using localized matter fields, including scalar field wormholes and nonlinear electrodynamics black strings that connect to known four-dimensional solutions.


<details>
  <summary>Details</summary>
Motivation: To explore how dynamically consistent and localizable matter fields can support compact objects in Randall-Sundrum braneworlds, building on foundational solutions like the Chamblin black string and extending them to more complex scenarios.

Method: Revisiting the Chamblin et al. black string, embedding the Ellis-Bronnikov wormhole with a localized scalar field, and deriving new black string solutions using localized nonlinear electrodynamics (NED) with a specific Lagrangian. The solutions are analyzed for both purely magnetic and dyonic configurations.

Result: Successfully embedded Ellis-Bronnikov wormhole via localized scalar field and obtained two novel NED black string solutions which reduce to the Chamblin string when β→0, showing connections between higher-dimensional fields and 4D solutions.

Conclusion: Localized higher-dimensional matter fields can consistently support braneworld compact objects, bridging higher-dimensional physics with established four-dimensional solutions, and providing nontrivial examples of dynamically consistent configurations.

Abstract: Building on our previous work [1], where the Local Sum Rules (LSR) were established, we investigate the construction of compact objects in Randall-Sundrum braneworlds supported by matter fields that are dynamically consistent and localizable. We begin by revisiting the Chamblin et al. black string, highlighting its role as a foundational higher-dimensional solution. We then show that the Ellis-Bronnikov wormhole can be consistently embedded in this framework via a localized free scalar field, providing a simple yet nontrivial example of a braneworld compact object. Finally, we derive two novel black string solutions sourced by a localized nonlinear electrodynamics (NED) theory with Lagrangian $\mathcal{L}(\mathcal{F}) = -β\sqrt{\mathcal{F}}$, corresponding to purely magnetic and dyonic configurations. The purely magnetic solution reproduces the classical Letelier string cloud on the brane, while the dyonic solution generalizes it to include electric charge, closely paralleling the Letelier-Alencar construction. Both NED solutions reduce smoothly to the Chamblin et al. black string in the limit $β\to 0$, illustrating how localized higher-dimensional matter fields can consistently support braneworld compact objects and connect higher-dimensional physics with well-known four-dimensional solutions.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [31] [proto-Lightspeed: a high-speed, ultra-low read noise imager on the Magellan Clay Telescope](https://arxiv.org/abs/2601.16268)
*Christopher Layden,Kevin Burdge,Gabor Furesz,Juliana Garcia-Mejia,Jack Dinsmore,Geoffrey Mo,David Osip,John J. Piotrowski,Roger W. Romani,August Berne,Deepro Chakrabarty,Emma Chickles*

Main category: astro-ph.IM

TL;DR: The proto-Lightspeed instrument, newly commissioned on the Magellan Clay Telescope, offers high-speed optical imaging with remarkable capabilities, including deep sub-electron read noise, adjustable magnification, and high frame rates (up to 6600 Hz for smaller fields). It is designed for studying rapid astronomical phenomena like compact binaries, exoplanet transits, and TNO occultations. A future facility instrument, Lightspeed, plans to expand its field of view and enable multicolor imaging.


<details>
  <summary>Details</summary>
Motivation: The motivation behind proto-Lightspeed is to address the need for rapid, high-sensitivity optical observations to study transient and variable astronomical events that occur on timescales of milliseconds to seconds, which are challenging for conventional instruments to capture effectively.

Method: The instrument utilizes commercial re-imaging lenses and the Hamamatsu ORCA-Quest 2 camera to achieve high frame rates and low read noise. It can image a 1' diameter field at 200 Hz or smaller windowed fields at higher rates (e.g., 6600 Hz for a 1.6''×1' field). Adjustable magnification allows pixel scales between 0.017'' and 0.050''.

Result: Commissioning runs confirmed proto-Lightspeed's performance with high-speed imaging in g', r', and i' bands, demonstrating its capability to resolve rapid phenomena. The data validates its potential for detecting short-lived events like TNO occultations and pulsar emission variations.

Conclusion: proto-Lightspeed successfully fulfills its design goals, demonstrating exceptional speed and sensitivity. Future upgrades to the full Lightspeed instrument will enhance its field of view and multiband imaging, enabling broader scientific applications for the Magellan Consortium starting in 2026B.

Abstract: proto-Lightspeed is a new instrument that has been commissioned on the Nasmyth East port of the Magellan Clay Telescope at Las Campanas Observatory to deliver high-speed optical imaging with deep sub-electron read noise. Making use of commercial re-imaging lenses and the ORCA-Quest 2 camera from Hamamatsu, proto-Lightspeed images a field $1'$ in diameter at up to $200$ Hz or windowed fields at higher rates, up to 6600 Hz for a $1.6''\times 1'$ field of view. proto-Lightspeed delivers seeing-limited image quality in the $g'$, $r'$, and $i'$ bands and adjustable magnification for pixel scales between $0.017''-0.050''$. proto-Lightspeed is well suited to studying compact binary systems, exoplanet transits, rapid flaring associated with accretion, periodic optical emission from pulsars, occultations of background stars by small trans-Neptunian Objects, and any other rapidly variable source. proto-Lightspeed will be a P.I. instrument beginning in 2026B, available for use by members of the Magellan Consortium. In this paper, we discuss the design and performance of the instrument, results from its two commissioning runs, and plans for a facility instrument, Lightspeed, to support simultaneous multicolor imaging across a $7'\times4'$ field.

</details>


### [32] [Characterization of the commercial spectrograph system for astronomical observations: PIXIS 1300BX Camera and IsoPlane 320A Spectrograph](https://arxiv.org/abs/2601.16371)
*Jiwon Jang,Changsu Choi,Ho Seong Hwang,Haeun Chung,Hyeonguk Bahk,Dongkok Kim,Jae-Woo Kim*

Main category: astro-ph.IM

TL;DR: The paper presents a detailed characterization of a PIXIS 1300BX CCD camera and IsoPlane 320A spectrograph system, validating their performance for the upcoming A-SPEC galaxy survey. Key results include successful mitigation of signal gradients, high quantum efficiency, and demonstrated multi-object spectroscopy capability with moderate resolution.


<details>
  <summary>Details</summary>
Motivation: The motivation is to evaluate the spectrograph system's performance to ensure it meets the requirements for the A-SPEC survey, which aims to conduct an all-sky spectroscopic study of nearby galaxies. Rigorous testing is needed to confirm its readiness for large-scale observations.

Method: Laboratory tests (readout noise, dark current, gain, full-well capacity via bias/dark/photon transfer curves) and on-sky observations were performed. A gradient correction technique was developed to address row-dependent signal artifacts caused by the shutter-less test setup. Quantum efficiency and spectral resolution with different gratings were measured.

Result: The gradient correction technique successfully matched manufacturer specifications. The system shows >80% quantum efficiency at 400-800 nm, fast persistence decay, and achieved multi-object spectroscopy with R ≈ 600–2600 at the SAO 1m telescope. Python tools for testing were released to aid future evaluations.

Conclusion: The spectrograph system is validated for A-SPEC's needs, demonstrating reliability for rapid, high-efficiency multi-object spectroscopy. Released tools enable broader community adoption and future instrument assessments.

Abstract: We present the result from a comprehensive laboratory and on-sky characterization of the commercial spectrograph system consisting of a PIXIS 1300BX charge-coupled device (CCD) camera and an IsoPlane 320A spectrograph as part of the preparation of the forthcoming all-sky spectroscopic survey of nearby galaxies (A-SPEC). In the laboratory, we have quantified readout noise, dark current, gain, and full-well capacity via bias, dark, and photon transfer curve analysis at all acquisition modes. To do that, we have developed a gradient correction technique to address row-dependent signal gradients in the image, which are caused by the shutter-less condition of our CCD camera test setup. The technique successfully reproduces the values in the manufacturer specifications. We also have measured quantum efficiency exceeding 80% from 400--800 nm and $\gtrsim$ 90% between 450--750 nm, with sub-second persistence decay, making it ideal for rapid, multi-object spectroscopy. Using a set of diffraction gratings (150, 300, and 600 gr mm$^{-1}$), we have evaluated the spatial separability of multiple spectra and spectral resolution. We have conducted a test observation with this spectrograph system at the Seoul National University Astronomical Observatory (SAO) 1 m telescope and successfully demonstrated its capability of multi-object spectroscopy with moderate resolution of $R \approx 600 - 2600$. We release all Python codes for the test and recipes to facilitate further instrument evaluations.

</details>


### [33] [ALMA publication statistics](https://arxiv.org/abs/2601.16687)
*Felix Stoehr,María Díaz Trigo,Evanthia Hatziminaoglou,Uta Grothkopf,Silvia Meakins,Leslie Kiefer,Lance Utley,Mika Konuma,Eelco van Kampen,Gergö Popping,Enrique Macias,Martin Zwaan*

Main category: astro-ph.IM

TL;DR: The paper analyzes the scientific impact of the ALMA observatory through its 4,190 publications from 2012 to 2024, highlighting its high citation rates, extensive authorship, archival data usage, and contributions to major projects like the first black hole image.


<details>
  <summary>Details</summary>
Motivation: To evaluate ALMA's scientific success by quantifying its publication output, citation metrics, and comparison with other facilities, ensuring its role in advancing astronomical research.

Method: Comprehensive analysis of ALMA publications (4,190 papers) and related data (2,670 projects, 19,265 hours observing time), examining categories like science areas, geographical distribution, archival use, publication timelines, and comparisons with HST and VLT.

Result: ALMA demonstrates high impact with 41 citations/paper average, 70% project publication rate, 40% archival data use in 2024, and over 9,400 authors. Its impact factor exceeds other major facilities, and it contributes critically to VLBI milestones.

Conclusion: ALMA is confirmed as a leading high-impact astronomical facility, comparable to HST and VLT, with pivotal roles in both standard observations and collaborative projects like black hole imaging.

Abstract: The success of an astronomical facility is measured by its scientific impact. A principal metric for this impact is the ensemble of peer-reviewed publications based on the observational data obtained by the facility. We present a comprehensive study of the statistics of the 4,190 refereed publications of the Atacama Large Millimeter/Submillimeter Array (ALMA) in the period from 2012 to 2024. The publications have received 169,985 citations and are based on 2,670 ALMA projects totalling 19,265 hours of 12-m-array-equivalent observing time. Our study analyses publication statistics related to various aspects, e.g. science categories, geographical distribution, archival research, time to publication, publication fraction, and citations. We also look into the community and compare ALMA with other facilities. We find that ALMA is a high-impact observatory with an average of 41 citations per publication, ~70% of observed projects published, ~40% of publications making use of archival data in 2024, more than 9,400 unique authors, and a publication evolution following that of HST and VLT. Currently, the impact factor for ALMA publications is larger than that of all other major astronomical facilities. ALMA also plays a pivotal role in very long baseline interferometry (VLBI), substantially contributing to landmark achievements such as capturing the first image of a black hole shadow.

</details>


### [34] [Illuminating the Physics of Cosmic Origin and Evolution: A UK Space Frontiers 2035 White Paper](https://arxiv.org/abs/2601.16761)
*Florian Beutler,Eva-Maria Mueller,Seshadri Nadathur,Yun Wang,David Alonso,Tessa Baker,Sownak Bose,Rebecca Canning,Shaun Cole,Fergus Cullen,Willem Elbers,Pedro Ferreira,Carlos Frenk,Oscar Gonzalez,Or Graur,Boryana Hadzhiyska,Alex Hall,Catherine Heymans,Sergey Koposov,Kazuya Koyama,Ofer Lahav,Baojiu Li,Avery Meiksin,Johannes Noller,John Peacock,Alkistis Pourtsidou,Giorgio Savini,Andy Taylor,Rita Tojeiro,David Wands,Massimo Robberto,Gregory Wirth,Mark Dickinson,Thomas Greene,Jeffrey Kruk,Will Percival,Andreas Faisst,Lynne Hillenbrand,Jeyhan Kartaltepe,Nikhil Padmanabhan,Lado Samushia,Lee Armus,Andrew Benson,Micol Bolzonella,Samuel Brieden,Jarle Brinchmann,Robert Content,Emanuele Daddi,Kyle Finner,Andrew Hearin,Cullan Howlett,Jon Lawrence,Gregory Mosby,Zoran Ninkov,Ken Osato,Casey Papovich,Jack Piotrowski,Lucia Pozzetti,Alvise Raccanelli,Jason Rhodes,Shun Saito,Hee-Jong Seo,Zachary Slepian,Steve Smee*

Main category: astro-ph.IM

TL;DR: This paper proposes SIRMOS, a space-based spectroscopic survey to address key cosmological questions through precise redshift measurements of 100 million galaxies. It aims to study cosmic inflation, the accelerating expansion of the Universe, and neutrino masses, leveraging advanced technologies to advance UK space capabilities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle fundamental cosmological challenges: understanding cosmic inflation's physics, investigating dark energy via the Universe's accelerating expansion, and determining neutrino mass constraints. Current surveys lack the necessary precision and scale, necessitating a dedicated mission.

Method: SIRMOS uses a high-throughput, multiplexed spectrograph covering 1.25-2.5 μm wavelengths at moderate resolution to measure redshifts for galaxies at 1<z<4. It builds on existing photometric data, employs digital micromirror devices, advanced mirrors, and detectors, and emphasizes space-based implementation of proven terrestrial technologies.

Result: The survey would provide unprecedented galaxy redshift measurements with minimal bias, enabling precise cosmological parameter estimation (e.g., inflation parameters, dark energy properties, neutrino mass sum) and testing fundamental physics in the early universe.

Conclusion: SIRMOS offers a robust science case for UK participation, aligning with national space strategies, showcasing technical capabilities, and driving economic growth through industrial collaboration. It represents a strategic investment in both science and space economy development.

Abstract: Understanding the Universe's origins and evolution remains one of the most fundamental challenges in modern cosmology. This white paper explores three key science priorities in this field: unravelling the physics of cosmic inflation, investigating the accelerating expansion of the Universe, and precisely measuring the sum of the neutrino masses. Achieving these goals requires a dedicated survey to map the large-scale structure at high redshift in unprecedented detail. We describe how this can be achieved through a mission concept called SIRMOS, providing a high-throughput, highly multiplexed spectroscopic capability to obtain accurate redshifts for over 100 million galaxies over a wide sky area. Such a survey would leverage the deepest existing wide-area photometric catalogues for targeting, with spectra offering continuous 1.25-2.5~$μ$m wavelength coverage at moderate resolution, allowing precise redshift measurements in the $1<z<4$ range with minimal bias. We outline the scientific opportunities this presents. Recent years have seen significant advances in instrumentation, including digital micromirror devices, complex telescope mirrors, large detector arrays, and data processing pipelines. While these technologies have been demonstrated in terrestrial applications, such a survey is a unique opportunity to apply these proven capabilities in space to address fundamental questions in cosmology. Participation in such a mission will simultaneously deliver a compelling science case, help align UK Space Agency and STFC strategies, demonstrate the UK's growing capability in end-to-end space missions, and strengthen the national space economy through high-value industrial participation.

</details>


### [35] [The Astrosky Ecosystem: An independent online platform for science communication and social networking](https://arxiv.org/abs/2601.16838)
*Emily L. Hunt,Vincent S. Carpenter,Kyle W. Cook,Douglas G. Hilton,Mehnaaz Asad,Janine Jochum,Kelly Lepo,Jamie Zvirzdin*

Main category: astro-ph.IM

TL;DR: The Astrosky Ecosystem project aims to provide open-source social media tools for astronomers, emphasizing independence from corporate platforms like Twitter through the use of Bluesky and the AT Protocol.


<details>
  <summary>Details</summary>
Motivation: Traditional social media's reliance on corporations poses risks due to potential platform instability or policy changes, as seen with Twitter's decline. Astronomers need reliable, community-controlled tools for communication and outreach.

Method: Developing open-source tools and integrations on Bluesky's decentralized platform using the AT Protocol to facilitate networking, outreach, and self-hosted infrastructure for the astronomy community.

Result: The Astrosky Ecosystem offers a viable alternative to proprietary social media, allowing astronomers to manage their own social infrastructure and ensuring continuity in communication despite corporate platform vulnerabilities.

Conclusion: The project highlights the importance of decentralized social media for scientific communities. Future work includes expanding tools and encouraging wider adoption to foster a sustainable, community-driven communication network.

Abstract: While almost everything that astronomers study occurs in the vacuum of space, astronomy itself does not `happen in a vacuum'. Interactions between scientists, as well as outreach to members of the public, improve extensively from access to good communication tools. Social media has become a key tool for communication in astronomy, being widely used by individuals and organizations alike for networking, outreach, and more. However, traditional social media is reliant on benevolent corporations providing a free service without compromising on quality, and the recent takeover and decline of Twitter has shown how vulnerable these platforms can be. In this proceeding, we present The Astrosky Ecosystem, which is an initiative to develop open-source tools and integrations for social media, principally the Bluesky social network. We explain how our project enables the astronomy community to operate its own social media infrastructure, independent of for-profit corporations. We also discuss some of the project's technical aspects, including its use of the AT Protocol for social networking, before concluding with ideas for the future.

</details>


### [36] [CosmoSlider: An educational tool for cosmology](https://arxiv.org/abs/2601.16919)
*Andreas Nygaard,Steen Hannestad,Thomas Tram*

Main category: astro-ph.IM

TL;DR: CosmoSlider enables real-time, interactive exploration of CMB power spectra by varying cosmological parameters using a neural-network emulator, accessible via iOS and web platforms.


<details>
  <summary>Details</summary>
Motivation: The need for accessible, interactive tools to teach how cosmological parameters affect CMB spectra without computational barriers.

Method: Utilizes a TensorFlow Lite-based neural-network emulator for rapid CMB spectrum calculation, avoiding precomputed grids or Einstein-Boltzmann solver calls.

Result: Provides real-time visual feedback across platforms, enhancing intuition about CMB anisotropy physics.

Conclusion: CosmoSlider complements traditional education by offering an interactive, low-barrier resource for cosmology learners and educators.

Abstract: Understanding how cosmological parameters influence the cosmic microwave background (CMB) power spectra is a central component of modern cosmology education, but interactive exploration is often limited by computational cost or technical complexity. We present CosmoSlider, a lightweight visualization tool that enables real-time exploration of CMB power spectra as multiple cosmological parameters are varied simultaneously. The tool employs a neural-network emulator implemented using TensorFlow Lite, allowing rapid evaluation of spectra without relying on large grids of precomputed models or on-demand execution of Einstein--Boltzmann solvers. CosmoSlider is available both as an iOS application and as a web-based tool, making it accessible across platforms and suitable for use in classrooms, lectures, and self-guided study. By providing immediate visual feedback, CosmoSlider supports the development of intuition for the physical processes underlying CMB anisotropies and serves as a complementary resource to traditional theoretical instruction.

</details>


### [37] [Probabilistic Graphical Models in Astronomy](https://arxiv.org/abs/2601.16959)
*Abigail Sheerin,Giuseppe Vinci*

Main category: astro-ph.IM

TL;DR: Probabilistic graphical models are effective for analyzing complex astronomical data, particularly in studying exoplanets and host stars, by revealing underlying dependencies and data-generating processes.


<details>
  <summary>Details</summary>
Motivation: Modern astronomical datasets from advanced instruments are too complex for classical methods, necessitating new tools like probabilistic graphical models to uncover their underlying structures.

Method: Using probabilistic graphical models, variables (e.g., exoplanet properties, host star characteristics) are represented as nodes in a network to visualize and analyze hierarchical relationships and dependencies.

Result: The application of these models successfully demonstrates their value in addressing the complexity of astronomical data, specifically in exoplanet and host star research.

Conclusion: Graphical models provide a robust framework for advancing astronomical studies by handling intricate dependencies, suggesting broader applicability across cosmic variables and theories.

Abstract: The field of astronomy is experiencing a data explosion driven by significant advances in observational instrumentation, and classical methods often fall short of addressing the complexity of modern astronomical datasets. Probabilistic graphical models offer powerful tools for uncovering the dependence structures and data-generating processes underlying a wide array of cosmic variables. By representing variables as nodes in a network, these models allow for the visualization and analysis of the intricate relationships that underpin theories of hierarchical structure formation within the universe. We highlight the value that graphical models bring to astronomical research by demonstrating their practical application to the study of exoplanets and host stars.

</details>
